{
  "embedding_model": "all-MiniLM-L6-v2",
  "embedding_dimension": 384,
  "created_at": "2026-01-19T07:21:39.883337Z",
  "settings": {
    "MAX_URLS": 69,
    "MAX_CHUNKS_PER_PAGE": 60,
    "CHUNK_SIZE": 1400,
    "CHUNK_OVERLAP": 220,
    "MIN_TEXT_LEN": 300,
    "BATCH_SIZE": 64
  },
  "stats": {
    "total_urls_requested": 69,
    "succeeded_urls": 58,
    "failed_urls": 11,
    "total_chunks": 436
  },
  "failed_urls": [
    {
      "url": "https://learn.microsoft.com/en-us/azure/architecture/",
      "error": "Text too short: 271 chars"
    },
    {
      "url": "https://adr.github.io/usage/",
      "error": "404 Client Error: Not Found for url: https://adr.github.io/usage/"
    },
    {
      "url": "https://aws.amazon.com/builders-library/what-is-the-builders-library/",
      "error": "404 Client Error:  for url: https://aws.amazon.com/builders-library/what-is-the-builders-library/"
    },
    {
      "url": "https://sre.google/sre-book/table-of-contents/",
      "error": "Text too short: 95 chars"
    },
    {
      "url": "https://www.nist.gov/cyberframework/framework",
      "error": "Text too short: 51 chars"
    },
    {
      "url": "https://sre.google/workbook/table-of-contents/",
      "error": "Text too short: 95 chars"
    },
    {
      "url": "https://docs.docker.com/compose/getting-started/",
      "error": "404 Client Error: Not Found for url: https://docs.docker.com/compose/getting-started/"
    },
    {
      "url": "https://kafka.apache.org/documentation.html",
      "error": "Text too short: 0 chars"
    },
    {
      "url": "https://mlflow.org/docs/latest/llms/langchain.html",
      "error": "Text too short: 0 chars"
    },
    {
      "url": "https://developer.hashicorp.com/terraform/docs/concepts/",
      "error": "404 Client Error: Not Found for url: https://developer.hashicorp.com/terraform/docs/concepts"
    },
    {
      "url": "https://grafana.com/docs/k6/latest/getting-started/what-is-k6/",
      "error": "404 Client Error: Not Found for url: https://grafana.com/docs/k6/latest/getting-started/what-is-k6/"
    }
  ],
  "chunks": [
    {
      "chunk_id": "15f01f143858de0c9bf40e032c3bb58b00a37934",
      "url": "https://microservices.io/patterns/index.html",
      "title": "A pattern language for microservices",
      "text": "A pattern language for microservices pattern The beginnings of a pattern language for microservice architectures. 点击这里，访问本系列文章的中文翻译 Click here for Chinese translation of the patterns Architectural style Which architectural style should you choose for an application? Service boundaries How to decompose an application into services? Refactoring to services Service collaboration How to implement operations that span multiple services? Database per Service - each service has its own private database Shared database - services share a database Saga - use sagas, which a sequences of local transactions, to maintain data consistency across services Command-side replica - maintain a queryable replica of data in a service that implements a command API Composition - implement queries by invoking the services that own the data and performing an in-memory join CQRS - implement queries by maintaining one or more materialized views that can be efficiently queried Domain event - publish an event whenever data changes Event sourcing - persist aggregates as a sequence of events Transactional messaging How to send messages as part of a database transaction? Testing How to test services? Consumer-driven contract test - a test suite for a service that is written by the developers of another service that consumes it Consumer-side contract test - a test suite for a service client (e.g. another servic",
      "chunk_index": 0
    },
    {
      "chunk_id": "425bcdaa93edc7eb7e53c11a7a6ecb46cdef14fa",
      "url": "https://microservices.io/patterns/index.html",
      "title": "A pattern language for microservices",
      "text": "vices? Consumer-driven contract test - a test suite for a service that is written by the developers of another service that consumes it Consumer-side contract test - a test suite for a service client (e.g. another service) that verifies that it can communicate with the service Service component sest - a test suite that tests a service in isolation using test doubles for any services that it invokes Deployment How to deploy an application’s services? Cross-cutting concerns How to handle cross cutting concerns? Microservice chassis - a framework that handles cross-cutting concerns and simplifies the development of services Externalized configuration - externalize all configuration such as database location and credentials Service Template - a template that implements standard cross cutting concerns and is intended to be copied by a developer in order to quickly start developing a new service Communication styles Which mechanisms do services use to communicate with each other and their external clients? External API How do external clients communicate with the services? Service discovery How does the client of an RPI-based service discover the network location of a service instance? Reliability How to prevent a network or service failure from cascading to other services? Circuit Breaker - invoke a remote service via a proxy that fails immediately when the failure rate of the remot",
      "chunk_index": 1
    },
    {
      "chunk_id": "820ea85f428c766406731d545b31a75a6504949e",
      "url": "https://microservices.io/patterns/index.html",
      "title": "A pattern language for microservices",
      "text": " service instance? Reliability How to prevent a network or service failure from cascading to other services? Circuit Breaker - invoke a remote service via a proxy that fails immediately when the failure rate of the remote call exceeds a threshold Security How to communicate the identity of the requestor to the services that handle the request? Access Token - a token that securely stores information about user that is exchanged between services Observability How to understand the behavior of an application and troubleshoot problems? Log aggregation - aggregate application logs Application metrics - instrument a service’s code to gather statistics about operations Audit logging - record user activity in a database Distributed tracing - instrument services with code that assigns each external request an unique identifier that is passed between services. Record information (e.g. start time, end time) about the work (e.g. service requests) performed when handling the external request in a centralized service Exception tracking - report all exceptions to a centralized exception tracking service that aggregates and tracks exceptions and notifies developers. Health check API - service API (e.g. HTTP endpoint) that returns the health of the service and is intended to be pinged, for example, by a monitoring service Log deployments and changes UI design How to implement a UI screen or pag",
      "chunk_index": 2
    },
    {
      "chunk_id": "53248c9dd2870c21a916f5ed6a557643c71c6f21",
      "url": "https://microservices.io/patterns/index.html",
      "title": "A pattern language for microservices",
      "text": "k API - service API (e.g. HTTP endpoint) that returns the health of the service and is intended to be pinged, for example, by a monitoring service Log deployments and changes UI design How to implement a UI screen or page that displays data from multiple services? Server-side page fragment composition - build a webpage on the server by composing HTML fragments generated by multiple, business capability/subdomain-specific web applications Client-side UI composition - Build a UI on the client by composing UI fragments rendered by multiple, business capability/subdomain-specific UI components",
      "chunk_index": 3
    },
    {
      "chunk_id": "0b8346e45e301c1c803645842979a75f0632f727",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "Ambassador Create helper services that send network requests on behalf of a consumer service or application. - Reliability - Security Anti-Corruption Layer Implement a faÃ§ade or adapter layer between a modern application and a legacy system. - Operational Excellence Asynchronous Request-Reply Decouple back-end processing from a front-end host. This pattern is useful when back-end processing must be asynchronous, but the front end requires a clear and timely response. - Performance Efficiency Backends for Frontends Create separate backend services for specific frontend applications or interfaces. - Reliability - Security - Performance Efficiency Bulkhead Isolate elements of an application into pools so that if one fails, the others continue to function. - Reliability - Security - Performance Efficiency Cache-Aside Load data on demand into a cache from a data store. - Reliability - Performance Efficiency Choreography Let individual services decide when and how a business operation is processed, instead of depending on a central orchestrator. - Operational Excellence - Performance Efficiency Circuit Breaker Handle faults that might take a variable amount of time to fix when an application connects to a remote service or resource. - Reliability - Performance Efficiency Claim Check Split a large message into a claim check and a payload to avoid overwhelming a message bus. - Reliabi",
      "chunk_index": 0
    },
    {
      "chunk_id": "752906fa2ac4040433cabcd106c5698c03ae7b92",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "o fix when an application connects to a remote service or resource. - Reliability - Performance Efficiency Claim Check Split a large message into a claim check and a payload to avoid overwhelming a message bus. - Reliability - Security - Cost Optimization - Performance Efficiency Compensating Transaction Undo the work performed by a sequence of steps that collectively form an eventually consistent operation. - Reliability Competing Consumers Enable multiple concurrent consumers to process messages that they receive on the same messaging channel. - Reliability - Cost Optimization - Performance Efficiency Compute Resource Consolidation Consolidate multiple tasks or operations into a single computational unit. - Cost Optimization - Operational Excellence - Performance Efficiency CQRS Separate operations that read data from those that update data by using distinct interfaces. - Performance Efficiency Deployment Stamps Deploy multiple independent copies of application components, including data stores. - Operational Excellence - Performance Efficiency Event Sourcing Use an append-only store to record a full series of events that describe actions taken on data in a domain. - Reliability - Performance Efficiency External Configuration Store Move configuration information out of an application deployment package to a centralized location. - Operational Excellence Federated Identity Del",
      "chunk_index": 1
    },
    {
      "chunk_id": "32b4f187f1f6a65a54db395b6b96c2875bc5e0ae",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "main. - Reliability - Performance Efficiency External Configuration Store Move configuration information out of an application deployment package to a centralized location. - Operational Excellence Federated Identity Delegate authentication to an external identity provider. - Reliability - Security - Performance Efficiency Gateway Aggregation Use a gateway to aggregate multiple individual requests into a single request. - Reliability - Security - Operational Excellence - Performance Efficiency Gateway Offloading Offload shared or specialized service functionality to a gateway proxy. - Reliability - Security - Cost Optimization - Operational Excellence - Performance Efficiency Gateway Routing Route requests to multiple services by using a single endpoint. - Reliability - Operational Excellence - Performance Efficiency Geode Deploy back-end services across geographically distributed nodes. Each node can handle client requests from any region. - Reliability - Performance Efficiency Health Endpoint Monitoring Implement functional checks in an application that external tools can access through exposed endpoints at regular intervals. - Reliability - Operational Excellence - Performance Efficiency Index Table Create indexes over the fields in data stores that queries frequently reference. - Reliability - Performance Efficiency Leader Election Coordinate actions in a distributed applic",
      "chunk_index": 2
    },
    {
      "chunk_id": "6d851e8714221d965aaf251ecb85bc1838c8d7d2",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "ence - Performance Efficiency Index Table Create indexes over the fields in data stores that queries frequently reference. - Reliability - Performance Efficiency Leader Election Coordinate actions in a distributed application by electing one instance as the leader. The leader manages a collection of collaborating task instances. - Reliability Materialized View Generate prepopulated views over the data in one or more data stores when the data is poorly formatted for required query operations. - Performance Efficiency Messaging Bridge Build an intermediary to enable communication between messaging systems that are otherwise incompatible. - Cost Optimization - Operational Excellence Pipes and Filters Break down a task that performs complex processing into a series of separate elements that can be reused. - Reliability Priority Queue Prioritize requests sent to services so that requests with a higher priority are processed more quickly. - Reliability - Performance Efficiency Publisher/Subscriber Enable an application to announce events to multiple consumers asynchronously, without coupling senders to receivers. - Reliability - Security - Cost Optimization - Operational Excellence - Performance Efficiency Quarantine Ensure that external assets meet a team-agreed quality level before the workload consumes them. - Security - Operational Excellence Queue-Based Load Leveling Use a queue",
      "chunk_index": 3
    },
    {
      "chunk_id": "63dff6c400941ccef024f185a0c3e23a271e7116",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "nal Excellence - Performance Efficiency Quarantine Ensure that external assets meet a team-agreed quality level before the workload consumes them. - Security - Operational Excellence Queue-Based Load Leveling Use a queue that creates a buffer between a task and a service to smooth intermittent heavy loads. - Reliability - Cost Optimization - Performance Efficiency Rate Limiting Avoid or minimize throttling errors by controlling the consumption of resources. - Reliability Retry Enable applications to handle anticipated temporary failures by retrying failed operations. - Reliability Saga Manage data consistency across microservices in distributed transaction scenarios. - Reliability Scheduler Agent Supervisor Coordinate a set of actions across distributed services and resources. - Reliability - Performance Efficiency Sequential Convoy Process a set of related messages in a defined order without blocking other message groups. - Reliability Sharding Divide a data store into a set of horizontal partitions or shards. - Reliability - Cost Optimization Sidecar Deploy components into a separate process or container to provide isolation and encapsulation. - Security - Operational Excellence Static Content Hosting Deploy static content to a cloud-based storage service for direct client delivery. - Cost Optimization Strangler Fig Incrementally migrate a legacy system by gradually replacing",
      "chunk_index": 4
    },
    {
      "chunk_id": "2e0146a72ecea456fe1a17e8be47b3116019cb27",
      "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/",
      "title": "Cloud Design Patterns - Azure Architecture Center",
      "text": "rational Excellence Static Content Hosting Deploy static content to a cloud-based storage service for direct client delivery. - Cost Optimization Strangler Fig Incrementally migrate a legacy system by gradually replacing pieces of functionality with new applications and services. - Reliability - Cost Optimization - Operational Excellence Throttling Control the consumption of resources from applications, tenants, or services. - Reliability - Security - Cost Optimization - Performance Efficiency Valet Key Use a token or key to provide clients with restricted, direct access to a specific resource or service. - Security - Cost Optimization - Performance Efficiency",
      "chunk_index": 5
    },
    {
      "chunk_id": "f734eaaaf1f01b3cd53023854fdbf751edce54d0",
      "url": "https://c4model.com/",
      "title": "Home | C4 model",
      "text": "The C4 model for visualising software architecture The C4 model is an easy to learn, developer friendly approach to software architecture diagramming: A set of hierarchical abstractions - software systems , containers , components , and code . A set of hierarchical diagrams - system context , containers , components , and code . An additional set of supporting diagrams - system landscape , dynamic , and deployment . Notation independent . Tooling independent . Visualising software architecture with the C4 model Recorded at \"Agile on the Beach 2019\", July 2019 The C4 model Simon Brown About this website This is the official website for the “C4 model for visualising software architecture”, written by its creator Simon Brown .",
      "chunk_index": 0
    },
    {
      "chunk_id": "099363a08a2bc17be9730c57251e87fe6ee853ec",
      "url": "https://aws.amazon.com/architecture/well-architected/",
      "title": "AWS Well-Architected",
      "text": "AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for a variety of applications and workloads. Built around six pillars—operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability—AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures and implement scalable designs. The AWS Well-Architected Framework includes domain-specific lenses, hands-on labs , and the AWS Well-Architected Tool. The AWS Well-Architected Tool , available at no cost in the AWS Management Console , provides a mechanism for regularly evaluating workloads, identifying high-risk issues, and recording improvements. AWS also provides access to an environment of hundreds of members in the AWS Well-Architected Partner Program. Engage a partner in your area to help analyze and review your applications.",
      "chunk_index": 0
    },
    {
      "chunk_id": "7de048d09c090f42107c311492cd6c0bad9469a7",
      "url": "https://martinfowler.com/architecture/",
      "title": "Software Architecture Guide",
      "text": "Graphical User Interfaces provide a rich interaction between the user and a software system. Such richness is complex to manage, so it's important to contain that complexity with a thoughtful architecture. The Forms and Controls pattern works well for systems with a simple flow, but as it breaks down under the weight of greater complexity, most people turn to “Model-View-Controller” (MVC). Sadly MVC is one of the most misunderstood architectural patterns around, and systems using that name display a range of important differences, sometimes described under names like Application Model, Model-View-Presenter, Presentation Model, MVVM, and the like. The best way to think of MVC is as set of principles including the separation of presentation from domain logic and synchronizing presentation state through events (the observer pattern).",
      "chunk_index": 0
    },
    {
      "chunk_id": "a4228037dcfacc7a3d692f13055f266e6be0c5ee",
      "url": "https://raft.github.io/",
      "title": "Raft Consensus Algorithm",
      "text": "What is Raft? Raft is a consensus algorithm that is designed to be easy to understand. It's equivalent to Paxos in fault-tolerance and performance. The difference is that it's decomposed into relatively independent subproblems, and it cleanly addresses all major pieces needed for practical systems. We hope Raft will make consensus available to a wider audience, and that this wider audience will be able to develop a variety of higher quality consensus-based systems than are available today. Hold on—what is consensus? Consensus is a fundamental problem in fault-tolerant distributed systems. Consensus involves multiple servers agreeing on values. Once they reach a decision on a value, that decision is final. Typical consensus algorithms make progress when any majority of their servers is available; for example, a cluster of 5 servers can continue to operate even if 2 servers fail. If more servers fail, they stop making progress (but will never return an incorrect result). Consensus typically arises in the context of replicated state machines, a general approach to building fault-tolerant systems. Each server has a state machine and a log. The state machine is the component that we want to make fault-tolerant, such as a hash table. It will appear to clients that they are interacting with a single, reliable state machine, even if a minority of the servers in the cluster fail. Each s",
      "chunk_index": 0
    },
    {
      "chunk_id": "d371047194185f177a635b8be7fe6235bb514ddb",
      "url": "https://raft.github.io/",
      "title": "Raft Consensus Algorithm",
      "text": "omponent that we want to make fault-tolerant, such as a hash table. It will appear to clients that they are interacting with a single, reliable state machine, even if a minority of the servers in the cluster fail. Each state machine takes as input commands from its log. In our hash table example, the log would include commands like set x to 3 . A consensus algorithm is used to agree on the commands in the servers' logs. The consensus algorithm must ensure that if any state machine applies set x to 3 as the n th command, no other state machine will ever apply a different n th command. As a result, each state machine processes the same series of commands and thus produces the same series of results and arrives at the same series of states.",
      "chunk_index": 1
    },
    {
      "chunk_id": "e2d132ba45c988a07cf89f9d57d12c3de6f3a490",
      "url": "https://refactoring.guru/design-patterns",
      "title": "Design Patterns",
      "text": "Design Patterns Design patterns are typical solutions to common problems in software design. Each pattern is like a blueprint that you can customize to solve a particular design problem in your code. Catalog of patterns List of 22 classic design patterns, grouped by their intent. Benefits of patterns Patterns are a toolkit of solutions to common problems in software design. They define a common language that helps your team communicate more efficiently. Classification Design patterns differ by their complexity, level of detail and scale of applicability. In addition, they can be categorized by their intent and divided into three groups. History of patterns Who invented patterns and when? Can you use patterns outside software development? How do you do that? Criticism of patterns Are patterns as good as advertised? Is it always possible to use them? Can patterns sometimes be harmful? Dive Into Design Patterns Check out our ebook on design patterns and principles. It's available in PDF/ePUB/MOBI formats and includes the archive with code examples in Java, C#, C++, PHP, Python, Ruby, Go, Swift, & TypeScript.",
      "chunk_index": 0
    },
    {
      "chunk_id": "2b2279c93c493ba8b595f7d73f064f3d191a84c7",
      "url": "https://jepsen.io/consistency",
      "title": "Consistency",
      "text": "Jepsen analyzes the safety properties of distributed systemsâmost notably, identifying violations of consistency models. But what are consistency models? What phenomena do they allow? What kind of consistency does a given program really need? In this reference guide, we provide basic definitions, intuitive explanations, and theoretical underpinnings of various consistency models, as well as links to the literature. We aim to make consistency properties accessible for industry practitioners, academics, and enthusiasts. A consistency model is a safety property which declares what a system can do. Formally, a consistency model defines a set of histories that a system can legally execute. For instance, the model known as Serializability guarantees that every legal history must be equivalent to a totally ordered execution. Consistency models are often defined in terms of proscribed phenomena : specific patterns of operations. For example, G1a (Aborted Read) occurs when a transaction observes a write performed by a different, aborted transaction. Consistency models and phenomena are often defined in terms of dependencies between different operations. For example, we say that transaction T2 write-read depends on transaction T1 if T2 reads some value that T1 wrote. These dependencies form a graph. Preventing specific cycles in that graph gives rise to different consistency models.",
      "chunk_index": 0
    },
    {
      "chunk_id": "1ac3c1cb390b6a04fad71970046b8abfc9dfc355",
      "url": "https://refactoring.guru/refactoring",
      "title": "Refactoring: clean your code",
      "text": "Refactoring Refactoring is a systematic process of improving code without creating new functionality that can transform a mess into clean code and simple design. Dirty Code Dirty code is result of inexperience multiplied by tight deadlines, mismanagement, and nasty shortcuts taken during the development process. Clean Code Clean code is code that is easy to read, understand and maintain. Clean code makes software development predictable and increases the quality of a resulting product. Refactoring Process Performing refactoring step-by-step and running tests after each change are key elements of refactoring that make it predictable and safe. Code Smells Code smells are indicators of problems that can be addressed during refactoring. Code smells are easy to spot and fix, but they may be just symptoms of a deeper problem with code. Refactoring Techniques Refactoring techniques describe actual refactoring steps. Most refactoring techniques have their pros and cons. Therefore, each refactoring should be properly motivated and applied with caution. Premium Course 21 code smells, 66 refactorings Interactive examples in Java/C#/PHP No time limits. Study at your own pace",
      "chunk_index": 0
    },
    {
      "chunk_id": "47e6f091be646b15804a51339769be128f828c54",
      "url": "https://12factor.net/",
      "title": "The Twelve-Factor App",
      "text": "Introduction In the modern era, software is commonly delivered as a service: called web apps , or software-as-a-service . The twelve-factor app is a methodology for building software-as-a-service apps that: Use declarative formats for setup automation, to minimize time and cost for new developers joining the project; Have a clean contract with the underlying operating system, offering maximum portability between execution environments; Are suitable for deployment on modern cloud platforms , obviating the need for servers and systems administration; Minimize divergence between development and production, enabling continuous deployment for maximum agility; And can scale up without significant changes to tooling, architecture, or development practices. The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc). Background The contributors to this document have been directly involved in the development and deployment of hundreds of apps, and indirectly witnessed the development, operation, and scaling of hundreds of thousands of apps via our work on the Heroku platform. This document synthesizes all of our experience and observations on a wide variety of software-as-a-service apps in the wild. It is a triangulation on ideal practices for app development, paying part",
      "chunk_index": 0
    },
    {
      "chunk_id": "97d17ef3f818a961e8d5fb585377586789b60578",
      "url": "https://12factor.net/",
      "title": "The Twelve-Factor App",
      "text": "the Heroku platform. This document synthesizes all of our experience and observations on a wide variety of software-as-a-service apps in the wild. It is a triangulation on ideal practices for app development, paying particular attention to the dynamics of the organic growth of an app over time, the dynamics of collaboration between developers working on the app’s codebase, and avoiding the cost of software erosion . Our motivation is to raise awareness of some systemic problems we’ve seen in modern application development, to provide a shared vocabulary for discussing those problems, and to offer a set of broad conceptual solutions to those problems with accompanying terminology. The format is inspired by Martin Fowler’s books Patterns of Enterprise Application Architecture and Refactoring . Who should read this document? Any developer building applications which run as a service. Ops engineers who deploy or manage such applications. The Twelve Factors One codebase tracked in revision control, many deploys Explicitly declare and isolate dependencies Store config in the environment Treat backing services as attached resources Strictly separate build and run stages Execute the app as one or more stateless processes Export services via port binding Scale out via the process model Maximize robustness with fast startup and graceful shutdown Keep development, staging, and production ",
      "chunk_index": 1
    },
    {
      "chunk_id": "084d18cf63df8b522b5edb464ffe7aa5eba4dd4c",
      "url": "https://12factor.net/",
      "title": "The Twelve-Factor App",
      "text": "s Execute the app as one or more stateless processes Export services via port binding Scale out via the process model Maximize robustness with fast startup and graceful shutdown Keep development, staging, and production as similar as possible Treat logs as event streams Run admin/management tasks as one-off processes",
      "chunk_index": 2
    },
    {
      "chunk_id": "ed40e43278f37ec2b102509391667d16b229f110",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": "Last reviewed 2024-10-11 UTC To view all of the content in the Well-Architected Framework on a single page or to to get a PDF output of the content, see View on one page . The Well-Architected Framework provides recommendations to help architects, developers, administrators, and other cloud practitioners design and operate a cloud topology that's secure, efficient, resilient, high-performing, and cost-effective. A cross-functional team of experts at Google validates the recommendations in the Well-Architected Framework. The team curates the Well-Architected Framework to reflect the expanding capabilities of Google Cloud, industry best practices, community knowledge, and feedback from you. For a summary of the significant changes to the Well-Architected Framework, see What's new . The Well-Architected Framework is relevant to applications built for the cloud and for workloads migrated from on-premises to Google Cloud, hybrid cloud deployments, and multi-cloud environments. Well-Architected Framework pillars and perspectives The recommendations in the Well-Architected Framework are organized into pillars and perspectives, as shown in the following diagram. A pillar in the Well-Architected Framework provides principles and recommendations for a specific non-functional focus area: security, reliability, performance, cost, or operations. A perspective in the Well-Architected Framewo",
      "chunk_index": 0
    },
    {
      "chunk_id": "5af332ac6a15e824534666082916bd9c9aaeda73",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": "r in the Well-Architected Framework provides principles and recommendations for a specific non-functional focus area: security, reliability, performance, cost, or operations. A perspective in the Well-Architected Framework is a cross-pillar view of recommendations for a specific technology or industry. The recommendations in a perspective align with the general principles and recommendations in the pillars. For example, the financial services industry (FSI) perspective recommends a disaster recovery strategy that meets regulatory requirements for data residency. This FSI-specific recommendation aligns with the reliability pillar's principle about realistic targets, because the data residency requirements influence the choice of failover region and, consequently, the recovery objectives. Pillars construction Operational excellence Efficiently deploy, operate, monitor, and manage your cloud workloads. security Security, privacy, and compliance Maximize the security of your data and workloads in the cloud, design for privacy, and align with regulatory requirements and standards. restore Reliability Design and operate resilient and highly available workloads in the cloud. payment Cost optimization Maximize the business value of your investment in Google Cloud. speed Performance optimization Design and tune your cloud resources for optimal performance. Perspectives saved_search AI a",
      "chunk_index": 1
    },
    {
      "chunk_id": "a86a4e2f51904225c9e42794effa8a90d272a6d6",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": "cloud. payment Cost optimization Maximize the business value of your investment in Google Cloud. speed Performance optimization Design and tune your cloud resources for optimal performance. Perspectives saved_search AI and ML A cross-pillar view of technology-specific recommendations for AI and ML workloads. saved_search Financial services industry (FSI) A cross-pillar view of industry-specific recommendations for FSI workloads. Core principles Before you explore the recommendations in each pillar of the Well-Architected Framework, review the following core principles: Design for change No system is static. The needs of its users, the goals of the team that builds the system, and the system itself are constantly changing. With the need for change in mind, build a development and production process that enables teams to regularly deliver small changes and get fast feedback on those changes. Consistently demonstrating the ability to deploy changes helps to build trust with stakeholders, including the teams responsible for the system, and the users of the system. Using DORA's software delivery metrics can help your team monitor the speed, ease, and safety of making changes to the system. Document your architecture When you start to move your workloads to the cloud or build your applications, lack of documentation about the system can be a major obstacle. Documentation is especiall",
      "chunk_index": 2
    },
    {
      "chunk_id": "a8ac3644b29ecb6fd1bfbff0a40e3e39b03123f7",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": " changes to the system. Document your architecture When you start to move your workloads to the cloud or build your applications, lack of documentation about the system can be a major obstacle. Documentation is especially important for correctly visualizing the architecture of your current deployments. Quality documentation isn't achieved by producing a specific amount of documentation, but by how clear content is, how useful it is, and how it's maintained as the system changes. A properly documented cloud architecture establishes a common language and standards, which enable cross-functional teams to communicate and collaborate effectively. The documentation also provides the information that's necessary to identify and guide future design decisions. Documentation should be written with your use cases in mind, to provide context for the design decisions. Over time, your design decisions will evolve and change. The change history provides the context that your teams require to align initiatives, avoid duplication, and measure performance changes effectively over time. Change logs are particularly valuable when you onboard a new cloud architect who is not yet familiar with your current design, strategy, or history. Analysis by DORA has found a clear link between documentation quality and organizational performance — the organization's ability to meet their performance and profit",
      "chunk_index": 3
    },
    {
      "chunk_id": "ad117e415fe373a6efda572f583b4504e32d6b0f",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": "iliar with your current design, strategy, or history. Analysis by DORA has found a clear link between documentation quality and organizational performance — the organization's ability to meet their performance and profitability goals. Simplify your design and use fully managed services Simplicity is crucial for design. If your architecture is too complex to understand, it will be difficult to implement the design and manage it over time. Where feasible, use fully managed services to minimize the risks, time, and effort associated with managing and maintaining baseline systems. If you're already running your workloads in production, test with managed services to see how they might help to reduce operational complexities. If you're developing new workloads, then start simple, establish a minimal viable product (MVP), and resist the urge to over-engineer. You can identify exceptional use cases, iterate, and improve your systems incrementally over time. Decouple your architecture Research from DORA shows that architecture is an important predictor for achieving continuous delivery. Decoupling is a technique that's used to separate your applications and service components into smaller components that can operate independently. For example, you might separate a monolithic application stack into individual service components. In a loosely coupled architecture, an application can run i",
      "chunk_index": 4
    },
    {
      "chunk_id": "fe59a4df160475d6f6c214a199781ebe67a12306",
      "url": "https://cloud.google.com/architecture/framework",
      "title": "Google Cloud Well-Architected Framework",
      "text": "nents into smaller components that can operate independently. For example, you might separate a monolithic application stack into individual service components. In a loosely coupled architecture, an application can run its functions independently, regardless of the various dependencies. A decoupled architecture gives you increased flexibility to do the following: Apply independent upgrades. Enforce specific security controls. Establish reliability goals for each subsystem. Monitor health. Granularly control performance and cost parameters. You can start the decoupling process early in your design phase or incorporate it as part of your system upgrades as you scale. Use a stateless architecture A stateless architecture can increase both the reliability and scalability of your applications. Stateful applications rely on various dependencies to perform tasks, such as local caching of data. Stateful applications often require additional mechanisms to capture progress and restart gracefully. Stateless applications can perform tasks without significant local dependencies by using shared storage or cached services. A stateless architecture enables your applications to scale up quickly with minimum boot dependencies. The applications can withstand hard restarts, have lower downtime, and provide better performance for end users.",
      "chunk_index": 5
    },
    {
      "chunk_id": "71c074cfd65477740ca093a71fa7f1a5b693af06",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "OpenAPI Specification Version 3.1.1 The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in BCP 14 RFC2119 RFC8174 when, and only when, they appear in all capitals, as shown here. This document is licensed under The Apache License, Version 2.0 . Introduction The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to HTTP APIs which allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic. An OpenAPI Description can then be used by documentation generation tools to display the API, code generation tools to generate servers and clients in various programming languages, testing tools, and many other use cases. For examples of OpenAPI usage and additional documentation, please visit [[?OpenAPI-Learn]]. For extension registries and other specifications published by the OpenAPI Initiative, as well as the authoritative rendering of this specification, please visit spec.openapis.org . Definitions OpenAPI Description An OpenAPI Description (OAD) formally describes the sur",
      "chunk_index": 0
    },
    {
      "chunk_id": "55450a729eb37b193516fcccbf57a1ae5a7788be",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " published by the OpenAPI Initiative, as well as the authoritative rendering of this specification, please visit spec.openapis.org . Definitions OpenAPI Description An OpenAPI Description (OAD) formally describes the surface of an API and its semantics. It is composed of an entry document , which must be an OpenAPI Document, and any/all of its referenced documents. An OAD uses and conforms to the OpenAPI Specification, and MUST contain at least one paths field, components field, or webhooks field. OpenAPI Document An OpenAPI Document is a single JSON or YAML document that conforms to the OpenAPI Specification. An OpenAPI Document compatible with OAS 3.*.* contains a required openapi field which designates the version of the OAS that it uses. Schema A \"schema\" is a formal description of syntax and structure. This document serves as the schema for the OpenAPI Specification format; a non-authoritative JSON Schema based on this document is also provided on spec.openapis.org for informational purposes. This specification also uses schemas in the form of the Schema Object . Object When capitalized, the word \"Object\" refers to any of the Objects that are named by section headings in this document. Path Templating Path templating refers to the usage of template expressions, delimited by curly braces ( {} ), to mark a section of a URL path as replaceable using path parameters. Each temp",
      "chunk_index": 1
    },
    {
      "chunk_id": "0e83d58510cc4044287720957839f99cb444ddc6",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "on headings in this document. Path Templating Path templating refers to the usage of template expressions, delimited by curly braces ( {} ), to mark a section of a URL path as replaceable using path parameters. Each template expression in the path MUST correspond to a path parameter that is included in the Path Item itself and/or in each of the Path Item's Operations . An exception is if the path item is empty, for example due to ACL constraints, matching path parameters are not required. The value for these path parameters MUST NOT contain any unescaped \"generic syntax\" characters described by RFC3986 : forward slashes ( / ), question marks ( ? ), or hashes ( # ). Media type definitions are spread across several resources. The media type definitions SHOULD be in compliance with RFC6838 . Some examples of possible media type definitions: text/plain; charset=utf-8 application/json application/vnd.github+json application/vnd.github.v3+json application/vnd.github.v3.raw+json application/vnd.github.v3.text+json application/vnd.github.v3.html+json application/vnd.github.v3.full+json application/vnd.github.v3.diff application/vnd.github.v3.patch HTTP Status Codes The HTTP Status Codes are used to indicate the status of the executed operation. Status codes SHOULD be selected from the available status codes registered in the IANA Status Code Registry . Case Sensitivity As most field na",
      "chunk_index": 2
    },
    {
      "chunk_id": "a9332628e810ff4ddb6ae8296878093089f598c9",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "HTTP Status Codes are used to indicate the status of the executed operation. Status codes SHOULD be selected from the available status codes registered in the IANA Status Code Registry . Case Sensitivity As most field names and values in the OpenAPI Specification are case-sensitive, this document endeavors to call out any case-insensitive names and values. However, the case sensitivity of field names and values that map directly to HTTP concepts follow the case sensitivity rules of HTTP, even if this document does not make a note of every concept. Undefined and Implementation-Defined Behavior This specification deems certain situations to have either undefined or implementation-defined behavior. Behavior described as undefined is likely, at least in some circumstances, to result in outcomes that contradict the specification. This description is used when detecting the contradiction is impossible or impractical. Implementations MAY support undefined scenarios for historical reasons, including ambiguous text in prior versions of the specification. This support might produce correct outcomes in many cases, but relying on it is NOT RECOMMENDED as there is no guarantee that it will work across all tools or with future specification versions, even if those versions are otherwise strictly compatible with this one. Behavior described as implementation-defined allows implementations to ",
      "chunk_index": 3
    },
    {
      "chunk_id": "9ab72c50833d400f490011710b24a92df8a7abab",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ee that it will work across all tools or with future specification versions, even if those versions are otherwise strictly compatible with this one. Behavior described as implementation-defined allows implementations to choose which of several different-but-compliant approaches to a requirement to implement. This documents ambiguous requirements that API description authors are RECOMMENDED to avoid in order to maximize interoperability. Unlike undefined behavior, it is safe to rely on implementation-defined behavior if and only if it can be guaranteed that all relevant tools support the same behavior. Specification Versions The OpenAPI Specification is versioned using a major . minor . patch versioning scheme. The major . minor portion of the version string (for example 3.1 ) SHALL designate the OAS feature set. .patch versions address errors in, or provide clarifications to, this document, not the feature set. Tooling which supports OAS 3.1 SHOULD be compatible with all OAS 3.1.* versions. The patch version SHOULD NOT be considered by tooling, making no distinction between 3.1.0 and 3.1.1 for example. Occasionally, non-backwards compatible changes may be made in minor versions of the OAS where impact is believed to be low relative to the benefit provided. Format An OpenAPI Document that conforms to the OpenAPI Specification is itself a JSON object, which may be represented eit",
      "chunk_index": 4
    },
    {
      "chunk_id": "d301f46fe759357677aa421bd9f367e2eb20e396",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "n minor versions of the OAS where impact is believed to be low relative to the benefit provided. Format An OpenAPI Document that conforms to the OpenAPI Specification is itself a JSON object, which may be represented either in JSON or YAML format. For example, if a field has an array value, the JSON array representation will be used: { \"field\": [1, 2, 3] } All field names in the specification are case sensitive . This includes all fields that are used as keys in a map, except where explicitly noted that keys are case insensitive . The schema exposes two types of fields: fixed fields , which have a declared name, and patterned fields , which have a declared pattern for the field name. Patterned fields MUST have unique names within the containing object. In order to preserve the ability to round-trip between YAML and JSON formats, YAML version 1.2 is RECOMMENDED along with some additional constraints: Tags MUST be limited to those allowed by YAML's JSON schema ruleset , which defines a subset of the YAML syntax and is unrelated to [[JSON-Schema-2020-12|JSON Schema]]. Keys used in YAML maps MUST be limited to a scalar string, as defined by the YAML Failsafe schema ruleset . Note: While APIs may be described by OpenAPI Descriptions in either YAML or JSON format, the API request and response bodies and other content are not required to be JSON or YAML. OpenAPI Description Structure ",
      "chunk_index": 5
    },
    {
      "chunk_id": "916e42997a41aeed89544e525680a0a7b5257fff",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ruleset . Note: While APIs may be described by OpenAPI Descriptions in either YAML or JSON format, the API request and response bodies and other content are not required to be JSON or YAML. OpenAPI Description Structure An OpenAPI Description (OAD) MAY be made up of a single JSON or YAML document or be divided into multiple, connected parts at the discretion of the author. In the latter case, Reference Object , Path Item Object and Schema Object $ref fields, as well as the Link Object operationRef field, and the URI form of the Discriminator Object mapping field, are used to identify the referenced elements. In a multi-document OAD, the document containing the OpenAPI Object where parsing begins is known as that OAD's entry document . It is RECOMMENDED that the entry document of an OAD be named: openapi.json or openapi.yaml . Parsing Documents In order to properly handle Schema Objects , OAS 3.1 inherits the parsing requirements of JSON Schema Specification Draft 2020-12 , with appropriate modifications regarding base URIs as specified in Relative References In URIs . This includes a requirement to parse complete documents before deeming a Schema Object reference to be unresolvable, in order to detect keywords that might provide the reference target or impact the determination of the appropriate base URI. Implementations MAY support complete-document parsing in any of the follo",
      "chunk_index": 6
    },
    {
      "chunk_id": "1b66deb2b114c9bd02fc68a9182da36777b9aa87",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "e to be unresolvable, in order to detect keywords that might provide the reference target or impact the determination of the appropriate base URI. Implementations MAY support complete-document parsing in any of the following ways: Detecting OpenAPI or JSON Schema documents using media types Detecting OpenAPI documents through the root openapi field Detecting JSON Schema documents through detecting keywords or otherwise successfully parsing the document in accordance with the JSON Schema specification Detecting a document containing a referenceable Object at its root based on the expected type of the reference Allowing users to configure the type of documents that might be loaded due to a reference to a non-root Object Implementations that parse referenced fragments of OpenAPI content without regard for the content of the rest of the containing document will miss keywords that change the meaning and behavior of the reference target. In particular, failing to take into account keywords that change the base URI introduces security risks by causing references to resolve to unintended URIs, with unpredictable results. While some implementations support this sort of parsing due to the requirements of past versions of this specification, in version 3.1, the result of parsing fragments in isolation is undefined and likely to contradict the requirements of this specification. While it i",
      "chunk_index": 7
    },
    {
      "chunk_id": "d8f754e528d1d23d35316ec8ca977dd4959b1a0e",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "arsing due to the requirements of past versions of this specification, in version 3.1, the result of parsing fragments in isolation is undefined and likely to contradict the requirements of this specification. While it is possible to structure certain OpenAPI Descriptions to ensure that they will behave correctly when references are parsed as isolated fragments, depending on this is NOT RECOMMENDED. This specification does not explicitly enumerate the conditions under which such behavior is safe and provides no guarantee for continued safety in any future versions of the OAS. A special case of parsing fragments of OAS content would be if such fragments are embedded in another format, referred to as an embedding format with respect to the OAS. Note that the OAS itself is an embedding format with respect to JSON Schema, which is embedded as Schema Objects. It is the responsibility of an embedding format to define how to parse embedded content, and OAS implementations that do not document support for an embedding format cannot be expected to parse embedded OAS content correctly. Structural Interoperability JSON or YAML objects within an OAD are interpreted as specific Objects (such as Operation Objects , Response Objects , Reference Objects , etc.) based on their context. Depending on how references are arranged, a given JSON or YAML object can be interpreted in multiple different",
      "chunk_index": 8
    },
    {
      "chunk_id": "299421b93f53695d5f0b6ef2d274d4893622007a",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "fic Objects (such as Operation Objects , Response Objects , Reference Objects , etc.) based on their context. Depending on how references are arranged, a given JSON or YAML object can be interpreted in multiple different contexts: As the root object of the entry document , which is always interpreted as an OpenAPI Object As the Object type implied by its parent Object within the document As a reference target, with the Object type matching the reference source's context If the same JSON/YAML object is parsed multiple times and the respective contexts require it to be parsed as different Object types, the resulting behavior is implementation defined , and MAY be treated as an error if detected. An example would be referencing an empty Schema Object under #/components/schemas where a Path Item Object is expected, as an empty object is valid for both types. For maximum interoperability, it is RECOMMENDED that OpenAPI Description authors avoid such scenarios. Resolving Implicit Connections Several features of this specification require resolution of non-URI-based connections to some other part of the OpenAPI Description (OAD). These connections are unambiguously resolved in single-document OADs, but the resolution process in multi-document OADs is implementation-defined , within the constraints described in this section. In some cases, an unambiguous URI-based alternative is availa",
      "chunk_index": 9
    },
    {
      "chunk_id": "8db5461b577354b929989229a53ec2e200c8ca8c",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "olved in single-document OADs, but the resolution process in multi-document OADs is implementation-defined , within the constraints described in this section. In some cases, an unambiguous URI-based alternative is available, and OAD authors are RECOMMENDED to always use the alternative: A fifth implicit connection involves appending the templated URL paths of the Paths Object to the appropriate Server Object 's url field. This is unambiguous because only the entry document's Paths Object contributes URLs to the described API. It is RECOMMENDED to consider all Operation Objects from all parsed documents when resolving any Link Object operationId . This requires parsing all referenced documents prior to determining an operationId to be unresolvable. The implicit connections in the Security Requirement Object and Discriminator Object rely on the component name , which is the name of the property holding the component in the appropriately typed sub-object of the Components Object. For example, the component name of the Schema Object at #/components/schemas/Foo is Foo . The implicit connection of tags in the Operation Object uses the name field of Tag Objects, which (like the Components Object) are found under the root OpenAPI Object. This means resolving component names and tag names both depend on starting from the correct OpenAPI Object. For resolving component and tag name conne",
      "chunk_index": 10
    },
    {
      "chunk_id": "d14694ab28effce933dfb8875d7a6bddcd833271",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "(like the Components Object) are found under the root OpenAPI Object. This means resolving component names and tag names both depend on starting from the correct OpenAPI Object. For resolving component and tag name connections from a referenced (non-entry) document, it is RECOMMENDED that tools resolve from the entry document, rather than the current document. This allows Security Scheme Objects and Tag Objects to be defined next to the API's deployment information (the top-level array of Server Objects), and treated as an interface for referenced documents to access. The interface approach can also work for Discriminator Objects and Schema Objects, but it is also possible to keep the Discriminator Object's behavior within a single document using the relative URI-reference syntax of mapping . There are no URI-based alternatives for the Security Requirement Object or for the Operation Object's tags field. These limitations are expected to be addressed in a future release. See Appendix F: Resolving Security Requirements in a Referenced Document for an example of the possible resolutions, including which one is recommended by this section. The behavior for Discrimator Object non-URI mappings and for the Operation Object's tags field operate on the same principles. Note that no aspect of implicit connection resolution changes how URIs are resolved , or restricts their possible targ",
      "chunk_index": 11
    },
    {
      "chunk_id": "628fcff1b9b65db1343c71551034775c0a5e1f8d",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "tor Object non-URI mappings and for the Operation Object's tags field operate on the same principles. Note that no aspect of implicit connection resolution changes how URIs are resolved , or restricts their possible targets. Data Types Data types in the OAS are based on the types defined by the JSON Schema Validation Specification Draft 2020-12 : \"null\", \"boolean\", \"object\", \"array\", \"number\", \"string\", or \"integer\". Models are defined using the Schema Object , which is a superset of the JSON Schema Specification Draft 2020-12. JSON Schema keywords and format values operate on JSON \"instances\" which may be one of the six JSON data types, \"null\", \"boolean\", \"object\", \"array\", \"number\", or \"string\", with certain keywords and formats only applying to a specific type. For example, the pattern keyword and the date-time format only apply to strings, and treat any instance of the other five types as automatically valid. This means JSON Schema keywords and formats do NOT implicitly require the expected type. Use the type keyword to explicitly constrain the type. Note that the type keyword allows \"integer\" as a value for convenience, but keyword and format applicability does not recognize integers as being of a distinct JSON type from other numbers because [[RFC7159|JSON]] itself does not make that distinction. Since there is no distinct JSON integer type, JSON Schema defines integers m",
      "chunk_index": 12
    },
    {
      "chunk_id": "84da86169d0449e3a7a9c5b658c26201218f476b",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "does not recognize integers as being of a distinct JSON type from other numbers because [[RFC7159|JSON]] itself does not make that distinction. Since there is no distinct JSON integer type, JSON Schema defines integers mathematically. This means that both 1 and 1.0 are equivalent , and are both considered to be integers. Data Type Format As defined by the JSON Schema Validation specification , data types can have an optional modifier keyword: format . As described in that specification, format is treated as a non-validating annotation by default; the ability to validate format varies across implementations. The OpenAPI Initiative also hosts a Format Registry for formats defined by OAS users and other specifications. Support for any registered format is strictly OPTIONAL, and support for one registered format does not imply support for any others. Types that are not accompanied by a format keyword follow the type definition in the JSON Schema. Tools that do not recognize a specific format MAY default back to the type alone, as if the format is not specified. For the purpose of JSON Schema validation , each format should specify the set of JSON data types for which it applies. In this registry, these types are shown in the \"JSON Data Type\" column. The formats defined by the OAS are: format JSON Data Type Comments int32 number signed 32 bits int64 number signed 64 bits (a.k.a long",
      "chunk_index": 13
    },
    {
      "chunk_id": "669895e44c26d903bcc4d067db7c4d6c29e281a5",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "h it applies. In this registry, these types are shown in the \"JSON Data Type\" column. The formats defined by the OAS are: format JSON Data Type Comments int32 number signed 32 bits int64 number signed 64 bits (a.k.a long) float number double number password string A hint to obscure the value. As noted under Data Type , both type: number and type: integer are considered to be numbers in the data model. Working with Binary Data The OAS can describe either raw or encoded binary data. raw binary is used where unencoded binary data is allowed, such as when sending a binary payload as the entire HTTP message body, or as part of a multipart/* payload that allows binary parts encoded binary is used where binary data is embedded in a text-only format such as application/json or application/x-www-form-urlencoded (either as a message body or in the URL query string). In the following table showing how to use Schema Object keywords for binary data, we use image/png as an example binary media type. Any binary media type, including application/octet-stream , is sufficient to indicate binary content. Keyword Raw Encoded Comments type omit string raw binary is outside of type contentMediaType image/png image/png can sometimes be omitted if redundant (see below) contentEncoding omit base64 or base64url other encodings are allowed Note that the encoding indicated by contentEncoding , which infla",
      "chunk_index": 14
    },
    {
      "chunk_id": "2b1f3878bba5b9363407f30f15523fe2a26a02f9",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ontentMediaType image/png image/png can sometimes be omitted if redundant (see below) contentEncoding omit base64 or base64url other encodings are allowed Note that the encoding indicated by contentEncoding , which inflates the size of data in order to represent it as 7-bit ASCII text, is unrelated to HTTP's Content-Encoding header, which indicates whether and how a message body has been compressed and is applied after all content serialization described in this section has occurred. Since HTTP allows unencoded binary message bodies, there is no standardized HTTP header for indicating base64 or similar encoding of an entire message body. Using a contentEncoding of base64url ensures that URL encoding (as required in the query string and in message bodies of type application/x-www-form-urlencoded ) does not need to further encode any part of the already-encoded binary data. The contentMediaType keyword is redundant if the media type is already set: If the Schema Object will be processed by a non-OAS-aware JSON Schema implementation, it may be useful to include contentMediaType even if it is redundant. However, if contentMediaType contradicts a relevant Media Type Object or Encoding Object, then contentMediaType SHALL be ignored. The maxLength keyword MAY be used to set an expected upper bound on the length of a streaming payload. The keyword can be applied to either string data, ",
      "chunk_index": 15
    },
    {
      "chunk_id": "cdc500c0c3f6d020aac56368a0108909ccdc7df2",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "Object or Encoding Object, then contentMediaType SHALL be ignored. The maxLength keyword MAY be used to set an expected upper bound on the length of a streaming payload. The keyword can be applied to either string data, including encoded binary data, or to unencoded binary data. For unencoded binary, the length is the number of octets. Migrating binary descriptions from OAS 3.0 The following table shows how to migrate from OAS 3.0 binary data descriptions, continuing to use image/png as the example binary media type: OAS < 3.1 OAS 3.1 Comments type: string format: binary contentMediaType: image/png if redundant, can be omitted, often resulting in an empty Schema Object type: string format: byte type: string contentMediaType: image/png contentEncoding: base64 note that base64url can be used to avoid re-encoding the base64 string to be URL-safe Rich Text Formatting Throughout the specification description fields are noted as supporting CommonMark markdown formatting. Where OpenAPI tooling renders rich text it MUST support, at a minimum, markdown syntax as described by CommonMark 0.27 . Tooling MAY choose to ignore some CommonMark or extension features to address security concerns. While the framing of CommonMark 0.27 as a minimum requirement means that tooling MAY choose to implement extensions on top of it, note that any such extensions are by definition implementation-defined a",
      "chunk_index": 16
    },
    {
      "chunk_id": "761340334eb02ebad7713a5ce87c44a8a3afbf4e",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ecurity concerns. While the framing of CommonMark 0.27 as a minimum requirement means that tooling MAY choose to implement extensions on top of it, note that any such extensions are by definition implementation-defined and will not be interoperable. OpenAPI Description authors SHOULD consider how text using such extensions will be rendered by tools that offer only the minimum support. Relative References in API Description URIs URIs used as references within an OpenAPI Description, or to external documentation or other supplementary information such as a license, are resolved as identifiers , and described by this specification as URIs . As noted under Parsing Documents , this specification inherits JSON Schema Specification Draft 2020-12's requirements for loading documents and associating them with their expected URIs, which might not match their current location. This feature is used both for working in development or test environments without having to change the URIs, and for working within restrictive network configurations or security policies. Note that some URI fields are named url for historical reasons, but the descriptive text for those fields uses the correct \"URI\" terminology. Unless specified otherwise, all fields that are URIs MAY be relative references as defined by RFC3986 . Relative references in Schema Objects , including any that appear as $id values, use t",
      "chunk_index": 17
    },
    {
      "chunk_id": "3f66ef9e062443119b42fc114233008b96f53fde",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "he correct \"URI\" terminology. Unless specified otherwise, all fields that are URIs MAY be relative references as defined by RFC3986 . Relative references in Schema Objects , including any that appear as $id values, use the nearest parent $id as a Base URI, as described by JSON Schema Specification Draft 2020-12 . Relative URI references in other Objects, and in Schema Objects where no parent schema contains an $id , MUST be resolved using the referring document's base URI, which is determined in accordance with [[RFC3986]] Section 5.1.2 – 5.1.4 . In practice, this is usually the retrieval URI of the document, which MAY be determined based on either its current actual location or a user-supplied expected location. If a URI contains a fragment identifier, then the fragment should be resolved per the fragment resolution mechanism of the referenced document. If the representation of the referenced document is JSON or YAML, then the fragment identifier SHOULD be interpreted as a JSON-Pointer as per RFC6901 . Relative references in CommonMark hyperlinks are resolved in their rendered context, which might differ from the context of the API description. Relative References in API URLs API endpoints are by definition accessed as locations, and are described by this specification as URLs . Unless specified otherwise, all fields that are URLs MAY be relative references as defined by RFC39",
      "chunk_index": 18
    },
    {
      "chunk_id": "39df3d4a0b75c07a0614df5f5daf8b8c3585ef74",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ces in API URLs API endpoints are by definition accessed as locations, and are described by this specification as URLs . Unless specified otherwise, all fields that are URLs MAY be relative references as defined by RFC3986 . Unless specified otherwise, relative references are resolved using the URLs defined in the Server Object as a Base URL. Note that these themselves MAY be relative to the referring document. Schema This section describes the structure of the OpenAPI Description format. This text is the only normative description of the format. A JSON Schema is hosted on spec.openapis.org for informational purposes. If the JSON Schema differs from this section, then this section MUST be considered authoritative. In the following description, if a field is not explicitly REQUIRED or described with a MUST or SHALL, it can be considered OPTIONAL. OpenAPI Object This is the root object of the OpenAPI Description . Fixed Fields Field Name Type Description openapi string REQUIRED . This string MUST be the version number of the OpenAPI Specification that the OpenAPI Document uses. The openapi field SHOULD be used by tooling to interpret the OpenAPI Document. This is not related to the API info.version string. info Info Object REQUIRED . Provides metadata about the API. The metadata MAY be used by tooling as required. jsonSchemaDialect string The default value for the $schema keyword",
      "chunk_index": 19
    },
    {
      "chunk_id": "ef36167019d3fa7cb910dbae4a931fe35307b721",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "not related to the API info.version string. info Info Object REQUIRED . Provides metadata about the API. The metadata MAY be used by tooling as required. jsonSchemaDialect string The default value for the $schema keyword within Schema Objects contained within this OAS document. This MUST be in the form of a URI. servers [ Server Object ] An array of Server Objects, which provide connectivity information to a target server. If the servers field is not provided, or is an empty array, the default value would be a Server Object with a url value of / . paths Paths Object The available paths and operations for the API. webhooks Map[ string , Path Item Object ] The incoming webhooks that MAY be received as part of this API and that the API consumer MAY choose to implement. Closely related to the callbacks feature, this section describes requests initiated other than by an API call, for example by an out of band registration. The key name is a unique string to refer to each webhook, while the (optionally referenced) Path Item Object describes a request that may be initiated by the API provider and the expected responses. An example is available. components Components Object An element to hold various Objects for the OpenAPI Description. security [ Security Requirement Object ] A declaration of which security mechanisms can be used across the API. The list of values includes alternative",
      "chunk_index": 20
    },
    {
      "chunk_id": "d14ca96fbf9de5e114b0ddb4f34e49127bbb9af1",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ject An element to hold various Objects for the OpenAPI Description. security [ Security Requirement Object ] A declaration of which security mechanisms can be used across the API. The list of values includes alternative Security Requirement Objects that can be used. Only one of the Security Requirement Objects need to be satisfied to authorize a request. Individual operations can override this definition. The list can be incomplete, up to being empty or absent. To make security explicitly optional, an empty security requirement ( {} ) can be included in the array. tags [ Tag Object ] A list of tags used by the OpenAPI Description with additional metadata. The order of the tags can be used to reflect on their order by the parsing tools. Not all tags that are used by the Operation Object must be declared. The tags that are not declared MAY be organized randomly or based on the tools' logic. Each tag name in the list MUST be unique. externalDocs External Documentation Object Additional external documentation. This object MAY be extended with Specification Extensions . Info Object The object provides metadata about the API. The metadata MAY be used by the clients if needed, and MAY be presented in editing or documentation generation tools for convenience. Fixed Fields Field Name Type Description title string REQUIRED . The title of the API. summary string A short summary of the AP",
      "chunk_index": 21
    },
    {
      "chunk_id": "5c0d56e81ef562872cb0a3556a27f9b20fe4d7d3",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "f needed, and MAY be presented in editing or documentation generation tools for convenience. Fixed Fields Field Name Type Description title string REQUIRED . The title of the API. summary string A short summary of the API. description string A description of the API. CommonMark syntax MAY be used for rich text representation. termsOfService string A URI for the Terms of Service for the API. This MUST be in the form of a URI. contact Contact Object The contact information for the exposed API. license License Object The license information for the exposed API. version string REQUIRED . The version of the OpenAPI Document (which is distinct from the OpenAPI Specification version or the version of the API being described or the version of the OpenAPI Description). This object MAY be extended with Specification Extensions . Info Object Example { \"title\": \"Example Pet Store App\", \"summary\": \"A pet store manager.\", \"description\": \"This is an example server for a pet store.\", \"termsOfService\": \"https://example.com/terms/\", \"contact\": { \"name\": \"API Support\", \"url\": \"https://www.example.com/support\", \"email\": \" [email protected] \" }, \"license\": { \"name\": \"Apache 2.0\", \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\" }, \"version\": \"1.0.1\" } title: Example Pet Store App summary: A pet store manager. description: This is an example server for a pet store. termsOfService: https://e",
      "chunk_index": 22
    },
    {
      "chunk_id": "ce07cf39545cf07851e5dc486591721753810fa8",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "rl\": \"https://www.apache.org/licenses/LICENSE-2.0.html\" }, \"version\": \"1.0.1\" } title: Example Pet Store App summary: A pet store manager. description: This is an example server for a pet store. termsOfService: https://example.com/terms/ contact: name: API Support url: https://www.example.com/support email: [email protected] license: name: Apache 2.0 url: https://www.apache.org/licenses/LICENSE-2.0.html version: 1.0.1 Contact information for the exposed API. Fixed Fields Field Name Type Description name string The identifying name of the contact person/organization. url string The URI for the contact information. This MUST be in the form of a URI. email string The email address of the contact person/organization. This MUST be in the form of an email address. This object MAY be extended with Specification Extensions . { \"name\": \"API Support\", \"url\": \"https://www.example.com/support\", \"email\": \" [email protected] \" } name: API Support url: https://www.example.com/support email: [email protected] License Object License information for the exposed API. Fixed Fields Field Name Type Description name string REQUIRED . The license name used for the API. identifier string An SPDX license expression for the API. The identifier field is mutually exclusive of the url field. url string A URI for the license used for the API. This MUST be in the form of a URI. The url field is mutually exclu",
      "chunk_index": 23
    },
    {
      "chunk_id": "a8790595fd5bbeced532d1771bf4752a9157095b",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "g An SPDX license expression for the API. The identifier field is mutually exclusive of the url field. url string A URI for the license used for the API. This MUST be in the form of a URI. The url field is mutually exclusive of the identifier field. This object MAY be extended with Specification Extensions . License Object Example { \"name\": \"Apache 2.0\", \"identifier\": \"Apache-2.0\" } name: Apache 2.0 identifier: Apache-2.0 Server Object An object representing a Server. Fixed Fields Field Name Type Description url string REQUIRED . A URL to the target host. This URL supports Server Variables and MAY be relative, to indicate that the host location is relative to the location where the document containing the Server Object is being served. Variable substitutions will be made when a variable is named in { braces } . description string An optional string describing the host designated by the URL. CommonMark syntax MAY be used for rich text representation. variables Map[ string , Server Variable Object ] A map between a variable name and its value. The value is used for substitution in the server's URL template. This object MAY be extended with Specification Extensions . Server Object Example A single server would be described as: { \"url\": \"https://development.gigantic-server.com/v1\", \"description\": \"Development server\" } url: https://development.gigantic-server.com/v1 description: De",
      "chunk_index": 24
    },
    {
      "chunk_id": "de460b752a44bfe8760ecef7de52b0a44af43d39",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " . Server Object Example A single server would be described as: { \"url\": \"https://development.gigantic-server.com/v1\", \"description\": \"Development server\" } url: https://development.gigantic-server.com/v1 description: Development server The following shows how multiple servers can be described, for example, at the OpenAPI Object's servers : { \"servers\": [ { \"url\": \"https://development.gigantic-server.com/v1\", \"description\": \"Development server\" }, { \"url\": \"https://staging.gigantic-server.com/v1\", \"description\": \"Staging server\" }, { \"url\": \"https://api.gigantic-server.com/v1\", \"description\": \"Production server\" } ] } servers: - url: https://development.gigantic-server.com/v1 description: Development server - url: https://staging.gigantic-server.com/v1 description: Staging server - url: https://api.gigantic-server.com/v1 description: Production server The following shows how variables can be used for a server configuration: { \"servers\": [ { \"url\": \"https://{username}.gigantic-server.com:{port}/{basePath}\", \"description\": \"The production API server\", \"variables\": { \"username\": { \"default\": \"demo\", \"description\": \"A user-specific subdomain. Use `demo` for a free sandbox environment.\" }, \"port\": { \"enum\": [\"8443\", \"443\"], \"default\": \"8443\" }, \"basePath\": { \"default\": \"v2\" } } } ] } servers: - url: https://{username}.gigantic-server.com:{port}/{basePath} description: The production",
      "chunk_index": 25
    },
    {
      "chunk_id": "89ff25f1e22cf7cbda959bd3c95c6aa38b00cffe",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "andbox environment.\" }, \"port\": { \"enum\": [\"8443\", \"443\"], \"default\": \"8443\" }, \"basePath\": { \"default\": \"v2\" } } } ] } servers: - url: https://{username}.gigantic-server.com:{port}/{basePath} description: The production API server variables: username: # note! no enum here means it is an open value default: demo description: A user-specific subdomain. Use `demo` for a free sandbox environment. port: enum: - '8443' - '443' default: '8443' basePath: # open meaning there is the opportunity to use special base paths as assigned by the provider, default is `v2` default: v2 Server Variable Object An object representing a Server Variable for server URL template substitution. Fixed Fields Field Name Type Description enum [ string ] An enumeration of string values to be used if the substitution options are from a limited set. The array MUST NOT be empty. default string REQUIRED . The default value to use for substitution, which SHALL be sent if an alternate value is not supplied. If the enum is defined, the value MUST exist in the enum's values. Note that this behavior is different from the Schema Object 's default keyword, which documents the receiver's behavior rather than inserting the value into the data. description string An optional description for the server variable. CommonMark syntax MAY be used for rich text representation. This object MAY be extended with Specification Exten",
      "chunk_index": 26
    },
    {
      "chunk_id": "d72ac7aba119aa2cd3dadb5f8079ad0fc053e310",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "than inserting the value into the data. description string An optional description for the server variable. CommonMark syntax MAY be used for rich text representation. This object MAY be extended with Specification Extensions . Components Object Holds a set of reusable objects for different aspects of the OAS. All objects defined within the Components Object will have no effect on the API unless they are explicitly referenced from outside the Components Object. Fixed Fields Field Name Type Description schemas Map[ string , Schema Object ] An object to hold reusable Schema Objects . responses Map[ string , Response Object | Reference Object ] An object to hold reusable Response Objects . parameters Map[ string , Parameter Object | Reference Object ] An object to hold reusable Parameter Objects . examples Map[ string , Example Object | Reference Object ] An object to hold reusable Example Objects . requestBodies Map[ string , Request Body Object | Reference Object ] An object to hold reusable Request Body Objects . headers Map[ string , Header Object | Reference Object ] An object to hold reusable Header Objects . securitySchemes Map[ string , Security Scheme Object | Reference Object ] An object to hold reusable Security Scheme Objects . links Map[ string , Link Object | Reference Object ] An object to hold reusable Link Objects . callbacks Map[ string , Callback Object | Refere",
      "chunk_index": 27
    },
    {
      "chunk_id": "a28c7c2f86e4eb3f7d17983b5438eaccb3f2dc31",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ct | Reference Object ] An object to hold reusable Security Scheme Objects . links Map[ string , Link Object | Reference Object ] An object to hold reusable Link Objects . callbacks Map[ string , Callback Object | Reference Object ] An object to hold reusable Callback Objects . pathItems Map[ string , Path Item Object ] An object to hold reusable Path Item Objects . This object MAY be extended with Specification Extensions . All the fixed fields declared above are objects that MUST use keys that match the regular expression: ^[a-zA-Z0-9\\.\\-_]+$ . Field Name Examples: User User_1 User_Name user-name my.org.User Components Object Example \"components\": { \"schemas\": { \"GeneralError\": { \"type\": \"object\", \"properties\": { \"code\": { \"type\": \"integer\", \"format\": \"int32\" }, \"message\": { \"type\": \"string\" } } }, \"Category\": { \"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\", \"format\": \"int64\" }, \"name\": { \"type\": \"string\" } } }, \"Tag\": { \"type\": \"object\", \"properties\": { \"id\": { \"type\": \"integer\", \"format\": \"int64\" }, \"name\": { \"type\": \"string\" } } } }, \"parameters\": { \"skipParam\": { \"name\": \"skip\", \"in\": \"query\", \"description\": \"number of items to skip\", \"required\": true, \"schema\": { \"type\": \"integer\", \"format\": \"int32\" } }, \"limitParam\": { \"name\": \"limit\", \"in\": \"query\", \"description\": \"max records to return\", \"required\": true, \"schema\" : { \"type\": \"integer\", \"format\": \"int32\"",
      "chunk_index": 28
    },
    {
      "chunk_id": "f87a823dd57eb1027788105f91ef138bcec197b3",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ed\": true, \"schema\": { \"type\": \"integer\", \"format\": \"int32\" } }, \"limitParam\": { \"name\": \"limit\", \"in\": \"query\", \"description\": \"max records to return\", \"required\": true, \"schema\" : { \"type\": \"integer\", \"format\": \"int32\" } } }, \"responses\": { \"NotFound\": { \"description\": \"Entity not found.\" }, \"IllegalInput\": { \"description\": \"Illegal input for operation.\" }, \"GeneralError\": { \"description\": \"General Error\", \"content\": { \"application/json\": { \"schema\": { \"$ref\": \"#/components/schemas/GeneralError\" } } } } }, \"securitySchemes\": { \"api_key\": { \"type\": \"apiKey\", \"name\": \"api-key\", \"in\": \"header\" }, \"petstore_auth\": { \"type\": \"oauth2\", \"flows\": { \"implicit\": { \"authorizationUrl\": \"https://example.org/api/oauth/dialog\", \"scopes\": { \"write:pets\": \"modify pets in your account\", \"read:pets\": \"read your pets\" } } } } } } components: schemas: GeneralError: type: object properties: code: type: integer format: int32 message: type: string Category: type: object properties: id: type: integer format: int64 name: type: string Tag: type: object properties: id: type: integer format: int64 name: type: string parameters: skipParam: name: skip in: query description: number of items to skip required: true schema: type: integer format: int32 limitParam: name: limit in: query description: max records to return required: true schema: type: integer format: int32 responses: NotFound: description: Entity ",
      "chunk_index": 29
    },
    {
      "chunk_id": "d8015d58d9981443d1778650f808fa0f6c36685a",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "to skip required: true schema: type: integer format: int32 limitParam: name: limit in: query description: max records to return required: true schema: type: integer format: int32 responses: NotFound: description: Entity not found. IllegalInput: description: Illegal input for operation. GeneralError: description: General Error content: application/json: schema: $ref: '#/components/schemas/GeneralError' securitySchemes: api_key: type: apiKey name: api-key in: header petstore_auth: type: oauth2 flows: implicit: authorizationUrl: https://example.org/api/oauth/dialog scopes: write:pets: modify pets in your account read:pets: read your pets Paths Object Holds the relative paths to the individual endpoints and their operations. The path is appended to the URL from the Server Object in order to construct the full URL. The Paths Object MAY be empty, due to Access Control List (ACL) constraints . Patterned Fields Field Pattern Type Description /{path} Path Item Object A relative path to an individual endpoint. The field name MUST begin with a forward slash ( / ). The path is appended (no relative URL resolution) to the expanded URL from the Server Object 's url field in order to construct the full URL. Path templating is allowed. When matching URLs, concrete (non-templated) paths would be matched before their templated counterparts. Templated paths with the same hierarchy but different t",
      "chunk_index": 30
    },
    {
      "chunk_id": "ddc3a8e2758f7245acb33f36e8e28898a80a13a9",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "rder to construct the full URL. Path templating is allowed. When matching URLs, concrete (non-templated) paths would be matched before their templated counterparts. Templated paths with the same hierarchy but different templated names MUST NOT exist as they are identical. In case of ambiguous matching, it's up to the tooling to decide which one to use. This object MAY be extended with Specification Extensions . Path Templating Matching Assuming the following paths, the concrete definition, /pets/mine , will be matched first if used: /pets/{petId} /pets/mine The following paths are considered identical and invalid: /pets/{petId} /pets/{name} The following may lead to ambiguous resolution: /{entity}/me /books/{id} Paths Object Example { \"/pets\": { \"get\": { \"description\": \"Returns all pets from the system that the user has access to\", \"responses\": { \"200\": { \"description\": \"A list of pets.\", \"content\": { \"application/json\": { \"schema\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/components/schemas/pet\" } } } } } } } } } /pets: get: description: Returns all pets from the system that the user has access to responses: '200': description: A list of pets. content: application/json: schema: type: array items: $ref: '#/components/schemas/pet' Path Item Object Describes the operations available on a single path. A Path Item MAY be empty, due to ACL constraints . The path itself is still expo",
      "chunk_index": 31
    },
    {
      "chunk_id": "d121af01a0927985e683f46d5b5a946fb8e76662",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ion/json: schema: type: array items: $ref: '#/components/schemas/pet' Path Item Object Describes the operations available on a single path. A Path Item MAY be empty, due to ACL constraints . The path itself is still exposed to the documentation viewer but they will not know which operations and parameters are available. Fixed Fields Field Name Type Description $ref string Allows for a referenced definition of this path item. The value MUST be in the form of a URI, and the referenced structure MUST be in the form of a Path Item Object . In case a Path Item Object field appears both in the defined object and the referenced object, the behavior is undefined. See the rules for resolving Relative References . Note: The behavior of $ref with adjacent properties is likely to change in future versions of this specification to bring it into closer alignment with the behavior of the Reference Object . summary string An optional string summary, intended to apply to all operations in this path. description string An optional string description, intended to apply to all operations in this path. CommonMark syntax MAY be used for rich text representation. get Operation Object A definition of a GET operation on this path. put Operation Object A definition of a PUT operation on this path. post Operation Object A definition of a POST operation on this path. delete Operation Object A definition o",
      "chunk_index": 32
    },
    {
      "chunk_id": "ce6d852fc2cd443b84807adf4f007e933e2dac31",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "A definition of a GET operation on this path. put Operation Object A definition of a PUT operation on this path. post Operation Object A definition of a POST operation on this path. delete Operation Object A definition of a DELETE operation on this path. options Operation Object A definition of a OPTIONS operation on this path. head Operation Object A definition of a HEAD operation on this path. patch Operation Object A definition of a PATCH operation on this path. trace Operation Object A definition of a TRACE operation on this path. servers [ Server Object ] An alternative servers array to service all operations in this path. If a servers array is specified at the OpenAPI Object level, it will be overridden by this value. parameters [ Parameter Object | Reference Object ] A list of parameters that are applicable for all the operations described under this path. These parameters can be overridden at the operation level, but cannot be removed there. The list MUST NOT include duplicated parameters. A unique parameter is defined by a combination of a name and location . The list can use the Reference Object to link to parameters that are defined in the OpenAPI Object's components.parameters . This object MAY be extended with Specification Extensions . Path Item Object Example { \"get\": { \"description\": \"Returns pets based on ID\", \"summary\": \"Find pets by ID\", \"operationId\": \"getPe",
      "chunk_index": 33
    },
    {
      "chunk_id": "01169f455f4d6b5f67d46d757a2b33ecc089f1cd",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ect's components.parameters . This object MAY be extended with Specification Extensions . Path Item Object Example { \"get\": { \"description\": \"Returns pets based on ID\", \"summary\": \"Find pets by ID\", \"operationId\": \"getPetsById\", \"responses\": { \"200\": { \"description\": \"pet response\", \"content\": { \"*/*\": { \"schema\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/components/schemas/Pet\" } } } } }, \"default\": { \"description\": \"error payload\", \"content\": { \"text/html\": { \"schema\": { \"$ref\": \"#/components/schemas/ErrorModel\" } } } } } }, \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"description\": \"ID of pet to use\", \"required\": true, \"schema\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } }, \"style\": \"simple\" } ] } get: description: Returns pets based on ID summary: Find pets by ID operationId: getPetsById responses: '200': description: pet response content: '*/*': schema: type: array items: $ref: '#/components/schemas/Pet' default: description: error payload content: text/html: schema: $ref: '#/components/schemas/ErrorModel' parameters: - name: id in: path description: ID of pet to use required: true schema: type: array items: type: string style: simple Operation Object Describes a single API operation on a path. Fixed Fields Field Name Type Description tags [ string ] A list of tags for API documentation control. Tags can be used for logical grouping of operations by resources or any",
      "chunk_index": 34
    },
    {
      "chunk_id": "313e785b4cc2dfbdeee32d5e474b5195301ffa8d",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "t Describes a single API operation on a path. Fixed Fields Field Name Type Description tags [ string ] A list of tags for API documentation control. Tags can be used for logical grouping of operations by resources or any other qualifier. summary string A short summary of what the operation does. description string A verbose explanation of the operation behavior. CommonMark syntax MAY be used for rich text representation. externalDocs External Documentation Object Additional external documentation for this operation. operationId string Unique string used to identify the operation. The id MUST be unique among all operations described in the API. The operationId value is case-sensitive . Tools and libraries MAY use the operationId to uniquely identify an operation, therefore, it is RECOMMENDED to follow common programming naming conventions. parameters [ Parameter Object | Reference Object ] A list of parameters that are applicable for this operation. If a parameter is already defined at the Path Item , the new definition will override it but can never remove it. The list MUST NOT include duplicated parameters. A unique parameter is defined by a combination of a name and location . The list can use the Reference Object to link to parameters that are defined in the OpenAPI Object's components.parameters . requestBody Request Body Object | Reference Object The request body applicabl",
      "chunk_index": 35
    },
    {
      "chunk_id": "5582946f4f1a96d2509584cde1b45a749596a290",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "me and location . The list can use the Reference Object to link to parameters that are defined in the OpenAPI Object's components.parameters . requestBody Request Body Object | Reference Object The request body applicable for this operation. The requestBody is fully supported in HTTP methods where the HTTP 1.1 specification RFC7231 has explicitly defined semantics for request bodies. In other cases where the HTTP spec is vague (such as GET , HEAD and DELETE ), requestBody is permitted but does not have well-defined semantics and SHOULD be avoided if possible. responses Responses Object The list of possible responses as they are returned from executing this operation. callbacks Map[ string , Callback Object | Reference Object ] A map of possible out-of band callbacks related to the parent operation. The key is a unique identifier for the Callback Object. Each value in the map is a Callback Object that describes a request that may be initiated by the API provider and the expected responses. deprecated boolean Declares this operation to be deprecated. Consumers SHOULD refrain from usage of the declared operation. Default value is false . security [ Security Requirement Object ] A declaration of which security mechanisms can be used for this operation. The list of values includes alternative Security Requirement Objects that can be used. Only one of the Security Requirement Objects",
      "chunk_index": 36
    },
    {
      "chunk_id": "ff233129c7690c3213ac52d5eeab01014ee5c5ff",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ment Object ] A declaration of which security mechanisms can be used for this operation. The list of values includes alternative Security Requirement Objects that can be used. Only one of the Security Requirement Objects need to be satisfied to authorize a request. To make security optional, an empty security requirement ( {} ) can be included in the array. This definition overrides any declared top-level security . To remove a top-level security declaration, an empty array can be used. servers [ Server Object ] An alternative servers array to service this operation. If a servers array is specified at the Path Item Object or OpenAPI Object level, it will be overridden by this value. This object MAY be extended with Specification Extensions . Operation Object Example { \"tags\": [\"pet\"], \"summary\": \"Updates a pet in the store with form data\", \"operationId\": \"updatePetWithForm\", \"parameters\": [ { \"name\": \"petId\", \"in\": \"path\", \"description\": \"ID of pet that needs to be updated\", \"required\": true, \"schema\": { \"type\": \"string\" } } ], \"requestBody\": { \"content\": { \"application/x-www-form-urlencoded\": { \"schema\": { \"type\": \"object\", \"properties\": { \"name\": { \"description\": \"Updated name of the pet\", \"type\": \"string\" }, \"status\": { \"description\": \"Updated status of the pet\", \"type\": \"string\" } }, \"required\": [\"status\"] } } } }, \"responses\": { \"200\": { \"description\": \"Pet updated.\", \"con",
      "chunk_index": 37
    },
    {
      "chunk_id": "318ebb45e091a3506e8957b113c513e5ac2ac680",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "n\": \"Updated name of the pet\", \"type\": \"string\" }, \"status\": { \"description\": \"Updated status of the pet\", \"type\": \"string\" } }, \"required\": [\"status\"] } } } }, \"responses\": { \"200\": { \"description\": \"Pet updated.\", \"content\": { \"application/json\": {}, \"application/xml\": {} } }, \"405\": { \"description\": \"Method Not Allowed\", \"content\": { \"application/json\": {}, \"application/xml\": {} } } }, \"security\": [ { \"petstore_auth\": [\"write:pets\", \"read:pets\"] } ] } tags: - pet summary: Updates a pet in the store with form data operationId: updatePetWithForm parameters: - name: petId in: path description: ID of pet that needs to be updated required: true schema: type: string requestBody: content: application/x-www-form-urlencoded: schema: type: object properties: name: description: Updated name of the pet type: string status: description: Updated status of the pet type: string required: - status responses: '200': description: Pet updated. content: application/json: {} application/xml: {} '405': description: Method Not Allowed content: application/json: {} application/xml: {} security: - petstore_auth: - write:pets - read:pets External Documentation Object Allows referencing an external resource for extended documentation. Fixed Fields Field Name Type Description description string A description of the target documentation. CommonMark syntax MAY be used for rich text representation. url str",
      "chunk_index": 38
    },
    {
      "chunk_id": "bea6450d15f314ab68cd4c2c74ddaa3761d5530d",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " an external resource for extended documentation. Fixed Fields Field Name Type Description description string A description of the target documentation. CommonMark syntax MAY be used for rich text representation. url string REQUIRED . The URI for the target documentation. This MUST be in the form of a URI. This object MAY be extended with Specification Extensions . External Documentation Object Example { \"description\": \"Find more info here\", \"url\": \"https://example.com\" } description: Find more info here url: https://example.com Parameter Object Describes a single operation parameter. A unique parameter is defined by a combination of a name and location . See Appendix E for a detailed examination of percent-encoding concerns, including interactions with the application/x-www-form-urlencoded query string format. Parameter Locations There are four possible parameter locations specified by the in field: path - Used together with Path Templating , where the parameter value is actually part of the operation's URL. This does not include the host or base path of the API. For example, in /items/{itemId} , the path parameter is itemId . query - Parameters that are appended to the URL. For example, in /items?id=### , the query parameter is id . header - Custom headers that are expected as part of the request. Note that RFC7230 states header names are case insensitive. cookie - Used to pa",
      "chunk_index": 39
    },
    {
      "chunk_id": "473b70bba67955690f76ed4c389ece1e7bbc4cd9",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ed to the URL. For example, in /items?id=### , the query parameter is id . header - Custom headers that are expected as part of the request. Note that RFC7230 states header names are case insensitive. cookie - Used to pass a specific cookie value to the API. Fixed Fields The rules for serialization of the parameter are specified in one of two ways. Parameter Objects MUST include either a content field or a schema field, but not both. See Appendix B for a discussion of converting values of various types to string representations. Common Fixed Fields These fields MAY be used with either content or schema . Field Name Type Description name string REQUIRED . The name of the parameter. Parameter names are case sensitive . If in is \"path\" , the name field MUST correspond to a template expression occurring within the path field in the Paths Object . See Path Templating for further information. If in is \"header\" and the name field is \"Accept\" , \"Content-Type\" or \"Authorization\" , the parameter definition SHALL be ignored. For all other cases, the name corresponds to the parameter name used by the in field. in string REQUIRED . The location of the parameter. Possible values are \"query\" , \"header\" , \"path\" or \"cookie\" . description string A brief description of the parameter. This could contain examples of use. CommonMark syntax MAY be used for rich text representation. required boolean ",
      "chunk_index": 40
    },
    {
      "chunk_id": "c287209b5c6cb65e92d27e515a27372e0a3946c5",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ues are \"query\" , \"header\" , \"path\" or \"cookie\" . description string A brief description of the parameter. This could contain examples of use. CommonMark syntax MAY be used for rich text representation. required boolean Determines whether this parameter is mandatory. If the parameter location is \"path\" , this field is REQUIRED and its value MUST be true . Otherwise, the field MAY be included and its default value is false . deprecated boolean Specifies that a parameter is deprecated and SHOULD be transitioned out of usage. Default value is false . allowEmptyValue boolean If true , clients MAY pass a zero-length string value in place of parameters that would otherwise be omitted entirely, which the server SHOULD interpret as the parameter being unused. Default value is false . If style is used, and if behavior is n/a (cannot be serialized) , the value of allowEmptyValue SHALL be ignored. Interactions between this field and the parameter's Schema Object are implementation-defined. This field is valid only for query parameters. Use of this field is NOT RECOMMENDED, and it is likely to be removed in a later revision. This object MAY be extended with Specification Extensions . Note that while \"Cookie\" as a name is not forbidden if in is \"header\" , the effect of defining a cookie parameter that way is undefined; use in: \"cookie\" instead. Fixed Fields for use with schema For simpler s",
      "chunk_index": 41
    },
    {
      "chunk_id": "8afede6ff3d880305654f56c5ad64424969a241e",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "tensions . Note that while \"Cookie\" as a name is not forbidden if in is \"header\" , the effect of defining a cookie parameter that way is undefined; use in: \"cookie\" instead. Fixed Fields for use with schema For simpler scenarios, a schema and style can describe the structure and syntax of the parameter. When example or examples are provided in conjunction with the schema field, the example SHOULD match the specified schema and follow the prescribed serialization strategy for the parameter. The example and examples fields are mutually exclusive, and if either is present it SHALL override any example in the schema. Serializing with schema is NOT RECOMMENDED for in: \"cookie\" parameters, in: \"header\" parameters that use HTTP header parameters (name=value pairs following a ; ) in their values, or in: \"header\" parameters where values might have non-URL-safe characters; see Appendix D for details. Field Name Type Description style string Describes how the parameter value will be serialized depending on the type of the parameter value. Default values (based on value of in ): for \"query\" - \"form\" ; for \"path\" - \"simple\" ; for \"header\" - \"simple\" ; for \"cookie\" - \"form\" . explode boolean When this is true, parameter values of type array or object generate separate parameters for each value of the array or key-value pair of the map. For other types of parameters this field has no effect. ",
      "chunk_index": 42
    },
    {
      "chunk_id": "3274a875c97722aeadfb69d6e643a0e86ee01791",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " explode boolean When this is true, parameter values of type array or object generate separate parameters for each value of the array or key-value pair of the map. For other types of parameters this field has no effect. When style is \"form\" , the default value is true . For all other styles, the default value is false . Note that despite false being the default for deepObject , the combination of false with deepObject is undefined. allowReserved boolean When this is true, parameter values are serialized using reserved expansion, as defined by RFC6570 , which allows RFC3986's reserved character set , as well as percent-encoded triples, to pass through unchanged, while still percent-encoding all other disallowed characters (including % outside of percent-encoded triples). Applications are still responsible for percent-encoding reserved characters that are not allowed in the query string ( [ , ] , # ), or have a special meaning in application/x-www-form-urlencoded ( - , & , + ); see Appendices C and E for details. This field only applies to parameters with an in value of query . The default value is false . schema Schema Object The schema defining the type used for the parameter. example Any Example of the parameter's potential value; see Working With Examples . examples Map[ string , Example Object | Reference Object ] Examples of the parameter's potential value; see Working With",
      "chunk_index": 43
    },
    {
      "chunk_id": "b7d2aa2484926a8c73114d79745dcc959523c809",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " the parameter. example Any Example of the parameter's potential value; see Working With Examples . examples Map[ string , Example Object | Reference Object ] Examples of the parameter's potential value; see Working With Examples . See also Appendix C: Using RFC6570-Based Serialization for additional guidance. Fixed Fields for use with content For more complex scenarios, the content field can define the media type and schema of the parameter, as well as give examples of its use. Using content with a text/plain media type is RECOMMENDED for in: \"header\" and in: \"cookie\" parameters where the schema strategy is not appropriate. Field Name Type Description content Map[ string , Media Type Object ] A map containing the representations for the parameter. The key is the media type and the value describes it. The map MUST only contain one entry. Style Values In order to support common ways of serializing simple parameters, a set of style values are defined. style type in Comments matrix primitive , array , object path Path-style parameters defined by RFC6570 label primitive , array , object path Label style parameters defined by RFC6570 simple primitive , array , object path , header Simple style parameters defined by RFC6570 . This option replaces collectionFormat with a csv value from OpenAPI 2.0. form primitive , array , object query , cookie Form style parameters defined by RFC6570",
      "chunk_index": 44
    },
    {
      "chunk_id": "4a4d87ea6c064fa025bca950b18f830f8fb25801",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " path , header Simple style parameters defined by RFC6570 . This option replaces collectionFormat with a csv value from OpenAPI 2.0. form primitive , array , object query , cookie Form style parameters defined by RFC6570 . This option replaces collectionFormat with a csv (when explode is false) or multi (when explode is true) value from OpenAPI 2.0. spaceDelimited array , object query Space separated array values or object properties and values. This option replaces collectionFormat equal to ssv from OpenAPI 2.0. pipeDelimited array , object query Pipe separated array values or object properties and values. This option replaces collectionFormat equal to pipes from OpenAPI 2.0. deepObject object query Allows objects with scalar properties to be represented using form parameters. The representation of array or object properties is not defined. See Appendix E for a discussion of percent-encoding, including when delimiters need to be percent-encoded and options for handling collisions with percent-encoded data. Style Examples Assume a parameter named color has one of the following values: string -> \"blue\" array -> [\"blue\", \"black\", \"brown\"] object -> { \"R\": 100, \"G\": 200, \"B\": 150 } The following table shows examples, as would be shown with the example or examples keywords, of the different serializations for each value. The value empty denotes the empty string, and is unrelated to",
      "chunk_index": 45
    },
    {
      "chunk_id": "8811f0ea9d94234b3921c95af55c646adc92d3e6",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": ": 200, \"B\": 150 } The following table shows examples, as would be shown with the example or examples keywords, of the different serializations for each value. The value empty denotes the empty string, and is unrelated to the allowEmptyValue field The behavior of combinations marked n/a is undefined The undefined column replaces the empty column in previous versions of this specification in order to better align with RFC6570 terminology, which describes certain values including but not limited to null as \"undefined\" values with special handling; notably, the empty string is not undefined For form and the non-RFC6570 query string styles spaceDelimited , pipeDelimited , and deepObject , each example is shown prefixed with ? as if it were the only query parameter; see Appendix C for more information on constructing query strings from multiple parameters, and Appendix D for warnings regarding form and cookie parameters Note that the ? prefix is not appropriate for serializing application/x-www-form-urlencoded HTTP message bodies, and MUST be stripped or (if constructing the string manually) not added when used in that context; see the Encoding Object for more information The examples are percent-encoded as required by RFC6570 and RFC3986; see Appendix E for a thorough discussion of percent-encoding concerns, including why unencoded | ( %7C ), [ ( %5B ), and ] ( %5D ) seem to work in",
      "chunk_index": 46
    },
    {
      "chunk_id": "b0aa75a5e530ee68e60dd0c78650006ba4df8355",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "tion The examples are percent-encoded as required by RFC6570 and RFC3986; see Appendix E for a thorough discussion of percent-encoding concerns, including why unencoded | ( %7C ), [ ( %5B ), and ] ( %5D ) seem to work in some environments despite not being compliant. style explode undefined string array object matrix false ;color ;color=blue ;color=blue,black,brown ;color=R,100,G,200,B,150 matrix true ;color ;color=blue ;color=blue;color=black;color=brown ;R=100;G=200;B=150 label false . .blue .blue,black,brown .R,100,G,200,B,150 label true . .blue .blue.black.brown .R=100.G=200.B=150 simple false empty blue blue,black,brown R,100,G,200,B,150 simple true empty blue blue,black,brown R=100,G=200,B=150 form false ?color= ?color=blue ?color=blue,black,brown ?color=R,100,G,200,B,150 form true ?color= ?color=blue ?color=blue&color=black&color=brown ?R=100&G=200&B=150 spaceDelimited false n/a n/a ?color=blue%20black%20brown ?color=R%20100%20G%20200%20B%20150 spaceDelimited true n/a n/a n/a n/a pipeDelimited false n/a n/a ?color=blue%7Cblack%7Cbrown ?color=R%7C100%7CG%7C200%7CB%7C150 pipeDelimited true n/a n/a n/a n/a deepObject false n/a n/a n/a n/a deepObject true n/a n/a n/a ?color%5BR%5D=100&color%5BG%5D=200&color%5BB%5D=150 Parameter Object Examples A header parameter with an array of 64-bit integer numbers: { \"name\": \"token\", \"in\": \"header\", \"description\": \"token to be passed as ",
      "chunk_index": 47
    },
    {
      "chunk_id": "fef1e4adc644b85ccdcef5420e131bd3b8cf72c5",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " n/a n/a ?color%5BR%5D=100&color%5BG%5D=200&color%5BB%5D=150 Parameter Object Examples A header parameter with an array of 64-bit integer numbers: { \"name\": \"token\", \"in\": \"header\", \"description\": \"token to be passed as a header\", \"required\": true, \"schema\": { \"type\": \"array\", \"items\": { \"type\": \"integer\", \"format\": \"int64\" } }, \"style\": \"simple\" } name: token in: header description: token to be passed as a header required: true schema: type: array items: type: integer format: int64 style: simple A path parameter of a string value: { \"name\": \"username\", \"in\": \"path\", \"description\": \"username to fetch\", \"required\": true, \"schema\": { \"type\": \"string\" } } name: username in: path description: username to fetch required: true schema: type: string An optional query parameter of a string value, allowing multiple values by repeating the query parameter: { \"name\": \"id\", \"in\": \"query\", \"description\": \"ID of the object to fetch\", \"required\": false, \"schema\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } }, \"style\": \"form\", \"explode\": true } name: id in: query description: ID of the object to fetch required: false schema: type: array items: type: string style: form explode: true A free-form query parameter, allowing undefined parameters of a specific type: { \"in\": \"query\", \"name\": \"freeForm\", \"schema\": { \"type\": \"object\", \"additionalProperties\": { \"type\": \"integer\" } }, \"style\": \"form\" ",
      "chunk_index": 48
    },
    {
      "chunk_id": "935b88f6dded6b23baf54509c0a3100fdf31c462",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "de: true A free-form query parameter, allowing undefined parameters of a specific type: { \"in\": \"query\", \"name\": \"freeForm\", \"schema\": { \"type\": \"object\", \"additionalProperties\": { \"type\": \"integer\" } }, \"style\": \"form\" } in: query name: freeForm schema: type: object additionalProperties: type: integer style: form A complex parameter using content to define serialization: { \"in\": \"query\", \"name\": \"coordinates\", \"content\": { \"application/json\": { \"schema\": { \"type\": \"object\", \"required\": [\"lat\", \"long\"], \"properties\": { \"lat\": { \"type\": \"number\" }, \"long\": { \"type\": \"number\" } } } } } } in: query name: coordinates content: application/json: schema: type: object required: - lat - long properties: lat: type: number long: type: number Request Body Object Describes a single request body. Fixed Fields Field Name Type Description description string A brief description of the request body. This could contain examples of use. CommonMark syntax MAY be used for rich text representation. content Map[ string , Media Type Object ] REQUIRED . The content of the request body. The key is a media type or media type range and the value describes it. For requests that match multiple keys, only the most specific key is applicable. e.g. \"text/plain\" overrides \"text/*\" required boolean Determines if the request body is required in the request. Defaults to false . This object MAY be extended with Spec",
      "chunk_index": 49
    },
    {
      "chunk_id": "89d0fe92c0c784e47b625ea4a46785d4c4f1d0e4",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "e keys, only the most specific key is applicable. e.g. \"text/plain\" overrides \"text/*\" required boolean Determines if the request body is required in the request. Defaults to false . This object MAY be extended with Specification Extensions . Request Body Examples A request body with a referenced schema definition. { \"description\": \"user to add to the system\", \"content\": { \"application/json\": { \"schema\": { \"$ref\": \"#/components/schemas/User\" }, \"examples\": { \"user\": { \"summary\": \"User Example\", \"externalValue\": \"https://foo.bar/examples/user-example.json\" } } }, \"application/xml\": { \"schema\": { \"$ref\": \"#/components/schemas/User\" }, \"examples\": { \"user\": { \"summary\": \"User example in XML\", \"externalValue\": \"https://foo.bar/examples/user-example.xml\" } } }, \"text/plain\": { \"examples\": { \"user\": { \"summary\": \"User example in Plain text\", \"externalValue\": \"https://foo.bar/examples/user-example.txt\" } } }, \"*/*\": { \"examples\": { \"user\": { \"summary\": \"User example in other format\", \"externalValue\": \"https://foo.bar/examples/user-example.whatever\" } } } } } description: user to add to the system content: application/json: schema: $ref: '#/components/schemas/User' examples: user: summary: User example externalValue: https://foo.bar/examples/user-example.json application/xml: schema: $ref: '#/components/schemas/User' examples: user: summary: User example in XML externalValue: https://f",
      "chunk_index": 50
    },
    {
      "chunk_id": "7eb544f4cf9ce7ce998066bde1985eedd1ebcb53",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ples: user: summary: User example externalValue: https://foo.bar/examples/user-example.json application/xml: schema: $ref: '#/components/schemas/User' examples: user: summary: User example in XML externalValue: https://foo.bar/examples/user-example.xml text/plain: examples: user: summary: User example in plain text externalValue: https://foo.bar/examples/user-example.txt '*/*': examples: user: summary: User example in other format externalValue: https://foo.bar/examples/user-example.whatever Each Media Type Object provides schema and examples for the media type identified by its key. When example or examples are provided, the example SHOULD match the specified schema and be in the correct format as specified by the media type and its encoding. The example and examples fields are mutually exclusive, and if either is present it SHALL override any example in the schema. See Working With Examples for further guidance regarding the different ways of specifying examples, including non-JSON/YAML values. Fixed Fields Field Name Type Description schema Schema Object The schema defining the content of the request, response, parameter, or header. example Any Example of the media type; see Working With Examples . examples Map[ string , Example Object | Reference Object ] Examples of the media type; see Working With Examples . encoding Map[ string , Encoding Object ] A map between a propert",
      "chunk_index": 51
    },
    {
      "chunk_id": "40f1919a33387a8b4feb95ee7a7b9be8794d01ac",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " media type; see Working With Examples . examples Map[ string , Example Object | Reference Object ] Examples of the media type; see Working With Examples . encoding Map[ string , Encoding Object ] A map between a property name and its encoding information. The key, being the property name, MUST exist in the schema as a property. The encoding field SHALL only apply to Request Body Objects , and only when the media type is multipart or application/x-www-form-urlencoded . If no Encoding Object is provided for a property, the behavior is determined by the default values documented for the Encoding Object. This object MAY be extended with Specification Extensions . { \"application/json\": { \"schema\": { \"$ref\": \"#/components/schemas/Pet\" }, \"examples\": { \"cat\": { \"summary\": \"An example of a cat\", \"value\": { \"name\": \"Fluffy\", \"petType\": \"Cat\", \"color\": \"White\", \"gender\": \"male\", \"breed\": \"Persian\" } }, \"dog\": { \"summary\": \"An example of a dog with a cat's name\", \"value\": { \"name\": \"Puma\", \"petType\": \"Dog\", \"color\": \"Black\", \"gender\": \"Female\", \"breed\": \"Mixed\" } }, \"frog\": { \"$ref\": \"#/components/examples/frog-example\" } } } } application/json: schema: $ref: '#/components/schemas/Pet' examples: cat: summary: An example of a cat value: name: Fluffy petType: Cat color: White gender: male breed: Persian dog: summary: An example of a dog with a cat's name value: name: Puma petType: Dog colo",
      "chunk_index": 52
    },
    {
      "chunk_id": "4533336070f1f43197206af1897445b9b2e84a51",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "s/schemas/Pet' examples: cat: summary: An example of a cat value: name: Fluffy petType: Cat color: White gender: male breed: Persian dog: summary: An example of a dog with a cat's name value: name: Puma petType: Dog color: Black gender: Female breed: Mixed frog: $ref: '#/components/examples/frog-example' Considerations for File Uploads In contrast to OpenAPI 2.0, file input/output content in OAS 3.x is described with the same semantics as any other schema type. In contrast to OAS 3.0, the format keyword has no effect on the content-encoding of the schema in OAS 3.1. Instead, JSON Schema's contentEncoding and contentMediaType keywords are used. See Working With Binary Data for how to model various scenarios with these keywords, and how to migrate from the previous format usage. Examples: Content transferred in binary (octet-stream) MAY omit schema : # a PNG image as a binary file: content: image/png: {} # an arbitrary binary file: content: application/octet-stream: {} # arbitrary JSON without constraints beyond being syntactically valid: content: application/json: {} These examples apply to either input payloads of file uploads or response payloads. A requestBody for submitting a file in a POST operation may look like the following example: requestBody: content: application/octet-stream: {} In addition, specific media types MAY be specified: # multiple, specific media types may ",
      "chunk_index": 53
    },
    {
      "chunk_id": "e9248cf15963e0a3c88da683d98ee78291993f2b",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " for submitting a file in a POST operation may look like the following example: requestBody: content: application/octet-stream: {} In addition, specific media types MAY be specified: # multiple, specific media types may be specified: requestBody: content: # a binary file of type png or jpeg image/jpeg: {} image/png: {} To upload multiple files, a multipart media type MUST be used as shown under Example: Multipart Form with Multiple Files . Support for x-www-form-urlencoded Request Bodies See Encoding the x-www-form-urlencoded Media Type for guidance and examples, both with and without the encoding field. Special Considerations for multipart Content See Encoding multipart Media Types for further guidance and examples, both with and without the encoding field. Encoding Object A single encoding definition applied to a single schema property. See Appendix B for a discussion of converting values of various types to string representations. Properties are correlated with multipart parts using the name parameter of Content-Disposition: form-data , and with application/x-www-form-urlencoded using the query string parameter names. In both cases, their order is implementation-defined. See Appendix E for a detailed examination of percent-encoding concerns for form media types. Fixed Fields Common Fixed Fields These fields MAY be used either with or without the RFC6570-style serialization f",
      "chunk_index": 54
    },
    {
      "chunk_id": "4b5f0fef7b8360789a6aaf5f3faff2bc367e7c4d",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ion-defined. See Appendix E for a detailed examination of percent-encoding concerns for form media types. Fixed Fields Common Fixed Fields These fields MAY be used either with or without the RFC6570-style serialization fields defined in the next section below. Field Name Type Description contentType string The Content-Type for encoding a specific property. The value is a comma-separated list, each element of which is either a specific media type (e.g. image/png ) or a wildcard media type (e.g. image/* ). Default value depends on the property type as shown in the table below. headers Map[ string , Header Object | Reference Object ] A map allowing additional information to be provided as headers. Content-Type is described separately and SHALL be ignored in this section. This field SHALL be ignored if the request body media type is not a multipart . This object MAY be extended with Specification Extensions . The default values for contentType are as follows, where an n/a in the contentEncoding column means that the presence or value of contentEncoding is irrelevant: type contentEncoding Default contentType absent n/a application/octet-stream string present application/octet-stream string absent text/plain number , integer , or boolean n/a text/plain object n/a application/json array n/a according to the type of the items schema Determining how to handle a type value of null depend",
      "chunk_index": 55
    },
    {
      "chunk_id": "a42f45a1875d424033101a9fe55bfe975cbdbc52",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "ion/octet-stream string absent text/plain number , integer , or boolean n/a text/plain object n/a application/json array n/a according to the type of the items schema Determining how to handle a type value of null depends on how null values are being serialized. If null values are entirely omitted, then the contentType is irrelevant. See Appendix B for a discussion of data type conversion options. Fixed Fields for RFC6570-style Serialization Field Name Type Description style string Describes how a specific property value will be serialized depending on its type. See Parameter Object for details on the style field. The behavior follows the same values as query parameters, including default values. Note that the initial ? used in query strings is not used in application/x-www-form-urlencoded message bodies, and MUST be removed (if using an RFC6570 implementation) or simply not added (if constructing the string manually). This field SHALL be ignored if the request body media type is not application/x-www-form-urlencoded or multipart/form-data . If a value is explicitly defined, then the value of contentType (implicit or explicit) SHALL be ignored. explode boolean When this is true, property values of type array or object generate separate parameters for each value of the array, or key-value-pair of the map. For other types of properties this field has no effect. When style is \"for",
      "chunk_index": 56
    },
    {
      "chunk_id": "ce0afd09a121053e0bce560d50c7f66553e5d9e6",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "hen this is true, property values of type array or object generate separate parameters for each value of the array, or key-value-pair of the map. For other types of properties this field has no effect. When style is \"form\" , the default value is true . For all other styles, the default value is false . Note that despite false being the default for deepObject , the combination of false with deepObject is undefined. This field SHALL be ignored if the request body media type is not application/x-www-form-urlencoded or multipart/form-data . If a value is explicitly defined, then the value of contentType (implicit or explicit) SHALL be ignored. allowReserved boolean When this is true, parameter values are serialized using reserved expansion, as defined by RFC6570 , which allows RFC3986's reserved character set , as well as percent-encoded triples, to pass through unchanged, while still percent-encoding all other disallowed characters (including % outside of percent-encoded triples). Applications are still responsible for percent-encoding reserved characters that are not allowed in the query string ( [ , ] , # ), or have a special meaning in application/x-www-form-urlencoded ( - , & , + ); see Appendices C and E for details. The default value is false . This field SHALL be ignored if the request body media type is not application/x-www-form-urlencoded or multipart/form-data . If a va",
      "chunk_index": 57
    },
    {
      "chunk_id": "7353da8b074e90ee0907b76f6457521b17c85880",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": "encoded ( - , & , + ); see Appendices C and E for details. The default value is false . This field SHALL be ignored if the request body media type is not application/x-www-form-urlencoded or multipart/form-data . If a value is explicitly defined, then the value of contentType (implicit or explicit) SHALL be ignored. See also Appendix C: Using RFC6570 Implementations for additional guidance, including on difficulties caused by the interaction between RFC6570's percent-encoding rules and the multipart/form-data media type. Note that the presence of at least one of style , explode , or allowReserved with an explicit value is equivalent to using schema with in: \"query\" Parameter Objects. The absence of all three of those fields is the equivalent of using content , but with the media type specified in contentType rather than through a Media Type Object. To submit content using form url encoding via RFC1866 , use the application/x-www-form-urlencoded media type in the Media Type Object under the Request Body Object . This configuration means that the request body MUST be encoded per RFC1866 when passed to the server, after any complex objects have been serialized to a string representation. See Appendix E for a detailed examination of percent-encoding concerns for form media types. Example: URL Encoded Form with JSON Values When there is no encoding field, the serialization strategy ",
      "chunk_index": 58
    },
    {
      "chunk_id": "f5da64f940b2675708538862cea7d3fdf4e93465",
      "url": "https://swagger.io/specification/",
      "title": "OpenAPI Specification - Version 3.1.0 | Swagger",
      "text": " string representation. See Appendix E for a detailed examination of percent-encoding concerns for form media types. Example: URL Encoded Form with JSON Values When there is no encoding field, the serialization strategy is based on the Encoding Object's default values: requestBody: content: application/x-www-form-urlencoded: schema: type: object properties: id: type: string format: uuid address: # complex types are stringified to support RFC 1866 type: object properties: {} With this example, consider an id of f81d4fae-7dec-11d0-a765-00a0c91e6bf6 and a US-style address (with ZIP+4) as follows: { \"streetAddress\": \"123 Example Dr.\", \"city\": \"Somewhere\", \"state\": \"CA\", \"zip\": \"99999+1234\" } Assuming the most compact representation of the JSON value (with unnecessary whitespace removed), we would expect to see the following request body, where space characters have been replaced with + and + , \" , { , and } have been percent-encoded to %2B , %22 , %7B , and %7D , respectively: id=f81d4fae-7dec-11d0-a765-00a0c91e6bf6&address=%7B%22streetAddress%22:%22123+Example+Dr.%22,%22city%22:%22Somewhere%22,%22state%22:%22CA%22,%22zip%22:%2299999%2B1234%22%7D Note that the id keyword is treated as text/plain per the Encoding Object 's default behavior, and is serialized as-is. If it were treated as application/json , then the serialized value would be a JSON string including quotation marks, wh",
      "chunk_index": 59
    },
    {
      "chunk_id": "425c2d94f4ce3e3644239ae4d32675de0e07fa79",
      "url": "https://owasp.org/www-project-top-ten/",
      "title": "OWASP Top Ten Web Application Security Risks",
      "text": "The most current released version is the OWASP Top Ten 2025 . Previous versions are available at OWASP Top Ten 2021 and OWASP Top 10 2017 (PDF) . Older versiona are available in the Github repo . The OWASP Top 10 is a standard awareness document for developers and web application security. It represents a broad consensus about the most critical security risks to web applications. Globally recognized by developers as the first step towards more secure coding. Companies should adopt this document and start the process of ensuring that their web applications minimize these risks. Using the OWASP Top 10 is perhaps the most effective first step towards changing the software development culture within your organization into one that produces more secure code. OWASP Top 10 2025 Data Analysis Plan Goals To collect the most comprehensive dataset related to identified application vulnerabilities to-date to enable analysis for the Top 10 and other future research as well. This data should come from a variety of sources; security vendors and consultancies, bug bounties, along with company/organizational contributions. Data will be normalized to allow for level comparison between Human assisted Tooling and Tooling assisted Humans. Analysis Infrastructure Plan to leverage the OWASP Azure Cloud Infrastructure to collect, analyze, and store the data contributed. Contributions We plan to suppor",
      "chunk_index": 0
    },
    {
      "chunk_id": "dac51ed796099e6acec6892302350fcc6e15d35c",
      "url": "https://owasp.org/www-project-top-ten/",
      "title": "OWASP Top Ten Web Application Security Risks",
      "text": "tween Human assisted Tooling and Tooling assisted Humans. Analysis Infrastructure Plan to leverage the OWASP Azure Cloud Infrastructure to collect, analyze, and store the data contributed. Contributions We plan to support both known and pseudo-anonymous contributions. The preference is for contributions to be known; this immensely helps with the validation/quality/confidence of the data submitted. If the submitter prefers to have their data stored anonymously and even go as far as submitting the data anonymously, then it will have to be classified as “unverified” vs. “verified”. Verified Data Contribution Scenario 1: The submitter is known and has agreed to be identified as a contributing party. Scenario 2: The submitter is known but would rather not be publicly identified. Scenario 3: The submitter is known but does not want it recorded in the dataset. Unverified Data Contribution Scenario 4: The submitter is anonymous. (Should we support?) The analysis of the data will be conducted with a careful distinction when the unverified data is part of the dataset that was analyzed. Contribution Process There are a few ways that data can be contributed: Email a CSV/Excel file with the dataset(s) to [email protected] Upload a CSV/Excel file to https://bit.ly/OWASPTop10Data Template examples can be found in GitHub: https://github.com/OWASP/Top10/tree/master/2025/Data Contribution Period",
      "chunk_index": 1
    },
    {
      "chunk_id": "17386f65b85bcc567a18e71add036f5855a9c5e7",
      "url": "https://owasp.org/www-project-top-ten/",
      "title": "OWASP Top Ten Web Application Security Risks",
      "text": "el file with the dataset(s) to [email protected] Upload a CSV/Excel file to https://bit.ly/OWASPTop10Data Template examples can be found in GitHub: https://github.com/OWASP/Top10/tree/master/2025/Data Contribution Period We plan to accept contributions to the new Top 10 until July 31, 2025, for data dating from 2021 to 2024. Data Structure The following data elements are required or optional. The more information provided the more accurate our analysis can be. At a bare minimum, we need the time period, total number of applications tested in the dataset, and the list of CWEs and counts of how many applications contained that CWE. If at all possible, please provide the additional metadata, because that will greatly help us gain more insights into the current state of testing and vulnerabilities. Contributor Name (org or anon) Contributor Contact Email Time period (2024, 2023, 2022, 2021) Number of applications tested Type of testing (TaH, HaT, Tools) Primary Language (code) Geographic Region (Global, North America, EU, Asia, other) Primary Industry (Multiple, Financial, Industrial, Software, ??) Whether or not data contains retests or the same applications multiple times (T/F) CWE Data A list of CWEs w/ count of applications found to contain that CWE If at all possible, please provide core CWEs in the data, not CWE categories. This will help with the analysis, any normalization/",
      "chunk_index": 2
    },
    {
      "chunk_id": "a6f58345121ae6e348a002484ad06e92d7480edf",
      "url": "https://owasp.org/www-project-top-ten/",
      "title": "OWASP Top Ten Web Application Security Risks",
      "text": "le times (T/F) CWE Data A list of CWEs w/ count of applications found to contain that CWE If at all possible, please provide core CWEs in the data, not CWE categories. This will help with the analysis, any normalization/aggregation done as a part of this analysis will be well documented. Note: If a contributor has two types of datasets, one from HaT and one from TaH sources, then it is recommended to submit them as two separate datasets. HaT = Human assisted Tools (higher volume/frequency, primarily from tooling) TaH = Tool assisted Human (lower volume/frequency, primarily from human testing) Survey Similarly to the Top Ten 2021, we plan to conduct a survey to identify up to two categories of the Top Ten that the community believes are important, but may not be reflected in the data yet. We plan to conduct the survey in early 2025, and will be utilizing Google forms in a similar manner as last time. The CWEs on the survey will come from current trending findings, CWEs that are outside the Top Ten in data, and other potential sources. Process At a high level, we plan to perform a level of data normalization; however, we will keep a version of the raw data contributed for future analysis. We will analyze the CWE distribution of the datasets and potentially reclassify some CWEs to consolidate them into larger buckets. We will carefully document all normalization actions taken so i",
      "chunk_index": 3
    },
    {
      "chunk_id": "5191311cd4bbc4d35c8cba85c2df85e4cc3a1109",
      "url": "https://owasp.org/www-project-top-ten/",
      "title": "OWASP Top Ten Web Application Security Risks",
      "text": "uted for future analysis. We will analyze the CWE distribution of the datasets and potentially reclassify some CWEs to consolidate them into larger buckets. We will carefully document all normalization actions taken so it is clear what has been done. We plan to calculate likelihood following the model we continued in 2021 to determine incidence rate instead of frequency to rate how likely a given app may contain at least one instance of a CWE. This means we aren’t looking for the frequency rate (number of findings) in an app, rather, we are looking for the number of applications that had one or more instances of a CWE. We can calculate the incidence rate based on the total number of applications tested in the dataset compared to how many applications each CWE was found in. In addition, we will be developing base CWSS scores for the top 20-30 CWEs and include potential impact into the Top 10 weighting. Also, would like to explore additional insights that could be gleaned from the contributed dataset to see what else can be learned that could be of use to the security and development communities.",
      "chunk_index": 4
    },
    {
      "chunk_id": "2355dd7e91813a3526516a1ebddad04aa12b8836",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": "What is API Security? A foundational element of innovation in today’s app-driven world is the API. From banks, retail and transportation to IoT, autonomous vehicles and smart cities, APIs are a critical part of modern mobile, SaaS and web applications and can be found in customer-facing, partner-facing and internal applications. By nature, APIs expose application logic and sensitive data such as Personally Identifiable Information (PII) and because of this have increasingly become a target for attackers. Without secure APIs, rapid innovation would be impossible. API Security focuses on strategies and solutions to understand and mitigate the unique vulnerabilities and security risks of Application Programming Interfaces (APIs). API Security Top 10 2023 Here is a sneak peek of the 2023 version: API1:2023 - Broken Object Level Authorization APIs tend to expose endpoints that handle object identifiers, creating a wide attack surface of Object Level Access Control issues. Object level authorization checks should be considered in every function that accesses a data source using an ID from the user. Continue reading . API2:2023 - Broken Authentication Authentication mechanisms are often implemented incorrectly, allowing attackers to compromise authentication tokens or to exploit implementation flaws to assume other user’s identities temporarily or permanently. Compromising a system’s ",
      "chunk_index": 0
    },
    {
      "chunk_id": "7c0f590f26d1f48c0d2e0eb683c23f0a5a3fa1f7",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": "chanisms are often implemented incorrectly, allowing attackers to compromise authentication tokens or to exploit implementation flaws to assume other user’s identities temporarily or permanently. Compromising a system’s ability to identify the client/user, compromises API security overall. Continue reading . API3:2023 - Broken Object Property Level Authorization This category combines API3:2019 Excessive Data Exposure and API6:2019 - Mass Assignment , focusing on the root cause: the lack of or improper authorization validation at the object property level. This leads to information exposure or manipulation by unauthorized parties. Continue reading . API4:2023 - Unrestricted Resource Consumption Satisfying API requests requires resources such as network bandwidth, CPU, memory, and storage. Other resources such as emails/SMS/phone calls or biometrics validation are made available by service providers via API integrations, and paid for per request. Successful attacks can lead to Denial of Service or an increase of operational costs. Continue reading . API5:2023 - Broken Function Level Authorization Complex access control policies with different hierarchies, groups, and roles, and an unclear separation between administrative and regular functions, tend to lead to authorization flaws. By exploiting these issues, attackers can gain access to other users’ resources and/or administrati",
      "chunk_index": 1
    },
    {
      "chunk_id": "cc433472c226c1e3808214b74bbddd7c8d94438e",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": " and roles, and an unclear separation between administrative and regular functions, tend to lead to authorization flaws. By exploiting these issues, attackers can gain access to other users’ resources and/or administrative functions. Continue reading . API6:2023 - Unrestricted Access to Sensitive Business Flows APIs vulnerable to this risk expose a business flow - such as buying a ticket, or posting a comment - without compensating for how the functionality could harm the business if used excessively in an automated manner. This doesn’t necessarily come from implementation bugs. Continue reading . API7:2023 - Server Side Request Forgery Server-Side Request Forgery (SSRF) flaws can occur when an API is fetching a remote resource without validating the user-supplied URI. This enables an attacker to coerce the application to send a crafted request to an unexpected destination, even when protected by a firewall or a VPN. Continue reading . API8:2023 - Security Misconfiguration APIs and the systems supporting them typically contain complex configurations, meant to make the APIs more customizable. Software and DevOps engineers can miss these configurations, or don’t follow security best practices when it comes to configuration, opening the door for different types of attacks. Continue reading . API9:2023 - Improper Inventory Management APIs tend to expose more endpoints than traditio",
      "chunk_index": 2
    },
    {
      "chunk_id": "af73df73ec729dc6d7b348c3004fa5e4beb931c0",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": "ollow security best practices when it comes to configuration, opening the door for different types of attacks. Continue reading . API9:2023 - Improper Inventory Management APIs tend to expose more endpoints than traditional web applications, making proper and updated documentation highly important. A proper inventory of hosts and deployed API versions also are important to mitigate issues such as deprecated API versions and exposed debug endpoints. Continue reading . API10:2023 - Unsafe Consumption of APIs Developers tend to trust data received from third-party APIs more than user input, and so tend to adopt weaker security standards. In order to compromise APIs, attackers go after integrated third-party services instead of trying to compromise the target API directly. Continue reading . Licensing The OWASP API Security Project documents are free to use! The OWASP API Security Project is licensed under the Creative Commons Attribution-ShareAlike 4.0 license , so you can copy, distribute and transmit the work, and you can adapt it, and use it commercially, but all provided that you attribute the work and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Founders Leaders 2023 Contributors 247arjun, abunuwas, Alissa Knight, Arik Atar, aymenfurter, Corey J. Ball, cyn8, d0znpp, Dan Gordon, dong",
      "chunk_index": 3
    },
    {
      "chunk_id": "8ad784987a26de4a57fb75af6192a13575cb7a47",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": "istribute the resulting work only under the same or similar license to this one. Founders Leaders 2023 Contributors 247arjun, abunuwas, Alissa Knight, Arik Atar, aymenfurter, Corey J. Ball, cyn8, d0znpp, Dan Gordon, donge, Dor Tumarkin, faizzaidi, gavjl, guybensimhon, Inês Martins, Isabelle Mauny, Ivan Novikov, jmanico, Juan Pablo, k7jto, LaurentCB, llegaz, Maxim Zavodchik, MrPRogers, planetlevel, rahulk22, Roey Eliyahu, Roshan Piyush, securitylevelup, sudeshgadewar123, Tatsuya-hasegawa, tebbers, vanderaj, wenz, xplo1t-sec, Yaniv Balmas, ynvb 2019 Contributors 007divyachawla, Abid Khan, Adam Fisher, anotherik, bkimminich, caseysoftware, Chris Westphal, dsopas, DSotnikov, emilva, ErezYalon, flascelles, Guillaume Benats, IgorSasovets, Inonshk, JonnySchnittger, jmanico, jmdx, Keith Casey, kozmic, LauraRosePorter, Matthieu Estrade, nathanawmk, PauloASilva, pentagramz, philippederyck, pleothaud, r00ter, Raj kumar, Sagar Popat, Stephen Gates, thomaskonrad, xycloops123, Raphael Hagi, Eduardo Bellis, Bruno Barbosa Google Group Join the discussion on the OWASP API Security Project Google group . This is the best place to introduce yourself, ask questions, suggest and discuss any topic that is relevant to the project. GitHub Discussions You can also use GitHub Discussions as a place to connect with other community members, asking questions or sharing ideas. GitHub The project is maintain",
      "chunk_index": 4
    },
    {
      "chunk_id": "9a511da450d2cb01f65acc95f39f4c7255c24bef",
      "url": "https://owasp.org/www-project-api-security/",
      "title": "OWASP API Security Project",
      "text": "cuss any topic that is relevant to the project. GitHub Discussions You can also use GitHub Discussions as a place to connect with other community members, asking questions or sharing ideas. GitHub The project is maintained in the OWASP API Security Project repo . The latest changes are under the develop branch . Feel free to open or solve an issue . Ready to contribute directly into the repo? Great! Just make sure you read the How to Contribute guide .",
      "chunk_index": 5
    },
    {
      "chunk_id": "9f5a4ce476f7c279ecf704990cd4e300908907d4",
      "url": "https://cheatsheetseries.owasp.org/",
      "title": "OWASP Cheat Sheet Series",
      "text": "The OWASP Cheat Sheet Series was created to provide a concise collection of high value information on specific application security topics. These cheat sheets were created by various application security professionals who have expertise in specific topics. We hope that this project provides you with excellent security guidance in an easy to read format. You can download this site here . An ATOM feed is available here with the latest updates. Project leaders: Core team: Project links:",
      "chunk_index": 0
    },
    {
      "chunk_id": "076bc069e3e6477c7c1b692b7903d7e66c6e1da4",
      "url": "https://cloud.google.com/apis/design",
      "title": "API design guide",
      "text": "Changelog Introduction This is a general design guide for networked APIs. It has been used inside Google since 2014 and is the guide that Google follows when designing Cloud APIs and other Google APIs . This design guide is shared here to inform outside developers and to make it easier for us all to work together. Cloud Endpoints developers may find this guide particularly useful when designing gRPC APIs, and we strongly recommend such developers use these design principles. However, we don't mandate its use. You can use Cloud Endpoints and gRPC without following the guide. This guide applies to both REST APIs and RPC APIs, with specific focus on gRPC APIs. gRPC APIs use Protocol Buffers to define their API surface and API Service Configuration to configure their API services, including HTTP mapping, logging, and monitoring. HTTP mapping features are used by Google APIs and Cloud Endpoints gRPC APIs for JSON/HTTP to Protocol Buffers/RPC transcoding . This guide is a living document and additions to it will be made over time as new style and design patterns are adopted and approved. In that spirit, it is never going to be complete and there will always be ample room for the art and craft of API design. Conventions Used in This Guide The requirement level keywords \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" us",
      "chunk_index": 0
    },
    {
      "chunk_id": "373c68633c1f42ebf427ef1b08aa2e496030fa10",
      "url": "https://cloud.google.com/apis/design",
      "title": "API design guide",
      "text": "oom for the art and craft of API design. Conventions Used in This Guide The requirement level keywords \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" used in this document are to be interpreted as described in RFC 2119 . In this document, such keywords are highlighted using bold font. Sections Resource-oriented Design For information about implementing resource-oriented design for RPC and REST APIs, see AIP-121 . Resource Names For information about resource names, see AIP-122 . Standard Methods For general information about methods, see AIP-130 . For information about standard methods, see the following AIPs: Custom Methods For information about custom methods, see AIP-136 . Additional topics For information about the following topics, see their related AIPs. For Standard fields , see AIP-148 For Errors , see AIP-193 For Design patterns , see AIP guidance on design patterns For Inline API documentation , see AIP-192 For Using proto3 , see the Syntax section of AIP-191 For Versioning , see AIP-185 For Backward compatibility , see AIP-180 For File structure , see the File Layout section of AIP-191 For a Glossary of terms, see AIP-9 For Naming conventions , see AIP-190 For information about the following topics, see their related pages in this guide.",
      "chunk_index": 1
    },
    {
      "chunk_id": "77d5ed1d6b14acec9aef9d731489fe2c2d0ca3c0",
      "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
      "title": "OWASP Top 10 for Large Language Model Applications",
      "text": "About This Repository This is the repository for the OWASP Top 10 for Large Language Model Applications . However, this project has now grown into the comprehensive OWASP GenAI Security Project - a global initiative that encompasses multiple security initiatives beyond just the Top 10 list. OWASP GenAI Security Project The OWASP GenAI Security Project is a global, open-source initiative dedicated to identifying, mitigating, and documenting security and safety risks associated with generative AI technologies, including large language models (LLMs), agentic AI systems, and AI-driven applications. Our mission is to empower organizations, security professionals, AI practitioners, and policymakers with comprehensive, actionable guidance and tools to ensure the secure development, deployment, and governance of generative AI systems. Learn more about our mission and charter: Project Mission and Charter Visit our main project site: genai.owasp.org Latest Top 10 for LLM Applications The OWASP Top 10 for Large Language Model Applications continues to be a core component of our work, identifying the most critical security vulnerabilities in LLM applications. Access the latest Top 10 for LLM: https://genai.owasp.org/llm-top-10/ Project Background and Growth The project has evolved significantly since its inception. From a small group of security professionals addressing an urgent security ",
      "chunk_index": 0
    },
    {
      "chunk_id": "104e7c4b4bad57dd0ad094be79a413657f3d497d",
      "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
      "title": "OWASP Top 10 for Large Language Model Applications",
      "text": "est Top 10 for LLM: https://genai.owasp.org/llm-top-10/ Project Background and Growth The project has evolved significantly since its inception. From a small group of security professionals addressing an urgent security gap in 2023, it has grown into a global community with over 600 contributing experts from more than 18 countries and nearly 8,000 active community members. Read our full project background: Introduction and Background Get Involved Contribute to the Project We welcome all expert ideas, contributions, suggestions, and remarks from security professionals, researchers, developers, and anyone passionate about AI security. Learn how to contribute: https://genai.owasp.org/contribute/ Join Our Meetings Participate in our bi-weekly sync meetings and stay connected with the community. Meeting information: https://genai.owasp.org/meetings/ Project Support We are a not-for-profit, open-source, community-driven project. If you are interested in supporting the project with resources or becoming a sponsor to help us sustain community efforts and offset operational and outreach costs, visit the Sponsor Section on our website. Thank you to our current Sponsors and Supporters Educational Resources New to LLM Application security? Check out our resources page to learn more. OWASP Top 10 for Large Language Model Applications version 1.1 LLM01: Prompt Injection Manipulating LLMs via",
      "chunk_index": 1
    },
    {
      "chunk_id": "ea3a28e82883d047451a0114e7e30cd09071dab4",
      "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
      "title": "OWASP Top 10 for Large Language Model Applications",
      "text": "d Supporters Educational Resources New to LLM Application security? Check out our resources page to learn more. OWASP Top 10 for Large Language Model Applications version 1.1 LLM01: Prompt Injection Manipulating LLMs via crafted inputs can lead to unauthorized access, data breaches, and compromised decision-making. LLM02: Insecure Output Handling Neglecting to validate LLM outputs may lead to downstream security exploits, including code execution that compromises systems and exposes data. LLM03: Training Data Poisoning Tampered training data can impair LLM models leading to responses that may compromise security, accuracy, or ethical behavior. LLM04: Model Denial of Service Overloading LLMs with resource-heavy operations can cause service disruptions and increased costs. LLM05: Supply Chain Vulnerabilities Depending upon compromised components, services or datasets undermine system integrity, causing data breaches and system failures. LLM06: Sensitive Information Disclosure Failure to protect against disclosure of sensitive information in LLM outputs can result in legal consequences or a loss of competitive advantage. LLM07: Insecure Plugin Design LLM plugins processing untrusted inputs and having insufficient access control risk severe exploits like remote code execution. LLM08: Excessive Agency Granting LLMs unchecked autonomy to take action can lead to unintended consequence",
      "chunk_index": 2
    },
    {
      "chunk_id": "f9d010ff2d192c6d153837f23769df04c7426524",
      "url": "https://owasp.org/www-project-top-10-for-large-language-model-applications/",
      "title": "OWASP Top 10 for Large Language Model Applications",
      "text": "ocessing untrusted inputs and having insufficient access control risk severe exploits like remote code execution. LLM08: Excessive Agency Granting LLMs unchecked autonomy to take action can lead to unintended consequences, jeopardizing reliability, privacy, and trust. LLM09: Overreliance Failing to critically assess LLM outputs can lead to compromised decision making, security vulnerabilities, and legal liabilities. LLM10: Model Theft Unauthorized access to proprietary large language models risks theft, competitive advantage, and dissemination of sensitive information.",
      "chunk_index": 3
    },
    {
      "chunk_id": "3824b4ccd5b7ba14263415b676c346b523095ce1",
      "url": "https://sre.google/",
      "title": "Google SRE - Site Reliability engineering",
      "text": "What is Site Reliability Engineering (SRE)? SRE is what you get when you treat operations as if itâs a software problem. Our mission is to protect, provide for, and progress the software and systems behind all of Googleâs public services â Google Search, Ads, Gmail, Android, YouTube, and App Engine, to name just a few â with an ever-watchful eye on their availability, latency, performance, and capacity.",
      "chunk_index": 0
    },
    {
      "chunk_id": "b17091ef4bde8185a2902cf6abcad504fce20de5",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "2024-06-27 : Clarified usage of x-extensible-enum for events in SHOULD use open-ended list of values (via examples ) for enumeration types . #807 2024-06-25 : Relaxed naming convention for date/time properties in SHOULD use naming convention for date/time properties . #811 2024-06-11 : Linked SHOULD use standard formats for time duration and interval properties (duration / period) from MUST use standard data formats . #810 2024-05-06 : Added new rule SHOULD select appropriate one of date or date-time format on selecting appropriate date or date-time format. #808 2024-04-16 : Removed sort example from simple query language in SHOULD design simple query languages using query parameters . Enhanced clarity for 'uid' usage and permission naming convention in MUST define and assign permissions (scopes) and MUST follow the naming convention for permissions (scopes) . #804 #801 2024-03-21 : Added best practices section for MUST prepare clients to accept compatible API extensions on handling compatible API extensions. #799 2024-03-05 : Updated security section about uid scope in MUST define and assign permissions (scopes) . #794 2024-02-29 : Improved guidance on POST usage in MUST use HTTP methods correctly . #791 2024-02-21 : Fixed discrepancy between SHOULD design APIs conservatively and MUST treat OpenAPI specification as open for extension by default regarding handling of unexpected",
      "chunk_index": 0
    },
    {
      "chunk_id": "9ca99d55138bcd1c3c49d00e27888ff829c72b28",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "ge in MUST use HTTP methods correctly . #791 2024-02-21 : Fixed discrepancy between SHOULD design APIs conservatively and MUST treat OpenAPI specification as open for extension by default regarding handling of unexpected fields. #793 2023-12-12 : Improved response code guidance in SHOULD only use most common HTTP status codes . #789 2023-11-22 : Added new rule MAY support asynchronous request processing for supporting asynchronous request processing. #787 2023-07-21 : Improved guidance on total counts in SHOULD avoid a total result count . #731 2023-05-12 : Added new rule SHOULD not use redirection codes recommending not to use redirection codes. #762 #781 2023-05-08 : Improved guidance on sanitizing JSON payload in SHOULD be aware of services not fully supporting JSON/unicode . #759 2023-04-18 : Added new rule SHOULD design single resource schema for reading and writing recommending to design single resource schema for reading and writing. Added exception for partner IAM to MUST secure endpoints . #764 #767 2022-12-20 : Clarify that event consumers must be robust against duplicates in MUST be robust against duplicates when consuming events . #749 2022-10-18 : Add X-Zalando-Customer to list of proprietary headers in SHOULD use only the specified proprietary Zalando headers . #743 2022-09-21 : Clarify that functional naming schema in MUST / SHOULD / MAY use functional naming sch",
      "chunk_index": 1
    },
    {
      "chunk_id": "cd5966a6dd48b2f8d95f8033dbec17f895451193",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "dd X-Zalando-Customer to list of proprietary headers in SHOULD use only the specified proprietary Zalando headers . #743 2022-09-21 : Clarify that functional naming schema in MUST / SHOULD / MAY use functional naming schema is a MUST / SHOULD / MAY rule. #740 2022-07-26 : Improve guidance for return code usage for (robust) create operations in MUST use HTTP methods correctly . #735 2022-07-21 : Improve format and time interval guidance in MUST use standard data formats and SHOULD use standard formats for time duration and interval properties . #733 2022-05-24 : Define next page link as optional in [pagination-fields] . #726 2022-04-19 : Change SHOULD avoid writing sensitive data to events from MUST to SHOULD avoid providing sensitive data with events. #723 2022-03-22 : More clarity about when error specification definitions can be omitted in MUST specify success and error responses . More clarity around HTTP status codes in MUST use official HTTP status codes . More clarity for avoiding null for boolean fields in MUST not use null for boolean properties . #715 #720 #721 2022-01-26 : Exclude 'type' from common field names in SHOULD use naming convention for date/time properties . Improve clarity around usage of extensible enums in SHOULD use open-ended list of values (via examples ) for enumeration types . #714 #717 2021-12-22 : Clarify that event id must not change in retry sit",
      "chunk_index": 2
    },
    {
      "chunk_id": "e22e2d5ce833cbd4e7339ec537f1833019a4b6f3",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "time properties . Improve clarity around usage of extensible enums in SHOULD use open-ended list of values (via examples ) for enumeration types . #714 #717 2021-12-22 : Clarify that event id must not change in retry situations when producers MUST provide unique event identifiers . #694 2021-12-09 : Improve clarity on PATCH and PUT usage in rule MUST use HTTP methods correctly . Only use codes registered via IANA in rule MUST use official HTTP status codes . cb0624b 2021-12-09 : event id must not change in retry situations when producers MUST provide unique event identifiers . 2021-11-24 : restructuring of the document and some rules. 2021-10-18 : new rule SHOULD use content negotiation, if clients may choose from different resource representations . 2021-10-12 : improve clarity on PATCH usage in rule MUST use HTTP methods correctly . 2021-08-24 : improve clarity on PUT usage in rule MUST use HTTP methods correctly . 2021-08-24 : only use codes registered via IANA in rule MUST use official HTTP status codes . 2021-08-17 : update formats per OpenAPI 3.1 in MUST use standard data formats . 2021-06-22 : MUST use standard data formats changed from SHOULD to MUST ; consistency for rules around standards for data. 2021-06-03 : MUST secure endpoints with clear distinction of OpenAPI security schemes, favoring bearer to oauth2 . 2021-06-01 : resolve uncertainties around 'occurred_at' s",
      "chunk_index": 3
    },
    {
      "chunk_id": "791276d1c79ad05b3073cb6f62c26d928289b88f",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "onsistency for rules around standards for data. 2021-06-03 : MUST secure endpoints with clear distinction of OpenAPI security schemes, favoring bearer to oauth2 . 2021-06-01 : resolve uncertainties around 'occurred_at' semantics of event metadata . 2021-05-25 : SHOULD use standard media types with API endpoint versioning as only custom media type usage exception. 2021-05-05 : define usage on resource-ids in PUT and POST in MUST use HTTP methods correctly . 2021-04-29 : improve clarity of MAY use standard headers . 2021-03-19 : clarity on MUST use JSON as payload data interchange format . 2021-03-15 : MUST provide explicit event ordering for data change events changed from SHOULD to MUST ; improve clarity around event ordering . 2021-03-19 : best practice section Cursor-based pagination in RESTful APIs 2021-02-16 : define how to reference models outside the api in MUST only use durable and immutable remote references . 2021-02-15 : improve guideline MUST support problem JSON — clients must be prepared to not receive problem return objects. 2021-01-19 : more details for GET with body and DELETE with body ( MUST use HTTP methods correctly ). 2020-09-29 : include models for headers to be included by reference in API definitions ( SHOULD use only the specified proprietary Zalando headers ) 2020-09-08 : add exception for legacy host names to MUST follow naming convention for hostname",
      "chunk_index": 4
    },
    {
      "chunk_id": "5a1083423982931102a542f3deb168c3c7e6ffb8",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "dels for headers to be included by reference in API definitions ( SHOULD use only the specified proprietary Zalando headers ) 2020-09-08 : add exception for legacy host names to MUST follow naming convention for hostnames 2020-08-25 : change SHOULD declare enum values using UPPER_SNAKE_CASE string from MUST to SHOULD , explain exceptions 2020-08-25 : add exception for self to MUST identify resources and sub-resources via path segments and MUST pluralize resource names . 2020-08-24 : change \" MUST avoid trailing slashes\" to MUST use normalized paths without empty path segments and trailing slashes . 2020-08-20 : change SHOULD use only the specified proprietary Zalando headers from MUST to SHOULD , mention gateway-specific headers (which are not part of the public API). 2020-06-30 : add details to MUST use media type versioning 2020-05-19 : new sections about DELETE with query parameters and DELETE with body in MUST use HTTP methods correctly . 2020-02-06 : new rule MAY expose compound keys as resource identifiers 2020-02-05 : add Sunset header, clarify deprecation producedure ( MUST obtain approval of clients before API shut down , MUST collect external partner consent on deprecation time span , MUST reflect deprecation in API specifications , MUST monitor usage of deprecated API scheduled for sunset , SHOULD add Deprecation and Sunset header to responses , SHOULD add monitoring",
      "chunk_index": 5
    },
    {
      "chunk_id": "ef0974e58975a4a2e1f11a8c67612be66ef8e7fb",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "onsent on deprecation time span , MUST reflect deprecation in API specifications , MUST monitor usage of deprecated API scheduled for sunset , SHOULD add Deprecation and Sunset header to responses , SHOULD add monitoring for Deprecation and Sunset header , MUST not start using deprecated APIs ) 2020-01-21 : new rule SHOULD declare enum values using UPPER_SNAKE_CASE string (as MUST, changed later to SHOULD) 2020-01-15 : change \"Warning\" to \"Deprecation\" header in SHOULD add Deprecation and Sunset header to responses , SHOULD add monitoring for Deprecation and Sunset header . 2019-10-10 : remove never-implemented rule \" MUST Permissions on events must correspond to API permissions\" 2019-09-10 : remove duplicated rule \" MAY Standards could be used for Language, Country and Currency\", upgrade MUST use standard formats for country, language and currency properties from MAY to SHOULD . 2019-08-29 : new rule MUST encode binary data in base64url , extend MUST use JSON as payload data interchange format pointing to RFC-7493 2019-08-29 : new rules SHOULD design simple query languages using query parameters , SHOULD design complex query languages using JSON 2019-07-30 : new rule MUST use standard data formats 2019-07-30 : change MUST use the common money object from SHOULD to MUST 2019-07-30 : change \" SHOULD Null values should have their fields removed to\" MUST use same semantics for nul",
      "chunk_index": 6
    },
    {
      "chunk_id": "13c2842bcf45356367f25ad3b5f781dc8f32a213",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "w rule MUST use standard data formats 2019-07-30 : change MUST use the common money object from SHOULD to MUST 2019-07-30 : change \" SHOULD Null values should have their fields removed to\" MUST use same semantics for null and absent properties . 2019-07-25 : new rule SHOULD use naming convention for date/time properties . 2019-07-18 : improved cursor guideline for GET with body . 2019-06-25 : change MUST define collection format of header and query parameters from SHOULD to MUST , use OpenAPI 3 syntax 2019-06-13 : remove X-App-Domain from SHOULD use only the specified proprietary Zalando headers . 2019-05-17 : add X-Mobile-Advertising-Id to SHOULD use only the specified proprietary Zalando headers . 2019-04-09 New rule MUST only use durable and immutable remote references 2019-02-19 : New rule MUST support X-Flow-ID extracted + expanded from SHOULD use only the specified proprietary Zalando headers . 2019-01-24: Improve guidance on caching ( MUST fulfill common method properties , MUST document cacheable GET , HEAD , and POST endpoints ). 2019-01-21: Improve guidance on idempotency, introduce idempotency-key ( SHOULD consider to design POST and PATCH idempotent , SHOULD use secondary key for idempotent POST design ). 2019-01-16 : Change SHOULD not use /api as base path from MAY to {SHOULD NOT} 2018-10-19 : Add ordering_key_field to event type definition schema ( MUST specify an",
      "chunk_index": 7
    },
    {
      "chunk_id": "bc4bea43e6a51b5936c230361e5ad022a09d68ce",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": ", SHOULD use secondary key for idempotent POST design ). 2019-01-16 : Change SHOULD not use /api as base path from MAY to {SHOULD NOT} 2018-10-19 : Add ordering_key_field to event type definition schema ( MUST specify and register events as event types , SHOULD provide explicit event ordering for general events ) 2018-09-28 : New rule MUST use URL-friendly resource identifiers 2018-09-13 : replaced OpenAPI 2.0 syntax with OpenAPI 3.0 in the example snippets 2018-08-10 : New rule MUST document implicit response filtering 2018-07-12 : Add audience field to event type definition ( MUST specify and register events as event types ) 2018-06-11: Introduced new naming guidelines for host, permission, and event names. 2018-01-10: Moved meta information related aspects into new chapter REST Basics - Meta information . 2018-01-09: Changed publication requirements for API specifications ( MUST publish OpenAPI specification for APIs ). 2017-12-07: Added best practices section including discussion about optimistic locking approaches. 2017-11-28: Changed OAuth flow example from password to client credentials in REST Basics - Security . 2017-11-22: Updated description of X-Tenant-ID header field 2017-08-22: Migration to Asciidoc 2017-07-20: Be more precise on client vs. server obligations for compatible API extensions. 2017-06-06: Made money object guideline clearer. 2017-05-17: Added guidelin",
      "chunk_index": 8
    },
    {
      "chunk_id": "ee9f065a9c4c975cdcaea55bcf9bbe746d91a97c",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": "nt-ID header field 2017-08-22: Migration to Asciidoc 2017-07-20: Be more precise on client vs. server obligations for compatible API extensions. 2017-06-06: Made money object guideline clearer. 2017-05-17: Added guideline on query parameter collection format. 2017-05-10: Added the convention of using RFC2119 to describe guideline levels, and replaced book.could with book.may . 2017-03-30: Added rule that permissions on resources in events must correspond to permissions on API resources 2017-03-30: Added rule that APIs should be modelled around business processes 2017-02-28: Extended information about how to reference sub-resources and the usage of composite identifiers in the MUST identify resources and sub-resources via path segments part. 2017-02-22: Added guidance for conditional requests with If-Match/If-None-Match 2017-02-02: Added guideline for batch and bulk request 2017-02-01: SHOULD use Location header instead of Content-Location header 2017-01-18: Removed \"Avoid Javascript Keywords\" rule 2017-01-05: Clarification on the usage of the term \"REST/RESTful\" 2016-12-07: Introduced \"API as a Product\" principle 2016-12-06: New guideline: \"Should Only Use UUIDs If Necessary\" 2016-12-04: Changed OAuth flow example from implicit to password in REST Basics - Security . 2016-10-13: SHOULD use standard media types 2016-10-10: Introduced the changelog. From now on all rule changes o",
      "chunk_index": 9
    },
    {
      "chunk_id": "d5ba4a8392aaeda19e4e1f44f08b7292b1bf06ce",
      "url": "https://opensource.zalando.com/restful-api-guidelines/",
      "title": "Zalando RESTful API and Event Guidelines",
      "text": " If Necessary\" 2016-12-04: Changed OAuth flow example from implicit to password in REST Basics - Security . 2016-10-13: SHOULD use standard media types 2016-10-10: Introduced the changelog. From now on all rule changes on API guidelines will be recorded here.",
      "chunk_index": 10
    },
    {
      "chunk_id": "78d24bd21d9c1b8c8750ec57edc8c091b60bc436",
      "url": "https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html",
      "title": "Elastic fundamentals",
      "text": "Â© 2026 Elasticsearch B.V. All Rights Reserved. This content is available in different formats for convenience only. All original licensing terms apply. Elasticsearch is a trademark of Elasticsearch B.V., registered in the U.S. and in other countries. Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the Apache Software Foundation in the United States and/or other countries.",
      "chunk_index": 0
    },
    {
      "chunk_id": "59511448862a0c890608cf61260e60b417a57031",
      "url": "https://prometheus.io/docs/introduction/overview/",
      "title": "Overview | Prometheus",
      "text": "What is Prometheus? Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud . Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community . It is now a standalone open source project and maintained independently of any company. To emphasize this, and to clarify the project's governance structure, Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes . Prometheus collects and stores its metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels. For more elaborate overviews of Prometheus, see the resources linked from the media section. Features Prometheus's main features are: a multi-dimensional data model with time series data identified by metric name and key/value pairs PromQL, a flexible query language to leverage this dimensionality no reliance on distributed storage; single server nodes are autonomous time series collection happens via a pull model over HTTP pushing time series is supported via an intermediary gateway targets are discovered via service discovery or static configuration multiple modes of graphing and dashboarding support What are metrics? Metrics are numerical measurement",
      "chunk_index": 0
    },
    {
      "chunk_id": "37ffa150b2eab87c40e72637efeef0ee7f714374",
      "url": "https://prometheus.io/docs/introduction/overview/",
      "title": "Overview | Prometheus",
      "text": "series is supported via an intermediary gateway targets are discovered via service discovery or static configuration multiple modes of graphing and dashboarding support What are metrics? Metrics are numerical measurements in layperson terms. The term time series refers to the recording of changes over time. What users want to measure differs from application to application. For a web server, it could be request times; for a database, it could be the number of active connections or active queries, and so on. Metrics play an important role in understanding why your application is working in a certain way. Let's assume you are running a web application and discover that it is slow. To learn what is happening with your application, you will need some information. For example, when the number of requests is high, the application may become slow. If you have the request count metric, you can determine the cause and increase the number of servers to handle the load. Components The Prometheus ecosystem consists of multiple components, many of which are optional: Most Prometheus components are written in Go , making them easy to build and deploy as static binaries. Architecture This diagram illustrates the architecture of Prometheus and some of its ecosystem components: Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived",
      "chunk_index": 1
    },
    {
      "chunk_id": "02c7165462610f3df0963b9b7716b5738120a09e",
      "url": "https://prometheus.io/docs/introduction/overview/",
      "title": "Overview | Prometheus",
      "text": "tecture This diagram illustrates the architecture of Prometheus and some of its ecosystem components: Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally and runs rules over this data to either aggregate and record new time series from existing data or generate alerts. Grafana or other API consumers can be used to visualize the collected data. When does it fit? Prometheus works well for recording any purely numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength. Prometheus is designed for reliability, to be the system you go to during an outage to allow you to quickly diagnose problems. Each Prometheus server is standalone, not depending on network storage or other remote services. You can rely on it when other parts of your infrastructure are broken, and you do not need to setup extensive infrastructure to use it. When does it not fit? Prometheus values reliability. You can always view what statistics are available about your system, even under failure conditions. If you need 100% accuracy, such as for per-request billing, Prometheus is not a good choice as the collected data ",
      "chunk_index": 2
    },
    {
      "chunk_id": "a2dbb85b69ad87084447292891629a890b132d35",
      "url": "https://prometheus.io/docs/introduction/overview/",
      "title": "Overview | Prometheus",
      "text": "lity. You can always view what statistics are available about your system, even under failure conditions. If you need 100% accuracy, such as for per-request billing, Prometheus is not a good choice as the collected data will likely not be detailed and complete enough. In such a case you would be best off using some other system to collect and analyze the data for billing, and Prometheus for the rest of your monitoring.",
      "chunk_index": 3
    },
    {
      "chunk_id": "7f889e9638cf620bea6c9d6f84b92209268549ff",
      "url": "https://grafana.com/docs/grafana/latest/fundamentals/",
      "title": "Introduction | Grafana documentation",
      "text": "Grafana Cloud Enterprise Open source Introduction This section provides basic information about observability topics in general and Grafana in particular. These topics will help people who are just starting out with observability and monitoring. Grafana Open Source Grafana open source is open source visualization and analytics software. It allows you to query, visualize, alert on, and explore your metrics, logs, and traces no matter where they are stored. It provides you with tools to turn your time-series database (TSDB) data into insightful graphs and visualizations. Grafana Loki Grafana Loki is an open-source set of components that can be composed into a fully featured logging stack. For more information, refer to Loki documentation . Grafana Tempo Grafana Tempo is an open source, easy-to-use and high-volume distributed tracing backend. For more information, refer to Tempo documentation . Grafana Mimir Grafana Mimir is an open source software project that provides a scalable long-term storage for Prometheus. For more information about Grafana Mimir, refer to Grafana Mimir documentation . Grafana Pyroscope Grafana Pyroscope is an open source software project for aggregating continuous profiling data. Continuous profiling is an observability signal that helps you understand your workloadâs resources usage. For more information, refer to Grafana Pyroscope documentation . Graf",
      "chunk_index": 0
    },
    {
      "chunk_id": "23cd872c572cfe577ef2b60c7a4740575b524a07",
      "url": "https://grafana.com/docs/grafana/latest/fundamentals/",
      "title": "Introduction | Grafana documentation",
      "text": "or aggregating continuous profiling data. Continuous profiling is an observability signal that helps you understand your workloadâs resources usage. For more information, refer to Grafana Pyroscope documentation . Grafana Oncall Grafana OnCall is an open source incident response management tool built to help teams improve their collaboration and resolve incidents faster. For more information about Grafana OnCall, refer to Grafana OnCall documentation . Grafana Cloud Grafana Cloud is a highly available, fast, fully managed OpenSaaS logging and metrics platform. It is everything you love about Grafana, but Grafana Labs hosts it for you and handles all the headaches. Grafana Enterprise Grafana Enterprise is the commercial edition of Grafana that includes additional features not found in the open source version. Building on everything you already know and love about Grafana, Grafana Enterprise adds enterprise data sources, advanced authentication options, more permission controls, 24x7x365 support, and training from the core Grafana team. Learn more about Grafana Enterprise and contact the Grafana Labs Sales Team to purchase an Enterprise license. You can also obtain a trial license before the purchase.",
      "chunk_index": 1
    },
    {
      "chunk_id": "eca1b1426202184031064143cecb67336bac10ec",
      "url": "https://docs.gitlab.com/ee/ci/",
      "title": "Get started with GitLab CI/CD",
      "text": "Tier : Free, Premium, Ultimate Offering : GitLab.com, GitLab Self-Managed, GitLab Dedicated CI/CD is a continuous method of software development, where you continuously build, test, deploy, and monitor iterative code changes. This iterative process helps reduce the chance that you develop new code based on buggy or failed previous versions. GitLab CI/CD can catch bugs early in the development cycle, and help ensure that the code deployed to production complies with your established code standards. This process is part of a larger workflow: Step 1: Configure your pipeline To use GitLab CI/CD, you start with a .gitlab-ci.yml file at the root of your project. This file specifies the stages, jobs, and scripts to be executed during your CI/CD pipeline. It is a YAML file with its own custom syntax. By default, the file is named .gitlab-ci.yml , but you can use any filename. In this file, you define variables, dependencies between jobs, and specify when and how each job should be executed. A pipeline is defined in the .gitlab-ci.yml file, and executes when the file runs on a runner. Pipelines are made up of stages and jobs: Stages define the order of execution. Typical stages might be build , test , and deploy . Jobs specify the tasks to be performed in each stage. For example, a job can compile or test code. Pipelines can be triggered by various events, like commits or merges, or can",
      "chunk_index": 0
    },
    {
      "chunk_id": "03d2e89fdc64f6b2d5b160ba2bfbe3b1a2afc13d",
      "url": "https://docs.gitlab.com/ee/ci/",
      "title": "Get started with GitLab CI/CD",
      "text": " stages might be build , test , and deploy . Jobs specify the tasks to be performed in each stage. For example, a job can compile or test code. Pipelines can be triggered by various events, like commits or merges, or can be on schedule. In your pipeline, you can integrate with a wide range of tools and platforms. For more information, see: Step 2: Find or create runners Runners are the agents that run your jobs. These agents can run on physical machines or virtual instances. In your .gitlab-ci.yml file, you can specify a container image you want to use when running the job. The runner loads the image, clones your project, and runs the job either locally or in the container. If you use GitLab.com, runners on Linux, Windows, and macOS are already available for use. If needed, you can also register your own runners. If you don’t use GitLab.com, you can: Register runners or use runners already registered for your GitLab Self-Managed instance. Create a runner on your local machine. For more information, see: Step 3: Use CI/CD variables and expressions GitLab CI/CD variables are key-value pairs you use to store and pass configuration settings and sensitive information, like passwords or API keys, to jobs in a pipeline. GitLab CI/CD expressions allow you to inject data dynamically into your pipeline configuration. The data available depends on the expression context. For example, the ",
      "chunk_index": 1
    },
    {
      "chunk_id": "0e58089376394c7ee546b37bcd232aae2c19a0d7",
      "url": "https://docs.gitlab.com/ee/ci/",
      "title": "Get started with GitLab CI/CD",
      "text": ", like passwords or API keys, to jobs in a pipeline. GitLab CI/CD expressions allow you to inject data dynamically into your pipeline configuration. The data available depends on the expression context. For example, the inputs context allows you to access information passed into the configuration file from a parent file or when a pipeline is run. CI/CD variables Use CI/CD variables to customize jobs by making values defined elsewhere accessible to jobs. You can hard-code CI/CD variables in your .gitlab-ci.yml file, set them in your project settings, or generate them dynamically. You can define them for the project, group, or instance. The following types of variables are available: Custom variables: Variables that you create and manage in the UI, API, or configuration files. Predefined variables: Variables that GitLab automatically sets to provide information about the current job, pipeline, and environment. You can configure variables with security settings: Protected variables: Restrict access to jobs running on protected branches or tags. Masked variables: Hide variable values in job logs to prevent sensitive information from being exposed. For more information, see: CI/CD expressions CI/CD expressions use the $[[ ]] syntax and are validated when you create a pipeline. You can also validate expressions in the pipeline editor before committing changes. Expressions enable dyna",
      "chunk_index": 2
    },
    {
      "chunk_id": "42e3f673bd0c42a3e707c45b3351a695217c40fe",
      "url": "https://docs.gitlab.com/ee/ci/",
      "title": "Get started with GitLab CI/CD",
      "text": "on, see: CI/CD expressions CI/CD expressions use the $[[ ]] syntax and are validated when you create a pipeline. You can also validate expressions in the pipeline editor before committing changes. Expressions enable dynamic configuration based on different contexts: Inputs context ( $[[ inputs.INPUT_NAME ]] ): Access typed parameters passed into configuration files with include:inputs or when a new pipeline is run Matrix context ( $[[ matrix.IDENTIFIER ]] ): Access matrix values in job dependencies to create 1:1 mappings between matrix jobs For more information, see: Step 4: Use CI/CD components A CI/CD component is a reusable pipeline configuration unit. Use a CI/CD component to compose an entire pipeline configuration or a small part of a larger pipeline. You can add a component to your pipeline configuration with include:component . Reusable components help reduce duplication, improve maintainability, and promote consistency across projects. Create a component project and publish it to the CI/CD Catalog to share your component across multiple projects. GitLab also has CI/CD component templates for common tasks and integrations. For more information, see:",
      "chunk_index": 3
    },
    {
      "chunk_id": "f94e4cf2b8d96216999cb4608a5c95339ad4cdad",
      "url": "https://argo-cd.readthedocs.io/en/stable/",
      "title": "Declarative GitOps CD for Kubernetes",
      "text": "Overview What Is Argo CD? Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Why Argo CD? Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Getting Started Quick Start kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Follow our getting started guide . Further user oriented documentation is provided for additional features. If you are looking to upgrade Argo CD, see the upgrade guide . Developer oriented documentation is available for people interested in building third-party integrations. How it works Argo CD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways: kustomize applications helm charts jsonnet files Plain directory of YAML/json manifests Any custom config management tool configured as a config management plugin Argo CD automates the deployment of the desired application states in the specified target environments. Application deployments can track updates to branches, tags, or be pinned to a specific version of manifests at a Git commit. See tracking strategies for additional detai",
      "chunk_index": 0
    },
    {
      "chunk_id": "4c0a05490685f4109314413911f85b3ccf547239",
      "url": "https://argo-cd.readthedocs.io/en/stable/",
      "title": "Declarative GitOps CD for Kubernetes",
      "text": "tion states in the specified target environments. Application deployments can track updates to branches, tags, or be pinned to a specific version of manifests at a Git commit. See tracking strategies for additional details about the different tracking strategies available. For a quick 10 minute overview of Argo CD, check out the demo presented to the Sig Apps community meeting: Architecture Argo CD is implemented as a Kubernetes controller which continuously monitors running applications and compares the current, live state against the desired target state (as specified in the Git repo). A deployed application whose live state deviates from the target state is considered OutOfSync . Argo CD reports & visualizes the differences, while providing facilities to automatically or manually sync the live state back to the desired target state. Any modifications made to the desired target state in the Git repo can be automatically applied and reflected in the specified target environments. For additional details, see architecture overview . Features Automated deployment of applications to specified target environments Support for multiple config management/templating tools (Kustomize, Helm, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authoriz",
      "chunk_index": 1
    },
    {
      "chunk_id": "96fb75552751b77f5dedf1e0fc62783c776ce438",
      "url": "https://argo-cd.readthedocs.io/en/stable/",
      "title": "Declarative GitOps CD for Kubernetes",
      "text": "ls (Kustomize, Helm, Jsonnet, plain-YAML) Ability to manage and deploy to multiple clusters SSO Integration (OIDC, OAuth2, LDAP, SAML 2.0, GitHub, GitLab, Microsoft, LinkedIn) Multi-tenancy and RBAC policies for authorization Rollback/Roll-anywhere to any application configuration committed in Git repository Health status analysis of application resources Automated configuration drift detection and visualization Automated or manual syncing of applications to its desired state Web UI which provides real-time view of application activity CLI for automation and CI integration Webhook integration (GitHub, BitBucket, GitLab) Access tokens for automation PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades) Audit trails for application events and API calls Prometheus metrics Parameter overrides for overriding helm parameters in Git Development Status Argo CD is being actively developed by the community. Our releases can be found here . Adoption Organizations who have officially adopted Argo CD can be found here .",
      "chunk_index": 2
    },
    {
      "chunk_id": "0741a4109ad0a617ae891b8941602e0341a6c328",
      "url": "https://www.jenkins.io/doc/",
      "title": "Jenkins User Documentation",
      "text": "Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed.",
      "chunk_index": 0
    },
    {
      "chunk_id": "ea7b11bfa751607b47d024d3b16caa0bce4f665c",
      "url": "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions",
      "title": "Understanding GitHub Actions",
      "text": "GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. GitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository. GitHub provides Linux, Windows, and macOS virtual machines to run your workflows, or you can host your own self-hosted runners in your own data center or cloud infrastructure. You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner , or inside a container, and has one or more steps that either run a script that you define or run an action , which is a reusable extension that can simplify your workflow. A workflow is a configurable automated process that will run one or more jobs. Workflows are defined by a YAML file checked in to your repository and will run when triggered by an event in your repository, o",
      "chunk_index": 0
    },
    {
      "chunk_id": "96fdccba9e88bfb88dce7c1d12730e335f4edb34",
      "url": "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions",
      "title": "Understanding GitHub Actions",
      "text": "ur workflow. A workflow is a configurable automated process that will run one or more jobs. Workflows are defined by a YAML file checked in to your repository and will run when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule. Workflows are defined in the .github/workflows directory in a repository. A repository can have multiple workflows, each of which can perform a different set of tasks such as: Building and testing pull requests Deploying your application every time a release is created Adding a label whenever a new issue is opened You can reference a workflow within another workflow. For more information, see Reuse workflows . For more information, see Writing workflows . An event is a specific activity in a repository that triggers a workflow run. For example, an activity can originate from GitHub when someone creates a pull request, opens an issue, or pushes a commit to a repository. You can also trigger a workflow to run on a schedule , by posting to a REST API , or manually. For a complete list of events that can be used to trigger workflows, see Events that trigger workflows . A job is a set of steps in a workflow that is executed on the same runner . Each step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other. Since each step ",
      "chunk_index": 1
    },
    {
      "chunk_id": "8adfa8ec7d5f9118ec61bd71d39e204e015fb517",
      "url": "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions",
      "title": "Understanding GitHub Actions",
      "text": "s in a workflow that is executed on the same runner . Each step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other. Since each step is executed on the same runner, you can share data from one step to another. For example, you can have a step that builds your application followed by a step that tests the application that was built. You can configure a job's dependencies with other jobs; by default, jobs have no dependencies and run in parallel. When a job takes a dependency on another job, it waits for the dependent job to complete before running. You can also use a matrix to run the same job multiple times, each with a different combination of variables—like operating systems or language versions. For example, you might configure multiple build jobs for different architectures without any job dependencies and a packaging job that depends on those builds. The build jobs run in parallel, and once they complete successfully, the packaging job runs. For more information, see Choosing what your workflow does . An action is a pre-defined, reusable set of jobs or code that performs specific tasks within a workflow , reducing the amount of repetitive code you write in your workflow files. Actions can perform tasks such as: Pulling your Git repository from GitHub Setting up the correct toolchain for ",
      "chunk_index": 2
    },
    {
      "chunk_id": "4a319ee4ac2a1c80ced6a0f942c69e24e777d7ee",
      "url": "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions",
      "title": "Understanding GitHub Actions",
      "text": "specific tasks within a workflow , reducing the amount of repetitive code you write in your workflow files. Actions can perform tasks such as: Pulling your Git repository from GitHub Setting up the correct toolchain for your build environment Setting up authentication to your cloud provider You can write your own actions, or you can find actions to use in your workflows in the GitHub Marketplace. For more information on actions, see Reusing automations . A runner is a server that runs your workflows when they're triggered. Each runner can run a single job at a time. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows . Each workflow run executes in a fresh, newly-provisioned virtual machine. GitHub also offers larger runners, which are available in larger configurations. For more information, see Using larger runners . If you need a different operating system or require a specific hardware configuration, you can host your own runners. For more information about self-hosted runners, see Managing self-hosted runners . GitHub Actions can help you automate nearly every aspect of your application development processes. Ready to get started? Here are some helpful resources for taking your next steps with GitHub Actions: To create a GitHub Actions workflow, see Using workflow templates . For continuous integration (CI) workflows, see Building and t",
      "chunk_index": 3
    },
    {
      "chunk_id": "9e2e294da6e97f2a43700dbbe1b1eecbaaf9bf78",
      "url": "https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions",
      "title": "Understanding GitHub Actions",
      "text": "t started? Here are some helpful resources for taking your next steps with GitHub Actions: To create a GitHub Actions workflow, see Using workflow templates . For continuous integration (CI) workflows, see Building and testing your code . For building and publishing packages, see Publishing packages . For deploying projects, see Deploying to third-party platforms . For automating tasks and processes on GitHub, see Managing your work with GitHub Actions . For examples that demonstrate more complex features of GitHub Actions, see Managing your work with GitHub Actions . These detailed examples explain how to test your code on a runner, access the GitHub CLI, and use advanced features such as concurrency and test matrices. To certify your proficiency in automating workflows and accelerating development with GitHub Actions, earn a GitHub Actions certificate with GitHub Certifications. For more information, see About GitHub Certifications .",
      "chunk_index": 4
    },
    {
      "chunk_id": "9f50968d2a2175a4c4ff21ee06fa7f0289000a3a",
      "url": "https://opentelemetry.io/docs/",
      "title": "Documentation | OpenTelemetry",
      "text": "Documentation OpenTelemetry, also known as OTel, is a vendor-neutral open source Observability framework for instrumenting, generating, collecting, and exporting telemetry data such as traces , metrics , and logs . As an industry-standard, OpenTelemetry is supported by more than 90 observability vendors , integrated by many libraries, services, and apps , and adopted by numerous end users . A brief explanation of what OpenTelemetry is and isn’t. Get started with OpenTelemetry based on your role. Key concepts in OpenTelemetry OpenTelemetry code instrumentation is supported for many popular programming languages Hardware & software platforms and environments supporting OpenTelemetry Vendor-agnostic way to receive, process and export telemetry data. How to migrate to OpenTelemetry Feedback Was this page helpful? Yes No Thank you. Your feedback is appreciated! Please let us know how we can improve this page . Your feedback is appreciated!",
      "chunk_index": 0
    },
    {
      "chunk_id": "6eec669669e5cefedad51856fb47c7ca86ee4315",
      "url": "https://www.rfc-editor.org/rfc/rfc9110",
      "title": "RFC 9110: HTTP Semantics",
      "text": "Aside from the current editors, the following individuals deserve special recognition for their contributions to early aspects of HTTP and its core specifications: Marc Andreessen , Tim Berners-Lee , Robert Cailliau , Daniel W. Connolly , Bob Denny , John Franks , Jim Gettys , Jean-François Groff , Phillip M. Hallam-Baker , Koen Holtman , Jeffery L. Hostetler , Shel Kaphan , Dave Kristol , Yves Lafon , Scott D. Lawrence , Paul J. Leach , Håkon W. Lie , Ari Luotonen , Larry Masinter , Rob McCool , Jeffrey C. Mogul , Lou Montulli , David Morris , Henrik Frystyk Nielsen , Dave Raggett , Eric Rescorla , Tony Sanders , Lawrence C. Stewart , Marc VanHeyningen , and Steve Zilles . ¶ This document builds on the many contributions that went into past specifications of HTTP, including [ HTTP/1.0 ] , [ RFC2068 ] , [ RFC2145 ] , [ RFC2616 ] , [ RFC2617 ] , [ RFC2818 ] , [ RFC7230 ] , [ RFC7231 ] , [ RFC7232 ] , [ RFC7233 ] , [ RFC7234 ] , and [ RFC7235 ] . The acknowledgements within those documents still apply. ¶ Since 2014, the following contributors have helped improve this specification by reporting bugs, asking smart questions, drafting or reviewing text, and evaluating issues: ¶ Alan Egerton , Alex Rousskov , Amichai Rothman , Amos Jeffries , Anders Kaseorg , Andreas Gebhardt , Anne van Kesteren , Armin Abfalterer , Aron Duby , Asanka Herath , Asbjørn Ulsberg , Asta Olofsson , Attila",
      "chunk_index": 0
    },
    {
      "chunk_id": "a0d572e7f4d3baade342410cb54e918ec2e721d3",
      "url": "https://www.rfc-editor.org/rfc/rfc9110",
      "title": "RFC 9110: HTTP Semantics",
      "text": "g issues: ¶ Alan Egerton , Alex Rousskov , Amichai Rothman , Amos Jeffries , Anders Kaseorg , Andreas Gebhardt , Anne van Kesteren , Armin Abfalterer , Aron Duby , Asanka Herath , Asbjørn Ulsberg , Asta Olofsson , Attila Gulyas , Austin Wright , Barry Pollard , Ben Burkert , Benjamin Kaduk , Björn Höhrmann , Brad Fitzpatrick , Chris Pacejo , Colin Bendell , Cory Benfield , Cory Nelson , Daisuke Miyakawa , Dale Worley , Daniel Stenberg , Danil Suits , David Benjamin , David Matson , David Schinazi , Дилян Палаузов ( Dilyan Palauzov ) , Eric Anderson , Eric Rescorla , Éric Vyncke , Erik Kline , Erwin Pe , Etan Kissling , Evert Pot , Evgeny Vrublevsky , Florian Best , Francesca Palombini , Igor Lubashev , James Callahan , James Peach , Jeffrey Yasskin , Kalin Gyokov , Kannan Goundan , 奥 一穂 ( Kazuho Oku ) , Ken Murchison , Krzysztof Maczyński , Lars Eggert , Lucas Pardue , Martin Duke , Martin Dürst , Martin Thomson , Martynas Jusevičius , Matt Menke , Matthias Pigulla , Mattias Grenfeldt , Michael Osipov , Mike Bishop , Mike Pennisi , Mike Taylor , Mike West , Mohit Sethi , Murray Kucherawy , Nathaniel J. Smith , Nicholas Hurley , Nikita Prokhorov , Patrick McManus , Piotr Sikora , Poul-Henning Kamp , Rick van Rein , Robert Wilton , Roberto Polli , Roman Danyliw , Samuel Williams , Semyon Kholodnov , Simon Pieters , Simon Schüppel , Stefan Eissing , Taylor Hunt , Todd Greer , Tomm",
      "chunk_index": 1
    },
    {
      "chunk_id": "c03c437bfb3ac9df5e8ab4932eb0b82d0d95aed3",
      "url": "https://www.rfc-editor.org/rfc/rfc9110",
      "title": "RFC 9110: HTTP Semantics",
      "text": " , Piotr Sikora , Poul-Henning Kamp , Rick van Rein , Robert Wilton , Roberto Polli , Roman Danyliw , Samuel Williams , Semyon Kholodnov , Simon Pieters , Simon Schüppel , Stefan Eissing , Taylor Hunt , Todd Greer , Tommy Pauly , Vasiliy Faronov , Vladimir Lashchev , Wenbo Zhu , William A. Rowe Jr. , Willy Tarreau , Xingwei Liu , Yishuai Li , and Zaheduzzaman Sarker . ¶",
      "chunk_index": 2
    },
    {
      "chunk_id": "885509c30e6a01234230279f860d2202c308e720",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "Overview Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services that facilitate both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. This page is an overview of Kubernetes. The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the \"K\" and the \"s\". Google open sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production workloads at scale with best-of-breed ideas and practices from the community. Why you need Kubernetes and what it can do Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system? That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system. Kubernetes provides you with: Ser",
      "chunk_index": 0
    },
    {
      "chunk_id": "5e5e6a38b6dd002d7a72a8e130f17ee88d8b2cf3",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "iently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system. Kubernetes provides you with: Service discovery and load balancing Kubernetes can expose a container using a DNS name or their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storage, public cloud providers, and more. Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health chec",
      "chunk_index": 1
    },
    {
      "chunk_id": "99f881dc12ad2ceafa351021bd9ff2f93f2d9df7",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": " can fit containers onto your nodes to make the best use of your resources. Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve. Secret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. Batch execution In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired. Horizontal scaling Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage. IPv4/IPv6 dual-stack Allocation of IPv4 and IPv6 addresses to Pods and Services Designed for extensibility Add features to your Kubernetes cluster without changing upstream source code. What Kubernetes is not Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, and lets users integrate their logging, monit",
      "chunk_index": 2
    },
    {
      "chunk_id": "cbbb4476b6c3ebc55e2e5d4a9f2d138af21b754c",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "t the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, and lets users integrate their logging, monitoring, and alerting solutions. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important. Kubernetes: Does not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes. Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and preferences as well as technical requirements. Does not provide application-level services, such as middleware (for example, message buses), data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor cluster storage systems (for example, Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker . Doe",
      "chunk_index": 3
    },
    {
      "chunk_id": "897b1c80dceb6f4513f7e62bab837a5984e7d903",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "age systems (for example, Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker . Does not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms to collect and export metrics. Does not provide nor mandate a configuration language/system (for example, Jsonnet). It provides a declarative API that may be targeted by arbitrary forms of declarative specifications. Does not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems. Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible. Historical context for Kubernetes Let's take a look at why Kubernetes is so useful by going back in time. Traditional deployment era: Early on, organiza",
      "chunk_index": 4
    },
    {
      "chunk_id": "a5cfc22fb23509f7edd03984b12b3b963156bf3b",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "easier to use and more powerful, robust, resilient, and extensible. Historical context for Kubernetes Let's take a look at why Kubernetes is so useful by going back in time. Traditional deployment era: Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers. Virtualized deployment era: As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set",
      "chunk_index": 5
    },
    {
      "chunk_id": "cab242a5fb4a40c1a4acd618a99e8508d72c1da7",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "s better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as a cluster of disposable virtual machines. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware. Container deployment era: Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions. Containers have become popular because they provide extra benefits, such as: Agile application creation and deployment: increased ease and efficiency of container image creation compared to VM image use. Continuous development, integration, and deployment: provides reliable and frequent container image build and deployment with quick and efficient rollbacks (due to image immutability). Dev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure. ",
      "chunk_index": 6
    },
    {
      "chunk_id": "f59439dabcf8d2195d20e23db7e8da5915a62bf1",
      "url": "https://kubernetes.io/docs/concepts/overview/",
      "title": "Overview | Kubernetes",
      "text": "ient rollbacks (due to image immutability). Dev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure. Observability: not only surfaces OS-level information and metrics, but also application health and other signals. Environmental consistency across development, testing, and production: runs the same on a laptop as it does in the cloud. Cloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on major public clouds, and anywhere else. Application-centric management: raises the level of abstraction from running an OS on virtual hardware to running an application on an OS using logical resources. Loosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can be deployed and managed dynamically – not a monolithic stack running on one big single-purpose machine. Resource isolation: predictable application performance. Resource utilization: high efficiency and density. What's next",
      "chunk_index": 7
    },
    {
      "chunk_id": "1181f3ae1fa185f7d4c4d00965a72feab8d7ffe0",
      "url": "https://tekton.dev/docs/",
      "title": "Welcome to Tekton",
      "text": "Welcome to Tekton Tekton is a cloud-native solution for building CI/CD systems. It consists of Tekton Pipelines, which provides the building blocks, and of supporting components, such as Tekton CLI and Tekton Catalog, that make Tekton a complete ecosystem. Tekton is part of the CD Foundation , a Linux Foundation project. For more information, see the Overview of Tekton . Installation instructions for Tekton components Result storage for Tekton CI/CD data. Building Blocks of Tekton CI/CD Workflow Conceptual and technical information about Tekton Event Based Triggers for Tekton Pipelines Guides to help you complete a specific goal Web-based UI for Tekton Pipelines and Tekton Triggers resources Reusable Task and Pipeline Resources Manage Tekton CI/CD Building Blocks Artifact signatures and attestations for Tekton Feedback Was this page helpful? Yes No",
      "chunk_index": 0
    },
    {
      "chunk_id": "8de7c9623f768630bca2248c1b386faa34a9a7b4",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "docker Open Markdown Ask Docs AI Claude Open in Claude Description The base command for the Docker CLI. Depending on your Docker system configuration, you may be required to preface each docker command with sudo . To avoid having to use sudo with the docker command, your system administrator can create a Unix group called docker and add users to it. For more information about installing Docker or sudo configuration, refer to the installation instructions for your operating system. To list the help on any command just execute the command, followed by the --help option. The following list of environment variables are supported by the docker command line: Variable Description DOCKER_API_VERSION Override the negotiated API version to use for debugging (e.g. 1.19 ) DOCKER_CERT_PATH Location of your authentication keys. This variable is used both by the docker CLI and the dockerd daemon DOCKER_CONFIG The location of your client configuration files. DOCKER_CONTEXT Name of the docker context to use (overrides DOCKER_HOST env var and default context set with docker context use ) DOCKER_CUSTOM_HEADERS (Experimental) Configure custom HTTP headers to be sent by the client. Headers must be provided as a comma-separated list of name=value pairs. This is the equivalent to the HttpHeaders field in the configuration file. DOCKER_DEFAULT_PLATFORM Default platform for commands that take the --pla",
      "chunk_index": 0
    },
    {
      "chunk_id": "c8ae7d054802bfb374e5466773772ed4d7a87801",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "Headers must be provided as a comma-separated list of name=value pairs. This is the equivalent to the HttpHeaders field in the configuration file. DOCKER_DEFAULT_PLATFORM Default platform for commands that take the --platform flag. DOCKER_HIDE_LEGACY_COMMANDS When set, Docker hides \"legacy\" top-level commands (such as docker rm , and docker pull ) in docker help output, and only Management commands per object-type (e.g., docker container ) are printed. This may become the default in a future release. DOCKER_HOST Daemon socket to connect to. DOCKER_TLS Enable TLS for connections made by the docker CLI (equivalent of the --tls command-line option). Set to a non-empty value to enable TLS. Note that TLS is enabled automatically if any of the other TLS options are set. DOCKER_TLS_VERIFY When set Docker uses TLS and verifies the remote. This variable is used both by the docker CLI and the dockerd daemon BUILDKIT_PROGRESS Set type of progress output ( auto , plain , tty , rawjson ) when building with BuildKit backend . Use plain to show container output (default auto ). NO_COLOR Disable any ANSI escape codes in the output in accordance with https://no-color.org/ Because Docker is developed using Go, you can also use any environment variables used by the Go runtime. In particular, you may find these useful: Variable Description HTTP_PROXY Proxy URL for HTTP requests unless overridden b",
      "chunk_index": 1
    },
    {
      "chunk_id": "ea459eb506cd05cbdf0937d5bb5777df6a0ff34e",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "e Docker is developed using Go, you can also use any environment variables used by the Go runtime. In particular, you may find these useful: Variable Description HTTP_PROXY Proxy URL for HTTP requests unless overridden by NoProxy. HTTPS_PROXY Proxy URL for HTTPS requests unless overridden by NoProxy. NO_PROXY Comma-separated values specifying hosts that should be excluded from proxying. See the Go specification for details on these variables. Single character command line options can be combined, so rather than typing docker run -i -t --name test busybox sh , you can write docker run -it --name test busybox sh . Boolean options take the form -d=false . The value you see in the help text is the default value which is set if you do not specify that flag. If you specify a Boolean flag without a value, this will set the flag to true , irrespective of the default value. For example, running docker run -d will set the value to true , so your container will run in \"detached\" mode, in the background. Options which default to true (e.g., docker build --rm=true ) can only be set to the non-default value by explicitly setting them to false : You can specify options like -a=[] multiple times in a single command line, for example in these commands: Sometimes, multiple options can call for a more complex value string as for -v : Do not use the -t and -a stderr options together due to limitat",
      "chunk_index": 2
    },
    {
      "chunk_id": "ea5cbc84e038499675d00f9ed9177ef84194a404",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "=[] multiple times in a single command line, for example in these commands: Sometimes, multiple options can call for a more complex value string as for -v : Do not use the -t and -a stderr options together due to limitations in the pty implementation. All stderr in pty mode simply goes to stdout . Options like --name=\"\" expect a string, and they can only be specified once. Options like -c=0 expect an integer, and they can only be specified once. By default, the Docker command line stores its configuration files in a directory called .docker within your $HOME directory. Docker manages most of the files in the configuration directory and you shouldn't modify them. However, you can modify the config.json file to control certain aspects of how the docker command behaves. You can modify the docker command behavior using environment variables or command-line options. You can also use options within config.json to modify some of the same behavior. If an environment variable and the --config flag are set, the flag takes precedent over the environment variable. Command line options override environment variables and environment variables override properties you specify in a config.json file. To specify a different directory, use the DOCKER_CONFIG environment variable or the --config command line option. If both are specified, then the --config option overrides the DOCKER_CONFIG environm",
      "chunk_index": 3
    },
    {
      "chunk_id": "fa0ff33685556d48f184ade52d26176426b3edfd",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "n a config.json file. To specify a different directory, use the DOCKER_CONFIG environment variable or the --config command line option. If both are specified, then the --config option overrides the DOCKER_CONFIG environment variable. The example below overrides the docker ps command using a config.json file located in the ~/testconfigs/ directory. This flag only applies to whatever command is being ran. For persistent configuration, you can set the DOCKER_CONFIG environment variable in your shell (e.g. ~/.profile or ~/.bashrc ). The example below sets the new directory to be HOME/newdir/.docker . Use the Docker CLI configuration to customize settings for the docker CLI. The configuration file uses JSON formatting, and properties: By default, configuration file is stored in ~/.docker/config.json . Refer to the change the .docker directory section to use a different location. The configuration file and other files inside the ~/.docker configuration directory may contain sensitive information, such as authentication information for proxies or, depending on your credential store, credentials for your image registries. Review your configuration file's content before sharing with others, and prevent committing the file to version control. These fields lets you customize the default output format for some commands if no --format flag is provided. Property Description configFormat Cust",
      "chunk_index": 4
    },
    {
      "chunk_id": "a0201c3f9ea3680473d86d5cd0fc139c62573015",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "sharing with others, and prevent committing the file to version control. These fields lets you customize the default output format for some commands if no --format flag is provided. Property Description configFormat Custom default format for docker config ls output. See docker config ls for a list of supported formatting directives. imagesFormat Custom default format for docker images / docker image ls output. See docker images for a list of supported formatting directives. networksFormat Custom default format for docker network ls output. See docker network ls for a list of supported formatting directives. nodesFormat Custom default format for docker node ls output. See docker node ls for a list of supported formatting directives. pluginsFormat Custom default format for docker plugin ls output. See docker plugin ls for a list of supported formatting directives. psFormat Custom default format for docker ps / docker container ps output. See docker ps for a list of supported formatting directives. secretFormat Custom default format for docker secret ls output. See docker secret ls for a list of supported formatting directives. serviceInspectFormat Custom default format for docker service inspect output. See docker service inspect for a list of supported formatting directives. servicesFormat Custom default format for docker service ls output. See docker service ls for a list of su",
      "chunk_index": 5
    },
    {
      "chunk_id": "5b49fab232391146fccb2391fde4315e92c28721",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "ormat for docker service inspect output. See docker service inspect for a list of supported formatting directives. servicesFormat Custom default format for docker service ls output. See docker service ls for a list of supported formatting directives. statsFormat Custom default format for docker stats output. See docker stats for a list of supported formatting directives. tasksFormat Custom default format for docker stack ps output. See docker stack ps for a list of supported formatting directives. volumesFormat Custom default format for docker volume ls output. See docker volume ls for a list of supported formatting directives. The property HttpHeaders specifies a set of headers to include in all messages sent from the Docker client to the daemon. Docker doesn't try to interpret or understand these headers; it simply puts them into the messages. Docker does not allow these headers to change any headers it sets for itself. Alternatively, use the DOCKER_CUSTOM_HEADERS environment variable , which is available in v27.1 and higher. This environment-variable is experimental, and its exact behavior may change. The property credsStore specifies an external binary to serve as the default credential store. When this property is set, docker login will attempt to store credentials in the binary specified by docker-credential-<value> which is visible on $PATH . If this property isn't set, ",
      "chunk_index": 6
    },
    {
      "chunk_id": "3d5b7cf4f2abbee95357758285dc63ede82e47d8",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "rve as the default credential store. When this property is set, docker login will attempt to store credentials in the binary specified by docker-credential-<value> which is visible on $PATH . If this property isn't set, credentials are stored in the auths property of the CLI configuration file. For more information, see the Credential stores section in the docker login documentation The property credHelpers specifies a set of credential helpers to use preferentially over credsStore or auths when storing and retrieving credentials for specific registries. If this property is set, the binary docker-credential-<value> will be used when storing or retrieving credentials for a specific registry. For more information, see the Credential helpers section in the docker login documentation The property proxies specifies proxy environment variables to be automatically set on containers, and set as --build-arg on containers used during docker build . A \"default\" set of proxies can be configured, and will be used for any Docker daemon that the client connects to, or a configuration per host (Docker daemon), for example, https://docker-daemon1.example.com . The following properties can be set for each environment: Property Description httpProxy Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build httpsProxy Default value of HTTPS_PROXY and https_proxy",
      "chunk_index": 7
    },
    {
      "chunk_id": "98e5e4577f71ead635fc031a0eb80807f89838cf",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "erties can be set for each environment: Property Description httpProxy Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build httpsProxy Default value of HTTPS_PROXY and https_proxy for containers, and as --build-arg on docker build ftpProxy Default value of FTP_PROXY and ftp_proxy for containers, and as --build-arg on docker build noProxy Default value of NO_PROXY and no_proxy for containers, and as --build-arg on docker build allProxy Default value of ALL_PROXY and all_proxy for containers, and as --build-arg on docker build These settings are used to configure proxy settings for containers only, and not used as proxy settings for the docker CLI or the dockerd daemon. Refer to the environment variables section and the Daemon proxy configuration guide for configuring proxy settings for the CLI and daemon. Proxy settings may contain sensitive information (for example, if the proxy requires authentication). Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using docker commit . Once attached to a container, users detach from it and leave it running using the using CTRL-p CTRL-q key sequence. This detach key sequence is customizable using the detachKeys property. Specify a <sequence> value for the property. The format of the <sequ",
      "chunk_index": 8
    },
    {
      "chunk_id": "e6da0e6fd0852572e33097702174cd289facd997",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "detach from it and leave it running using the using CTRL-p CTRL-q key sequence. This detach key sequence is customizable using the detachKeys property. Specify a <sequence> value for the property. The format of the <sequence> is a comma-separated list of either a letter [a-Z], or the ctrl- combined with any of the following: a-z (a single lowercase alpha character ) @ (at sign) [ (left bracket) \\\\ (two backward slashes) _ (underscore) ^ (caret) Your customization applies to all containers started in with your Docker client. Users can override your custom or the default key sequence on a per-container basis. To do this, the user specifies the --detach-keys flag with the docker attach , docker exec , docker run or docker start command. The property plugins contains settings specific to CLI plugins. The key is the plugin name, while the value is a further map of options, which are specific to that plugin. Following is a sample config.json file to illustrate the format used for various fields: Experimental features provide early access to future product functionality. These features are intended for testing and feedback, and they may change between releases without warning or can be removed from a future release. Starting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them. If using your own notary server and a self-signe",
      "chunk_index": 9
    },
    {
      "chunk_id": "8f387f2376a7c0abab178999e5bbd8b322e7efc4",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "warning or can be removed from a future release. Starting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them. If using your own notary server and a self-signed certificate or an internal Certificate Authority, you need to place the certificate at tls/<registry_url>/ca.crt in your Docker config directory. Alternatively you can trust the certificate globally by adding it to your system's list of root Certificate Authorities. Option Default Description --config /root/.docker Location of client config files -c, --context Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with docker context use ) -D, --debug Enable debug mode -H, --host Daemon socket to connect to -l, --log-level info Set the logging level ( debug , info , warn , error , fatal ) --tls Use TLS; implied by --tlsverify --tlscacert /root/.docker/ca.pem Trust certs signed only by this CA --tlscert /root/.docker/cert.pem Path to TLS certificate file --tlskey /root/.docker/key.pem Path to TLS key file --tlsverify Use TLS and verify the remote You can use the -H , --host flag to specify a socket to use when you invoke a docker command. You can use the following protocols: Scheme Description Example unix://[<path>] Unix socket (Linux only) unix:///var/run/docker.sock tcp://[<IP or host>[:port]] TCP connectio",
      "chunk_index": 10
    },
    {
      "chunk_id": "a521100a3d7129a0c35c31fc4efc3ec541d03b18",
      "url": "https://docs.docker.com/engine/reference/commandline/cli/",
      "title": "docker | Docker Docs",
      "text": "cket to use when you invoke a docker command. You can use the following protocols: Scheme Description Example unix://[<path>] Unix socket (Linux only) unix:///var/run/docker.sock tcp://[<IP or host>[:port]] TCP connection tcp://174.17.0.1:2376 ssh://[username@]<IP or host>[:port] SSH connection ssh://user@192.168.64.5 npipe://[<name>] Named pipe (Windows only) npipe:////./pipe/docker_engine If you don't specify the -H flag, and you're not using a custom context , commands use the following default sockets: unix:///var/run/docker.sock on macOS and Linux npipe:////./pipe/docker_engine on Windows To achieve a similar effect without having to specify the -H flag for every command, you could also create a context , or alternatively, use the DOCKER_HOST environment variable . For more information about the -H flag, see Daemon socket option . The following example shows how to invoke docker ps over TCP, to a remote daemon with IP address 174.17.0.1 , listening on port 2376 : By convention, the Docker daemon uses port 2376 for secure TLS connections, and port 2375 for insecure, non-TLS connections. When you use SSH invoke a command on a remote daemon, the request gets forwarded to the /var/run/docker.sock Unix socket on the SSH host. You can optionally specify the location of the socket by appending a path component to the end of the SSH address.",
      "chunk_index": 11
    },
    {
      "chunk_id": "19a8fafdbb0eea2908a4a6eb25e84f99f13e1038",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "Service Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends. In Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster. A key aim of Services in Kubernetes is that you don't need to modify your existing application to use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you've containerized. You use a Service to make that set of Pods available on the network so that clients can interact with it. If you use a Deployment to run your app, that Deployment can create and destroy Pods dynamically. From one moment to the next, you don't know how many of those Pods are working and healthy; you might not even know what those healthy Pods are named. Kubernetes Pods are created and destroyed to match the desired state of your cluster. Pods are ephemeral resources (you should not expect that an individual Pod is reliable and durable). Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a given Deployment in your cluster, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. This leads to a problem: if some set of Pods (call them \"bac",
      "chunk_index": 0
    },
    {
      "chunk_id": "e298cec81d8f0b862a322e92a784061e42ace0f3",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "iven Deployment in your cluster, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later. This leads to a problem: if some set of Pods (call them \"backends\") provides functionality to other Pods (call them \"frontends\") inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload? Enter Services . Services in Kubernetes The Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a network. Each Service object defines a logical set of endpoints (usually these endpoints are Pods) along with a policy about how to make those pods accessible. For example, consider a stateless image-processing backend which is running with 3 replicas. Those replicas are fungible—frontends do not care which backend they use. While the actual Pods that compose the backend set may change, the frontend clients should not need to be aware of that, nor should they need to keep track of the set of backends themselves. The Service abstraction enables this decoupling. The set of Pods targeted by a Service is usually determined by a selector that you define. To learn about other ways to define Service endpoints, see Services without selectors . If your workload speaks HTTP, you might choose to use an Ingress ",
      "chunk_index": 1
    },
    {
      "chunk_id": "9c76b1f36341689425f02a30b3820c3e56a3d757",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": " by a Service is usually determined by a selector that you define. To learn about other ways to define Service endpoints, see Services without selectors . If your workload speaks HTTP, you might choose to use an Ingress to control how web traffic reaches that workload. Ingress is not a Service type, but it acts as the entry point for your cluster. An Ingress lets you consolidate your routing rules into a single resource, so that you can expose multiple components of your workload, running separately in your cluster, behind a single listener. The Gateway API for Kubernetes provides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster - it is a family of extension APIs, implemented using CustomResourceDefinitions - and then use these to configure access to network services that are running in your cluster. Cloud-native service discovery If you're able to use Kubernetes APIs for service discovery in your application, you can query the API server for matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service whenever the set of Pods in a Service changes. For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods. Either way, your workload can use these service discovery mechanisms to find the target it wants to connect to. Defining a Service A Service i",
      "chunk_index": 2
    },
    {
      "chunk_id": "2bd766e1f5eaaa05c3472474ffd04863bd2b0f3c",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "twork port or load balancer in between your application and the backend Pods. Either way, your workload can use these service discovery mechanisms to find the target it wants to connect to. Defining a Service A Service is an object (the same way that a Pod or a ConfigMap is an object). You can create, view or modify Service definitions using the Kubernetes API. Usually you use a tool such as kubectl to make those API calls for you. For example, suppose you have a set of Pods that each listen on TCP port 9376 and are labelled as app.kubernetes.io/name=MyApp . You can define a Service to publish that TCP listener: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 Applying this manifest creates a new Service named \"my-service\" with the default ClusterIP service type . The Service targets TCP port 9376 on any Pod with the app.kubernetes.io/name: MyApp label. Kubernetes assigns this Service an IP address (the cluster IP ), that is used by the virtual IP address mechanism. For more details on that mechanism, read Virtual IPs and Service Proxies . The controller for that Service continuously scans for Pods that match its selector, and then makes any necessary updates to the set of EndpointSlices for the Service. The name of a Service object must be a valid RFC 1035 label nam",
      "chunk_index": 3
    },
    {
      "chunk_id": "1acbc4a82d6cf48cc8a0626d8a0b80b4d927f90b",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "ler for that Service continuously scans for Pods that match its selector, and then makes any necessary updates to the set of EndpointSlices for the Service. The name of a Service object must be a valid RFC 1035 label name . Note: A Service can map any incoming port to a targetPort . By default and for convenience, the targetPort is set to the same value as the port field. Relaxed naming requirements for Service objects FEATURE STATE: Kubernetes v1.34 [alpha] (disabled by default) The RelaxedServiceNameValidation feature gate allows Service object names to start with a digit. When this feature gate is enabled, Service object names must be valid RFC 1123 label names . Port definitions Port definitions in Pods have names, and you can reference these names in the targetPort attribute of a Service. For example, we can bind the targetPort of the Service to the Pod port in the following way: apiVersion : v1 kind : Service metadata : name : nginx-service spec : selector : app.kubernetes.io/name : proxy ports : - name : name-of-service-port protocol : TCP port : 80 targetPort : http-web-svc --- apiVersion : v1 kind : Pod metadata : name : nginx labels : app.kubernetes.io/name : proxy spec : containers : - name : nginx image : nginx:stable ports : - containerPort : 80 name : http-web-svc This works even if there is a mixture of Pods in the Service using a single configured name, with the",
      "chunk_index": 4
    },
    {
      "chunk_id": "3fdf40ea38d49f1c6fa9a59c2d13f9ebcf96c798",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "/name : proxy spec : containers : - name : nginx image : nginx:stable ports : - containerPort : 80 name : http-web-svc This works even if there is a mixture of Pods in the Service using a single configured name, with the same network protocol available via different port numbers. This offers a lot of flexibility for deploying and evolving your Services. For example, you can change the port numbers that Pods expose in the next version of your backend software, without breaking clients. The default protocol for Services is TCP ; you can also use any other supported protocol . Because many Services need to expose more than one port, Kubernetes supports multiple port definitions for a single Service. Each port definition can have the same protocol , or a different one. Services without selectors Services most commonly abstract access to Kubernetes Pods thanks to the selector, but when used with a corresponding set of EndpointSlices objects and without a selector, the Service can abstract other kinds of backends, including ones that run outside the cluster. For example: You want to have an external database cluster in production, but in your test environment you use your own databases. You want to point your Service to a Service in a different Namespace or on another cluster. You are migrating a workload to Kubernetes. While evaluating the approach, you run only a portion of your ba",
      "chunk_index": 5
    },
    {
      "chunk_id": "cd1e3a446e23870d6c7137ec3f91442b59252509",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "your own databases. You want to point your Service to a Service in a different Namespace or on another cluster. You are migrating a workload to Kubernetes. While evaluating the approach, you run only a portion of your backends in Kubernetes. In any of these scenarios you can define a Service without specifying a selector to match Pods. For example: apiVersion : v1 kind : Service metadata : name : my-service spec : ports : - name : http protocol : TCP port : 80 targetPort : 9376 Because this Service has no selector, the corresponding EndpointSlice objects are not created automatically. You can map the Service to the network address and port where it's running, by adding an EndpointSlice object manually. For example: apiVersion : discovery.k8s.io/v1 kind : EndpointSlice metadata : name : my-service-1 # by convention, use the name of the Service # as a prefix for the name of the EndpointSlice labels : # You should set the \"kubernetes.io/service-name\" label. # Set its value to match the name of the Service kubernetes.io/service-name : my-service addressType : IPv4 ports : - name : http # should match with the name of the service port defined above appProtocol : http protocol : TCP port : 9376 endpoints : - addresses : - \"10.4.5.6\" - addresses : - \"10.1.2.3\" Custom EndpointSlices When you create an EndpointSlice object for a Service, you can use any name for the EndpointSlice. Each ",
      "chunk_index": 6
    },
    {
      "chunk_id": "2eb201e3b0ab1bd772a9e2f594c1f341418e7ca4",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": " protocol : TCP port : 9376 endpoints : - addresses : - \"10.4.5.6\" - addresses : - \"10.1.2.3\" Custom EndpointSlices When you create an EndpointSlice object for a Service, you can use any name for the EndpointSlice. Each EndpointSlice in a namespace must have a unique name. You link an EndpointSlice to a Service by setting the kubernetes.io/service-name label on that EndpointSlice. Note: The endpoint IPs must not be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6). The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because kube-proxy doesn't support virtual IPs as a destination. For an EndpointSlice that you create yourself, or in your own code, you should also pick a value to use for the label endpointslice.kubernetes.io/managed-by . If you create your own controller code to manage EndpointSlices, consider using a value similar to \"my-domain.example/name-of-controller\" . If you are using a third party tool, use the name of the tool in all-lowercase and change spaces and other punctuation to dashes ( - ). If people are directly using a tool such as kubectl to manage EndpointSlices, use a name that describes this manual management, such as \"staff\" or \"cluster-admins\" . You should avoid using the reserved value \"controller\" , which identifies EndpointSlices managed by Kub",
      "chunk_index": 7
    },
    {
      "chunk_id": "55bd547cc569dcd0d490b3eba9827ee570588815",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "tl to manage EndpointSlices, use a name that describes this manual management, such as \"staff\" or \"cluster-admins\" . You should avoid using the reserved value \"controller\" , which identifies EndpointSlices managed by Kubernetes' own control plane. Accessing a Service without a selector Accessing a Service without a selector works the same as if it had a selector. In the example for a Service without a selector, traffic is routed to one of the two endpoints defined in the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376. Note: The Kubernetes API server does not allow proxying to endpoints that are not mapped to pods. Actions such as kubectl port-forward service/<service-name> forwardedPort:servicePort where the service has no selector will fail due to this constraint. This prevents the Kubernetes API server from being used as a proxy to endpoints the caller may not be authorized to access. An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead. For more information, see the ExternalName section. EndpointSlices FEATURE STATE: Kubernetes v1.21 [stable] EndpointSlices are objects that represent a subset (a slice ) of the backing network endpoints for a Service. Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are so many endpoints for a Service that a thresho",
      "chunk_index": 8
    },
    {
      "chunk_id": "818c8968380b916ccee2258124d4f2557a447074",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "t represent a subset (a slice ) of the backing network endpoints for a Service. Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are so many endpoints for a Service that a threshold is reached, then Kubernetes adds another empty EndpointSlice and stores new endpoint information there. By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices all contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice until an extra endpoint needs to be added. See EndpointSlices for more information about this API. Endpoints (deprecated) FEATURE STATE: Kubernetes v1.33 [deprecated] The EndpointSlice API is the evolution of the older Endpoints API. The deprecated Endpoints API has several problems relative to EndpointSlice: It does not support dual-stack clusters. It does not contain information needed to support newer features, such as trafficDistribution . It will truncate the list of endpoints if it is too long to fit in a single object. Because of this, it is recommended that all clients use the EndpointSlice API rather than Endpoints. Over-capacity endpoints Kubernetes limits the number of endpoints that can fit in a single Endpoints object. When there are over 1000 backing endpoints for a Service, Kubernetes truncates the data in the Endpoints object. Because a Service can be linked with more than one En",
      "chunk_index": 9
    },
    {
      "chunk_id": "f8b5e7ca77eaa1939ac7597e433e729936dd9fed",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": " endpoints that can fit in a single Endpoints object. When there are over 1000 backing endpoints for a Service, Kubernetes truncates the data in the Endpoints object. Because a Service can be linked with more than one EndpointSlice, the 1000 backing endpoint limit only affects the legacy Endpoints API. In that case, Kubernetes selects at most 1000 possible backend endpoints to store into the Endpoints object, and sets an annotation on the Endpoints: endpoints.kubernetes.io/over-capacity: truncated . The control plane also removes that annotation if the number of backend Pods drops below 1000. Traffic is still sent to backends, but any load balancing mechanism that relies on the legacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints. The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints. Application protocol FEATURE STATE: Kubernetes v1.20 [stable] The appProtocol field provides a way to specify an application protocol for each Service port. This is used as a hint for implementations to offer richer behavior for protocols that they understand. The value of this field is mirrored by the corresponding Endpoints and EndpointSlice objects. This field follows standard Kubernetes label syntax. Valid values are one of: IANA standard service names . Implementation-defined prefixed names such as mycompan",
      "chunk_index": 10
    },
    {
      "chunk_id": "b3f1b2968194fb50a9ac472fa3fa037b7b5c2b5b",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "by the corresponding Endpoints and EndpointSlice objects. This field follows standard Kubernetes label syntax. Valid values are one of: IANA standard service names . Implementation-defined prefixed names such as mycompany.com/my-custom-protocol . Kubernetes-defined prefixed names: Protocol Description kubernetes.io/h2c HTTP/2 over cleartext as described in RFC 7540 kubernetes.io/ws WebSocket over cleartext as described in RFC 6455 kubernetes.io/wss WebSocket over TLS as described in RFC 6455 Multi-port Services For some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object. When using multiple ports for a Service, you must give all of your ports names so that these are unambiguous. For example: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - name : http protocol : TCP port : 80 targetPort : 9376 - name : https protocol : TCP port : 443 targetPort : 9377 Note: As with Kubernetes names in general, names for ports must only contain lowercase alphanumeric characters and - . Port names must also start and end with an alphanumeric character. For example, the names 123-abc and web are valid, but 123_abc and -web are not. Service type For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP ",
      "chunk_index": 11
    },
    {
      "chunk_id": "89f504bff84dd126dc221b94477a8e102af5a0c8",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "ic character. For example, the names 123-abc and web are valid, but 123_abc and -web are not. Service type For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, one that's accessible from outside of your cluster. Kubernetes Service types allow you to specify what kind of Service you want. The available type values and their behaviors are: ClusterIP Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify a type for a Service. You can expose the Service to the public internet using an Ingress or a Gateway . NodePort Exposes the Service on each Node's IP at a static port (the NodePort ). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of type: ClusterIP . LoadBalancer Exposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider. ExternalName Maps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example ). The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No pr",
      "chunk_index": 12
    },
    {
      "chunk_id": "6421a3064220dbeaf607bbe36de79a98158a699a",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "he Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example ). The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up. The type field in the Service API is designed as nested functionality - each level adds to the previous. However there is an exception to this nested design. You can define a LoadBalancer Service by disabling the load balancer NodePort allocation . type: ClusterIP This default Service type assigns an IP address from a pool of IP addresses that your cluster has reserved for that purpose. Several of the other types for Service build on the ClusterIP type as a foundation. If you define a Service that has the .spec.clusterIP set to \"None\" then Kubernetes does not assign an IP address. See headless Services for more information. Choosing your own IP address You can specify your own cluster IP address as part of a Service creation request. To do this, set the .spec.clusterIP field. For example, if you already have an existing DNS entry that you wish to reuse, or legacy systems that are configured for a specific IP address and difficult to re-configure. The IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is configured for the API server. If you try to create a Service",
      "chunk_index": 13
    },
    {
      "chunk_id": "530f229b3cab0e9050a94948797eae1e8a767b26",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "and difficult to re-configure. The IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is configured for the API server. If you try to create a Service with an invalid clusterIP address value, the API server will return a 422 HTTP status code to indicate that there's a problem. Read avoiding collisions to learn how Kubernetes helps reduce the risk and impact of two different Services both trying to use the same IP address. type: NodePort If you set the type field to NodePort , the Kubernetes control plane allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. Your Service reports the allocated port in its .spec.ports[*].nodePort field. Using a NodePort gives you the freedom to set up your own load balancing solution, to configure environments that are not fully supported by Kubernetes, or even to expose one or more nodes' IP addresses directly. For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or SCTP to match the protocol of the Service). Every node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outsid",
      "chunk_index": 14
    },
    {
      "chunk_id": "44a3d7f7a7fb298ea2dc14d106ba085e3a001936",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "y node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service). Choosing your own port If you want a specific port number, you can specify a value in the nodePort field. The control plane will either allocate you that port or report that the API transaction failed. This means that you need to take care of possible port collisions yourself. You also have to use a valid port number, one that's inside the range configured for NodePort use. Here is an example manifest for a Service of type: NodePort that specifies a NodePort value (30007, in this example): apiVersion : v1 kind : Service metadata : name : my-service spec : type : NodePort selector : app.kubernetes.io/name : MyApp ports : - port : 80 # By default and for convenience, the `targetPort` is set to # the same value as the `port` field. targetPort : 80 # Optional field # By default and for convenience, the Kubernetes control plane # will allocate a port from a range (default: 30000-32767) nodePort : 30007 Reserve Nodeport ranges to avoid collisions The policy for assigning ports to NodePort services applies to both the auto-",
      "chunk_index": 15
    },
    {
      "chunk_id": "8a3f6c921a5bd9f52c77f8836f677e85d39a7cb8",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "bernetes control plane # will allocate a port from a range (default: 30000-32767) nodePort : 30007 Reserve Nodeport ranges to avoid collisions The policy for assigning ports to NodePort services applies to both the auto-assignment and the manual assignment scenarios. When a user wants to create a NodePort service that uses a specific port, the target port may conflict with another port that has already been assigned. To avoid this problem, the port range for NodePort services is divided into two bands. Dynamic port assignment uses the upper band by default, and it may use the lower band once the upper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision. Custom IP address configuration for type: NodePort Services You can set up nodes in your cluster to use a particular IP address for serving node port services. You might want to do this if each node is connected to multiple networks (for example: one network for application traffic, and another network for traffic between nodes and the control plane). If you want to specify particular IP address(es) to proxy the port, you can set the --nodeport-addresses flag for kube-proxy or the equivalent nodePortAddresses field of the kube-proxy configuration file to particular IP block(s). This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP ",
      "chunk_index": 16
    },
    {
      "chunk_id": "a5634d4f31932eb4549236907a9aa4cf9ae6d880",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "or kube-proxy or the equivalent nodePortAddresses field of the kube-proxy configuration file to particular IP block(s). This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP address ranges that kube-proxy should consider as local to this node. For example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8 flag, kube-proxy only selects the loopback interface for NodePort Services. The default for --nodeport-addresses is an empty list. This means that kube-proxy should consider all available network interfaces for NodePort. (That's also compatible with earlier Kubernetes releases.) Note: This Service is visible as <NodeIP>:spec.ports[*].nodePort and .spec.clusterIP:spec.ports[*].port . If the --nodeport-addresses flag for kube-proxy or the equivalent field in the kube-proxy configuration file is set, <NodeIP> would be a filtered node IP address (or possibly IP addresses). type: LoadBalancer On cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer is published in the Service's .status.loadBalancer field. For example: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp po",
      "chunk_index": 17
    },
    {
      "chunk_id": "81437aaced18c355125f0908cc7c834b83e7e25b",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "mation about the provisioned balancer is published in the Service's .status.loadBalancer field. For example: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 clusterIP : 10.0.171.239 type : LoadBalancer status : loadBalancer : ingress : - ip : 192.0.2.127 Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced. To implement a Service of type: LoadBalancer , Kubernetes typically starts off by making the changes that are equivalent to you requesting a Service of type: NodePort . The cloud-controller-manager component then configures the external load balancer to forward traffic to that assigned node port. You can configure a load balanced Service to omit assigning a node port, provided that the cloud provider implementation supports this. Some cloud providers allow you to specify the loadBalancerIP . In those cases, the load-balancer is created with the user-specified loadBalancerIP . If the loadBalancerIP field is not specified, the load balancer is set up with an ephemeral IP address. If you specify a loadBalancerIP but your cloud provider does not support the feature, the loadbalancerIP field that you set is ignored. Note: The .spec.loadBalancerIP field for a Service was deprecated in Kubernetes v1.",
      "chunk_index": 18
    },
    {
      "chunk_id": "a4334ce06da34cfac5953284a2da2c45daf4478d",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": " If you specify a loadBalancerIP but your cloud provider does not support the feature, the loadbalancerIP field that you set is ignored. Note: The .spec.loadBalancerIP field for a Service was deprecated in Kubernetes v1.24. This field was under-specified and its meaning varies across implementations. It also cannot support dual-stack networking. This field may be removed in a future API version. If you're integrating with a provider that supports specifying the load balancer IP address(es) for a Service via a (provider specific) annotation, you should switch to doing that. If you are writing code for a load balancer integration with Kubernetes, avoid using this field. You can integrate with Gateway rather than Service, or you can define your own (provider specific) annotations on the Service that specify the equivalent detail. Node liveness impact on load balancer traffic Load balancer health checks are critical to modern applications. They are used to determine which server (virtual machine, or IP address) the load balancer should dispatch traffic to. The Kubernetes APIs do not define how health checks have to be implemented for Kubernetes managed load balancers, instead it's the cloud providers (and the people implementing integration code) who decide on the behavior. Load balancer health checks are extensively used within the context of supporting the externalTrafficPolicy f",
      "chunk_index": 19
    },
    {
      "chunk_id": "237f811a1929ea95ca62614fddbd65f5434dd1cf",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "s, instead it's the cloud providers (and the people implementing integration code) who decide on the behavior. Load balancer health checks are extensively used within the context of supporting the externalTrafficPolicy field for Services. Load balancers with mixed protocol types FEATURE STATE: Kubernetes v1.26 [stable] (enabled by default) By default, for LoadBalancer type of Services, when there is more than one port defined, all ports must have the same protocol, and the protocol must be one which is supported by the cloud provider. The feature gate MixedProtocolLBService (enabled by default for the kube-apiserver as of v1.24) allows the use of different protocols for LoadBalancer type of Services, when there is more than one port defined. Note: The set of protocols that can be used for load balanced Services is defined by your cloud provider; they may impose restrictions beyond what the Kubernetes API enforces. Disabling load balancer NodePort allocation FEATURE STATE: Kubernetes v1.24 [stable] You can optionally disable node port allocation for a Service of type: LoadBalancer , by setting the field spec.allocateLoadBalancerNodePorts to false . This should only be used for load balancer implementations that route traffic directly to pods as opposed to using node ports. By default, spec.allocateLoadBalancerNodePorts is true and type LoadBalancer Services will continue to allo",
      "chunk_index": 20
    },
    {
      "chunk_id": "35a4bfe8b5eed6ea044c9a043ca2369d04c6db32",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "ly be used for load balancer implementations that route traffic directly to pods as opposed to using node ports. By default, spec.allocateLoadBalancerNodePorts is true and type LoadBalancer Services will continue to allocate node ports. If spec.allocateLoadBalancerNodePorts is set to false on an existing Service with allocated node ports, those node ports will not be de-allocated automatically. You must explicitly remove the nodePorts entry in every Service port to de-allocate those node ports. Specifying class of load balancer implementation FEATURE STATE: Kubernetes v1.24 [stable] For a Service with type set to LoadBalancer , the .spec.loadBalancerClass field enables you to use a load balancer implementation other than the cloud provider default. By default, .spec.loadBalancerClass is not set and a LoadBalancer type of Service uses the cloud provider's default load balancer implementation if the cluster is configured with a cloud provider using the --cloud-provider component flag. If you specify .spec.loadBalancerClass , it is assumed that a load balancer implementation that matches the specified class is watching for Services. Any default load balancer implementation (for example, the one provided by the cloud provider) will ignore Services that have this field set. spec.loadBalancerClass can be set on a Service of type LoadBalancer only. Once set, it cannot be changed. The ",
      "chunk_index": 21
    },
    {
      "chunk_id": "988da73e03c6ae9d3ff853400b5aa4c3f771e7dd",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "entation (for example, the one provided by the cloud provider) will ignore Services that have this field set. spec.loadBalancerClass can be set on a Service of type LoadBalancer only. Once set, it cannot be changed. The value of spec.loadBalancerClass must be a label-style identifier, with an optional prefix such as \" internal-vip \" or \" example.com/internal-vip \". Unprefixed names are reserved for end-users. Load balancer IP address mode For a Service of type: LoadBalancer , a controller can set .status.loadBalancer.ingress.ipMode . The .status.loadBalancer.ingress.ipMode specifies how the load-balancer IP behaves. It may be specified only when the .status.loadBalancer.ingress.ip field is also specified. There are two possible values for .status.loadBalancer.ingress.ipMode : \"VIP\" and \"Proxy\". The default value is \"VIP\" meaning that traffic is delivered to the node with the destination set to the load-balancer's IP and port. There are two cases when setting this to \"Proxy\", depending on how the load-balancer from the cloud provider delivers the traffics: If the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port; If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port. Service implementations may use this information to adjust traffic routing. Internal load balancer ",
      "chunk_index": 22
    },
    {
      "chunk_id": "2ee8b488b4e7b3a2b5c91325be2b543ea89a349e",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "P and node port; If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port. Service implementations may use this information to adjust traffic routing. Internal load balancer In a mixed environment it is sometimes necessary to route traffic from Services inside the same (virtual) network address block. In a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your endpoints. To set an internal load balancer, add one of the following annotations to your Service depending on the cloud service provider you're using: metadata : name : my-service annotations : networking.gke.io/load-balancer-type : \"Internal\" metadata : name : my-service annotations : service.beta.kubernetes.io/aws-load-balancer-scheme : \"internal\" metadata : name : my-service annotations : service.beta.kubernetes.io/azure-load-balancer-internal : \"true\" metadata : name : my-service annotations : service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type : \"private\" metadata : name : my-service annotations : service.beta.kubernetes.io/openstack-internal-load-balancer : \"true\" metadata : name : my-service annotations : service.beta.kubernetes.io/cce-load-balancer-internal-vpc : \"true\" metadata : annotations : service.kubernetes.io/qcloud-loadbalancer-internal-subnetid : subnet-xxxxx metadata : annotations :",
      "chunk_index": 23
    },
    {
      "chunk_id": "01901b20f3b4c6872042035d0f3bd2c9ccb7bd5a",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "e : my-service annotations : service.beta.kubernetes.io/cce-load-balancer-internal-vpc : \"true\" metadata : annotations : service.kubernetes.io/qcloud-loadbalancer-internal-subnetid : subnet-xxxxx metadata : annotations : service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type : \"intranet\" metadata : name : my-service annotations : service.beta.kubernetes.io/oci-load-balancer-internal : true type: ExternalName Services of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service or cassandra . You specify these Services with the spec.externalName parameter. This Service definition, for example, maps the my-service Service in the prod namespace to my.database.example.com : apiVersion : v1 kind : Service metadata : name : my-service namespace : prod spec : type : ExternalName externalName : my.database.example.com Note: A Service of type: ExternalName accepts an IPv4 address string, but treats that string as a DNS name comprised of digits, not as an IP address (the internet does not however allow such names in DNS). Services with external names that resemble IPv4 addresses are not resolved by DNS servers. If you want to map a Service directly to a specific IP address, consider using headless Services . When looking up the host my-service.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record with the value my.database.e",
      "chunk_index": 24
    },
    {
      "chunk_id": "3d54fe0a839f6ef6c9b75c0118febdd788b4a55e",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "map a Service directly to a specific IP address, consider using headless Services . When looking up the host my-service.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record with the value my.database.example.com . Accessing my-service works in the same way as other Services but with the crucial difference that redirection happens at the DNS level rather than via proxying or forwarding. Should you later decide to move your database into your cluster, you can start its Pods, add appropriate selectors or endpoints, and change the Service's type . Caution: You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName then the hostname used by clients inside your cluster is different from the name that the ExternalName references. For protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a Host: header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the hostname that the client connected to. Headless Services Sometimes you don't need load-balancing and a single Service IP. In this case, you can create what are termed headless Services , by explicitly specifying \"None\" for the cluster IP address ( .spec.clusterIP ). You can use a headless Service to interface with other service discovery mechanis",
      "chunk_index": 25
    },
    {
      "chunk_id": "9446623b4272c5fd0766640ac4d56328dcd5d4a8",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "s case, you can create what are termed headless Services , by explicitly specifying \"None\" for the cluster IP address ( .spec.clusterIP ). You can use a headless Service to interface with other service discovery mechanisms, without being tied to Kubernetes' implementation. For headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them. A headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't configure routes and packet forwarding using virtual IP addresses and proxies ; instead, headless Services report the endpoint IP addresses of the individual pods via internal DNS records, served through the cluster's DNS service . To define a headless Service, you make a Service with .spec.type set to ClusterIP (which is also the default for type ), and you additionally set .spec.clusterIP to None. The string value None is a special case and is not the same as leaving the .spec.clusterIP field unset. How DNS is automatically configured depends on whether the Service has selectors defined: With selectors For headless Services that define selectors, the endpoints controller creates EndpointSlices in the Kubernetes API, and modifies the DNS configuration to return A or AAAA records (IPv4 or IPv6 addresses) that point directly t",
      "chunk_index": 26
    },
    {
      "chunk_id": "37d068e538c9c5ca924b0b814c334cc0ab883f74",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "eadless Services that define selectors, the endpoints controller creates EndpointSlices in the Kubernetes API, and modifies the DNS configuration to return A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service. Without selectors For headless Services that do not define selectors, the control plane does not create EndpointSlice objects. However, the DNS system looks for and configures either: DNS CNAME records for type: ExternalName Services. DNS A / AAAA records for all IP addresses of the Service's ready endpoints, for all Service types other than ExternalName . For IPv4 endpoints, the DNS system creates A records. For IPv6 endpoints, the DNS system creates AAAA records. When you define a headless Service without a selector, the port must match the targetPort . Discovering services For clients running inside your cluster, Kubernetes supports two primary modes of finding a Service: environment variables and DNS. Environment variables When a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service. It adds {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted to underscores. For example, the Service redis-primary which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11, produces the following environment variab",
      "chunk_index": 27
    },
    {
      "chunk_id": "1b986067c3250b6b0ad27834de2235f798d2ded7",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "me is upper-cased and dashes are converted to underscores. For example, the Service redis-primary which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11, produces the following environment variables: REDIS_PRIMARY_SERVICE_HOST = 10.0.0.11 REDIS_PRIMARY_SERVICE_PORT = 6379 REDIS_PRIMARY_PORT = tcp://10.0.0.11:6379 REDIS_PRIMARY_PORT_6379_TCP = tcp://10.0.0.11:6379 REDIS_PRIMARY_PORT_6379_TCP_PROTO = tcp REDIS_PRIMARY_PORT_6379_TCP_PORT = 6379 REDIS_PRIMARY_PORT_6379_TCP_ADDR = 10.0.0.11 Note: When you have a Pod that needs to access a Service, and you are using the environment variable method to publish the port and cluster IP to the client Pods, you must create the Service before the client Pods come into existence. Otherwise, those client Pods won't have their environment variables populated. If you only use DNS to discover the cluster IP for a Service, you don't need to worry about this ordering issue. Kubernetes also supports and provides variables that are compatible with Docker Engine's \" legacy container links \" feature. You can read makeLinkVariables to see how this is implemented in Kubernetes. DNS You can (and almost always should) set up a DNS service for your Kubernetes cluster using an add-on . A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services and creates a set of DNS records for each one. If DNS has be",
      "chunk_index": 28
    },
    {
      "chunk_id": "632c265a2d793a9ed0b8d012609753ed8f79faee",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "ld) set up a DNS service for your Kubernetes cluster using an add-on . A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services and creates a set of DNS records for each one. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name. For example, if you have a Service called my-service in a Kubernetes namespace my-ns , the control plane and the DNS Service acting together create a DNS record for my-service.my-ns . Pods in the my-ns namespace should be able to find the service by doing a name lookup for my-service ( my-service.my-ns would also work). Pods in other namespaces must qualify the name as my-service.my-ns . These names will resolve to the cluster IP assigned for the Service. Kubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns Service has a port named http with the protocol set to TCP , you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for http , as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services. You can find more information about ExternalName resolution in DNS for Services and Pods . Virtual IP addressing mechanism Read Virtual IPs and Service Proxies explains the mechanism Kubernetes provides to expose a Service with a virtual IP address. ",
      "chunk_index": 29
    },
    {
      "chunk_id": "6facf88e0fc004db96f3e1195c2916455351ae16",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "about ExternalName resolution in DNS for Services and Pods . Virtual IP addressing mechanism Read Virtual IPs and Service Proxies explains the mechanism Kubernetes provides to expose a Service with a virtual IP address. Traffic policies You can set the .spec.internalTrafficPolicy and .spec.externalTrafficPolicy fields to control how Kubernetes routes traffic to healthy (“ready”) backends. See Traffic Policies for more details. Traffic distribution control The .spec.trafficDistribution field provides another way to influence traffic routing within a Kubernetes Service. While traffic policies focus on strict semantic guarantees, traffic distribution allows you to express preferences (such as routing to topologically closer endpoints). This can help optimize for performance, cost, or reliability. In Kubernetes 1.35, the following values are supported: PreferSameZone Indicates a preference for routing traffic to endpoints that are in the same zone as the client. PreferSameNode Indicates a preference for routing traffic to endpoints that are on the same node as the client. PreferClose (deprecated) This is an older alias for PreferSameZone that is less clear about the semantics. If the field is not set, the implementation will apply its default routing strategy. See Traffic Distribution for more details Session stickiness If you want to make sure that connections from a particular cl",
      "chunk_index": 30
    },
    {
      "chunk_id": "8b796ea0305a46270e9bb01d2ebcb479a5c55419",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": " semantics. If the field is not set, the implementation will apply its default routing strategy. See Traffic Distribution for more details Session stickiness If you want to make sure that connections from a particular client are passed to the same Pod each time, you can configure session affinity based on the client's IP address. Read session affinity to learn more. External IPs If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs . When network traffic arrives into the cluster, with the external IP (as destination IP) and the port matching that Service, rules and routes that Kubernetes has configured ensure that the traffic is routed to one of the endpoints for that Service. When you define a Service, you can specify externalIPs for any service type . In the example below, the Service named \"my-service\" can be accessed by clients using TCP, on \"198.51.100.32:80\" (calculated from .spec.externalIPs[] and .spec.ports[].port ). apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - name : http protocol : TCP port : 80 targetPort : 49152 externalIPs : - 198.51.100.32 Note: Kubernetes does not manage allocation of externalIPs ; these are the responsibility of the cluster administrator. API Object Service is a top-level resource in the Kubernetes REST ",
      "chunk_index": 31
    },
    {
      "chunk_id": "4a86c218195dbf038bfe86e7a4bd0129186e6878",
      "url": "https://kubernetes.io/docs/concepts/services-networking/service/",
      "title": "Service | Kubernetes",
      "text": "152 externalIPs : - 198.51.100.32 Note: Kubernetes does not manage allocation of externalIPs ; these are the responsibility of the cluster administrator. API Object Service is a top-level resource in the Kubernetes REST API. You can find more details about the Service API object . What's next Learn more about Services and how they fit into Kubernetes: Follow the Connecting Applications with Services tutorial. Read about Ingress , which exposes HTTP and HTTPS routes from outside the cluster to Services within your cluster. Read about Gateway , an extension to Kubernetes that provides more flexibility than Ingress. For more context, read the following:",
      "chunk_index": 32
    },
    {
      "chunk_id": "95c7f1cbe6858c8e1c8b3098870a8c9e106ce451",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "Deployments A Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state. A Deployment provides declarative updates for Pods and ReplicaSets . You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Note: Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below. Use Case The following are typical use cases for Deployments: Creating a Deployment The following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods: apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 In this example: A Deployment named nginx-deployment is created, indicated by the .metadata.name field. This name will become the basis for the ReplicaSets and Pods which are created later. See Writing a Deployment Spec for more details. The Deployment creates a ReplicaSet that creates three replicated Pods, ",
      "chunk_index": 0
    },
    {
      "chunk_id": "096bd26b0f9e39387de4298186c9b4510d0887db",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "a.name field. This name will become the basis for the ReplicaSets and Pods which are created later. See Writing a Deployment Spec for more details. The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field. The .spec.selector field defines how the created ReplicaSet finds which Pods to manage. In this case, you select a label that is defined in the Pod template ( app: nginx ). However, more sophisticated selection rules are possible, as long as the Pod template itself satisfies the rule. Note: The .spec.selector.matchLabels field is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions , whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". All of the requirements, from both matchLabels and matchExpressions , must be satisfied in order to match. The .spec.template field contains the following sub-fields: The Pods are labeled app: nginx using the .metadata.labels field. The Pod template's specification, or .spec field, indicates that the Pods run one container, nginx , which runs the nginx Docker Hub image at version 1.14.2. Create one container and name it nginx using the .spec.containers[0].name field. Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Dep",
      "chunk_index": 1
    },
    {
      "chunk_id": "b505449533b27dd20251e43a586af0fc4cd416be",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "version 1.14.2. Create one container and name it nginx using the .spec.containers[0].name field. Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Deployment: Create the Deployment by running the following command: kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml Run kubectl get deployments to check if the Deployment was created. If the Deployment is still being created, the output is similar to the following: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 0 0 1s When you inspect the Deployments in your cluster, the following fields are displayed: NAME lists the names of the Deployments in the namespace. READY displays how many replicas of the application are available to your users. It follows the pattern ready/desired. UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state. AVAILABLE displays how many replicas of the application are available to your users. AGE displays the amount of time that the application has been running. Notice how the number of desired replicas is 3 according to .spec.replicas field. To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment . The output is similar to: Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \"nginx-deployment\" su",
      "chunk_index": 2
    },
    {
      "chunk_id": "df4e8eed2df99fe3f4cac70556302e3ac7d068f0",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "e Deployment rollout status, run kubectl rollout status deployment/nginx-deployment . The output is similar to: Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \"nginx-deployment\" successfully rolled out Run the kubectl get deployments again a few seconds later. The output is similar to this: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 18s Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available. To see the ReplicaSet ( rs ) created by the Deployment, run kubectl get rs . The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ReplicaSet output shows the following fields: NAME lists the names of the ReplicaSets in the namespace. DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state . CURRENT displays how many replicas are currently running. READY displays how many replicas of the application are available to your users. AGE displays the amount of time that the application has been running. Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[HASH] . This name will become the basis for the Pods which are created. The HASH string is the same as the pod-template-hash label on the Rep",
      "chunk_index": 3
    },
    {
      "chunk_id": "ecd3e6aee9e841d665840299014937c38e3be727",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "tice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[HASH] . This name will become the basis for the Pods which are created. The HASH string is the same as the pod-template-hash label on the ReplicaSet. To see the labels automatically generated for each Pod, run kubectl get pods --show-labels . The output is similar to: NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 The created ReplicaSet ensures that there are three nginx Pods. Note: You must specify an appropriate selector and Pod template labels in a Deployment (in this case, app: nginx ). Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly. Pod-template-hash label Caution: Do not change this label. The pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts. This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTempla",
      "chunk_index": 4
    },
    {
      "chunk_id": "817fa6ff9e65b36025774a061adc5f6eadc3e209",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts. This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have. Updating a Deployment Note: A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template ) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout. Follow the steps given below to update your Deployment: Let's update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image. kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.16.1 or use the following command: kubectl set image deployment/nginx-deployment nginx = nginx:1.16.1 where deployment/nginx-deployment indicates the Deployment, nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag. The output is similar to: deployment.apps/nginx-deployment image updated Alternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1 : ",
      "chunk_index": 5
    },
    {
      "chunk_id": "f1e3899de25576557deca3b6eb92c722a96bd9e2",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "age and its tag. The output is similar to: deployment.apps/nginx-deployment image updated Alternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1 : kubectl edit deployment/nginx-deployment The output is similar to: deployment.apps/nginx-deployment edited To see the rollout status, run: kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 2 out of 3 new replicas have been updated... or deployment \"nginx-deployment\" successfully rolled out Get more details on your updated Deployment: After the rollout succeeds, you can view the Deployment by running kubectl get deployments . The output is similar to this: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 36s Run kubectl get rs to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas. The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s Running get pods should now show only the new Pods: The output is similar to this: NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s Ne",
      "chunk_index": 6
    },
    {
      "chunk_id": "df6716a181e60337c74542f8f28b148ada1a1d1c",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "The output is similar to this: NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s Next time you want to update these Pods, you only need to update the Deployment's Pod template again. Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable). Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge). For example, if you look at the above Deployment closely, you will see that it first creates a new Pod, then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed. It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of a Deployment with 4 replicas, the number of Pods would be between 3 and 5. Get details of your Deployment: kubectl describe deployments The output is similar to this: Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +000",
      "chunk_index": 7
    },
    {
      "chunk_id": "16405898625137c19aea6287398b29ac72536be7",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "er of Pods would be between 3 and 5. Get details of your Deployment: kubectl describe deployments The output is similar to this: Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down repli",
      "chunk_index": 8
    },
    {
      "chunk_id": "5ec364946195cbfb48559fa1cbc5088854844b2a",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0. Note: Kubernetes doesn't count terminating Pods when calculating the number of availableReplicas , which must be between replicas - maxUnavailable and replicas + maxSurge . As a result, you might notice that there are more Pods than expected duri",
      "chunk_index": 9
    },
    {
      "chunk_id": "a70e3882c4c73e82e4aa511cf365d8ac4dd0d0d2",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ount terminating Pods when calculating the number of availableReplicas , which must be between replicas - maxUnavailable and replicas + maxSurge . As a result, you might notice that there are more Pods than expected during a rollout, and that the total resources consumed by the Deployment is more than replicas + maxSurge until the terminationGracePeriodSeconds of the terminating Pods expires. Rollover (aka multiple updates in-flight) Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template is scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0. If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously -- it will add it to its list of old ReplicaSets and start scaling it down. For example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2 , but then update the Deployment to create 5 replicas of nginx:1.16.1 , when only 3 replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately starts killing the 3 ngi",
      "chunk_index": 10
    },
    {
      "chunk_id": "073c4260f998ae118f77c426e08eccd71c4844e9",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " 5 replicas of nginx:1.14.2 , but then update the Deployment to create 5 replicas of nginx:1.16.1 , when only 3 replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2 Pods that it had created, and starts creating nginx:1.16.1 Pods. It does not wait for the 5 replicas of nginx:1.14.2 to be created before changing course. Label selector updates It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications. Note: In API version apps/v1 , a Deployment's label selector is immutable after it gets created. Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet. Selector updates changes the existing value in a selector key -- result in the same behavior as additions. Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and ",
      "chunk_index": 11
    },
    {
      "chunk_id": "eaf8b338e096a84a31c528e4e08c729e848e8445",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ey -- result in the same behavior as additions. Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets. Rolling Back a Deployment Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit). Note: A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template ( .spec.template ) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment's Pod template part is rolled back. Suppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of nginx:1.16.1 : kubectl set image deployment/nginx-deployment nginx = nginx:1.161 The outp",
      "chunk_index": 12
    },
    {
      "chunk_id": "bac01547713cbdd454a231afef68389871fe1a7e",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "rt is rolled back. Suppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of nginx:1.16.1 : kubectl set image deployment/nginx-deployment nginx = nginx:1.161 The output is similar to this: deployment.apps/nginx-deployment image updated The rollout gets stuck. You can verify it by checking the rollout status: kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 1 out of 3 new replicas have been updated... Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts, read more here . You see that the number of old replicas (adding the replica count from nginx-deployment-1564180365 and nginx-deployment-2035384211 ) is 3, and the number of new replicas (from nginx-deployment-3066724191 ) is 1. The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop. The output is similar to this: NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 ",
      "chunk_index": 13
    },
    {
      "chunk_id": "ee737df5155e252ad3c460dcb0b8290116418767",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "TS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s Note: The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters ( maxUnavailable specifically) that you have specified. Kubernetes by default sets the value to 25%. Get the description of the Deployment: kubectl describe deployment The output is similar to this: Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.161 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ----",
      "chunk_index": 14
    },
    {
      "chunk_id": "cb41c7ee40b6fe1a072b711a122ef73463069df7",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "nx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 To fix this, you need to rollback to a previous revision of Deployment that is stable. Checking Rollout History of a Deployment Follow the steps given below to check the rollout history: First, check the revisions of this ",
      "chunk_index": 15
    },
    {
      "chunk_id": "ffc3a5be9027e010e2079c25a72a914b3e8d1560",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " fix this, you need to rollback to a previous revision of Deployment that is stable. Checking Rollout History of a Deployment Follow the steps given below to check the rollout history: First, check the revisions of this Deployment: kubectl rollout history deployment/nginx-deployment The output is similar to this: deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 <none> 2 <none> 3 <none> CHANGE-CAUSE is copied from the Deployment annotation kubernetes.io/change-cause to its revisions upon creation. You can specify the CHANGE-CAUSE message by: Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image updated to 1.16.1\" Manually editing the manifest of the resource. Using tooling that sets the annotation automatically. Note: In older versions of Kubernetes, you could use the --record flag with kubectl commands to automatically populate the CHANGE-CAUSE field. This flag is deprecated and will be removed in a future release. To see the details of each revision, run: kubectl rollout history deployment/nginx-deployment --revision = 2 The output is similar to this: deployments \"nginx-deployment\" revision 2 Labels: app=nginx pod-template-hash=1159050644 Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: <none> No volumes. Rolling Back to a Previous Revision Follo",
      "chunk_index": 16
    },
    {
      "chunk_id": "baba1098f95b77d45b5afd3519b4e3cb2aa221d5",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " app=nginx pod-template-hash=1159050644 Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: <none> No volumes. Rolling Back to a Previous Revision Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2. Now you've decided to undo the current rollout and rollback to the previous revision: kubectl rollout undo deployment/nginx-deployment The output is similar to this: deployment.apps/nginx-deployment rolled back Alternatively, you can rollback to a specific revision by specifying it with --to-revision : kubectl rollout undo deployment/nginx-deployment --to-revision = 2 The output is similar to this: deployment.apps/nginx-deployment rolled back For more details about rollout related commands, read kubectl rollout . The Deployment is now rolled back to a previous stable revision. As you can see, a DeploymentRollback event for rolling back to revision 2 is generated from Deployment controller. Check if the rollback was successful and the Deployment is running as expected, run: kubectl get deployment nginx-deployment The output is similar to this: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 30m Get the description of the Deployment: kubectl describe deployment nginx-deployment The output is similar to this: Name: nginx-deployment N",
      "chunk_index": 17
    },
    {
      "chunk_id": "29ea3cb30556b5165737b8094245ef3233e614ba",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "imilar to this: NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 30m Get the description of the Deployment: kubectl describe deployment nginx-deployment The output is similar to this: Name: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment: <none> Mounts: <none> Volumes: <none> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: <none> NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c",
      "chunk_index": 18
    },
    {
      "chunk_id": "c87dff29b9d88672783980075ef5e2fc767d3320",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "yment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment \"nginx-deployment\" to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0 Scaling a Deployment You can scale a Deployment by using the following command: kubectl scale deployment/nginx-deployment --replicas = 10 The output is similar to this: deployment.apps/nginx-deployment scaled Assuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods. kubectl autoscale deployment/nginx-deployment --min = 10 --max = 15 --cpu-percent =",
      "chunk_index": 19
    },
    {
      "chunk_id": "5d9a8078aee976eb7ae722ad0f565aa9fe854b53",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "or your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods. kubectl autoscale deployment/nginx-deployment --min = 10 --max = 15 --cpu-percent = 80 The output is similar to this: deployment.apps/nginx-deployment scaled Proportional scaling RollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), the Deployment controller balances the additional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling . For example, you are running a Deployment with 10 replicas, maxSurge =3, and maxUnavailable =2. Ensure that the 10 replicas in your Deployment are running. The output is similar to this: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 50s You update to a new image which happens to be unresolvable from inside the cluster. kubectl set image deployment/nginx-deployment nginx = nginx:sometag The output is similar to this: deployment.apps/nginx-deployment image updated The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the maxUnavailable requirement that you mentioned above. Check out the roll",
      "chunk_index": 20
    },
    {
      "chunk_id": "becd28945750c8ffa53debcaabc1a513c5f733c7",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "t.apps/nginx-deployment image updated The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the maxUnavailable requirement that you mentioned above. Check out the rollout status: The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up. In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming the new replicas become healthy. To confirm this, run: The output is similar to this: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m The rollout status confirms how the replicas were added ",
      "chunk_index": 21
    },
    {
      "chunk_id": "33c8384dbe11a19c3e29ef177b0a53b9a4e63da7",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "g the new replicas become healthy. To confirm this, run: The output is similar to this: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m The rollout status confirms how the replicas were added to each ReplicaSet. The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m Pausing and Resuming a rollout of a Deployment When you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts. For example, with a Deployment that was created: Get the Deployment details: The output is similar to this: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m Get the rollout status: The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m Pause by running the following command: kubectl rollout pause deployment/nginx-deployment The output is similar to this: deployment.apps/nginx-deployment paused Then update the image of the Deployment: kubectl set image deployment/nginx-deployment nginx = nginx:1.16.1 The output is similar to this: deployment.apps/nginx-deployment image updated Notice that no n",
      "chunk_index": 22
    },
    {
      "chunk_id": "6bbce9081a6b0ff02afeff88559bc286521d0d75",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "deployment paused Then update the image of the Deployment: kubectl set image deployment/nginx-deployment nginx = nginx:1.16.1 The output is similar to this: deployment.apps/nginx-deployment image updated Notice that no new rollout started: kubectl rollout history deployment/nginx-deployment The output is similar to this: deployments \"nginx\" REVISION CHANGE-CAUSE 1 <none> Get the rollout status to verify that the existing ReplicaSet has not changed: The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m You can make as many updates as you wish, for example, update the resources that will be used: kubectl set resources deployment/nginx-deployment -c = nginx --limits = cpu = 200m,memory = 512Mi The output is similar to this: deployment.apps/nginx-deployment resource requirements updated The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to the Deployment will not have any effect as long as the Deployment rollout is paused. Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates: kubectl rollout resume deployment/nginx-deployment The output is similar to this: deployment.apps/nginx-deployment resumed Watch the status of the rollout until it's done. The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-39",
      "chunk_index": 23
    },
    {
      "chunk_id": "5e68d7527c4d0ae631d4599a0c5265ca33407463",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "nt The output is similar to this: deployment.apps/nginx-deployment resumed Watch the status of the rollout until it's done. The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s Get the status of the latest rollout: The output is similar to this: NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s Note: You cannot rollback a paused Deployment until you resume it. Deployment status A Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete , or it can fail to progress . Progressing Deployment Kubernetes marks a Deployment as progressing when one of the following tasks is performed: The Deployment creates a new ReplicaSet. The Deployment is scaling up its newest ReplicaSet. The Deployment is scaling down its older ReplicaSet(s). New Pods become ready or available (ready for at least MinReadySeconds ). When the rollout becomes “progressing”, the Deployment controller adds a condition with the following at",
      "chunk_index": 24
    },
    {
      "chunk_id": "bc1cec7a7d32e382dbdcc79a2f1ab977f90a102b",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " is scaling down its older ReplicaSet(s). New Pods become ready or available (ready for at least MinReadySeconds ). When the rollout becomes “progressing”, the Deployment controller adds a condition with the following attributes to the Deployment's .status.conditions : type: Progressing status: \"True\" reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdated You can monitor the progress for a Deployment by using kubectl rollout status . Complete Deployment Kubernetes marks a Deployment as complete when it has the following characteristics: All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any updates you've requested have been completed. All of the replicas associated with the Deployment are available. No old replicas for the Deployment are running. When the rollout becomes “complete”, the Deployment controller sets a condition with the following attributes to the Deployment's .status.conditions : type: Progressing status: \"True\" reason: NewReplicaSetAvailable This Progressing condition will retain a status value of \"True\" until a new rollout is initiated. The condition holds even when availability of replicas changes (which does instead affect the Available condition). You can check if a Deployment has completed by using kubectl rollout status . If the rollout completed successfully,",
      "chunk_index": 25
    },
    {
      "chunk_id": "f17d3b68bbf23722fcd997fd6922b3f54fbc71c9",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "n holds even when availability of replicas changes (which does instead affect the Available condition). You can check if a Deployment has completed by using kubectl rollout status . If the rollout completed successfully, kubectl rollout status returns a zero exit code. kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment \"nginx-deployment\" successfully rolled out and the exit status from kubectl rollout is 0 (success): 0 Failed Deployment Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur due to some of the following factors: Insufficient quota Readiness probe failures Image pull errors Insufficient permissions Limit ranges Application runtime misconfiguration One way you can detect this condition is to specify a deadline parameter in your Deployment spec: ( .spec.progressDeadlineSeconds ). .spec.progressDeadlineSeconds denotes the number of seconds the Deployment controller waits before indicating (in the Deployment status) that the Deployment progress has stalled. The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes: kubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":",
      "chunk_index": 26
    },
    {
      "chunk_id": "8f72541771e5eb51468f7f6d74fc32380104c43c",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes: kubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}' The output is similar to this: deployment.apps/nginx-deployment patched Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment's .status.conditions : type: Progressing status: \"False\" reason: ProgressDeadlineExceeded This condition can also fail early and is then set to status value of \"False\" due to reasons as ReplicaSetCreateError . Also, the deadline is not taken into account anymore once the Deployment rollout completes. See the Kubernetes API conventions for more information on status conditions. Note: Kubernetes takes no action on a stalled Deployment other than to report a status condition with reason: ProgressDeadlineExceeded . Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version. Note: If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering the condition for exceeding the deadline. You may experience transient errors with your Deployments, ei",
      "chunk_index": 27
    },
    {
      "chunk_id": "5f6e8af63627ce2e9b4930c41a78805e83a53dcc",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ecified deadline. You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering the condition for exceeding the deadline. You may experience transient errors with your Deployments, either due to a low timeout that you have set or due to any other kind of error that can be treated as transient. For example, let's suppose you have insufficient quota. If you describe the Deployment you will notice the following section: kubectl describe deployment nginx-deployment The output is similar to this: <...> Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate <...> If you run kubectl get deployment nginx-deployment -o yaml , the Deployment status is similar to this: status: availableReplicas: 2 conditions: - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set \"nginx-deployment-4262182780\" is progressing. reason: ReplicaSetUpdated status: \"True\" type: Progressing - lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: ",
      "chunk_index": 28
    },
    {
      "chunk_id": "52009ef0267d717860eef17bb4278035826fefba",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "reason: MinimumReplicasAvailable status: \"True\" type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2' reason: FailedCreate status: \"True\" type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the reason for the Progressing condition: Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota conditions and the Deployment controller then completes the Deployment rollout, you'll see the Deployment's status update with a successful condition ( status: \"True\" and reason: NewReplicaSetAvailable ). Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable type: Available with status: \"True\" means that your Deployment has minimum availability. Minimum availability is dictated by the parameters spec",
      "chunk_index": 29
    },
    {
      "chunk_id": "b14a7ac6c2e79ef7491654f6f751d97d8ea3bc65",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "lable True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable type: Available with status: \"True\" means that your Deployment has minimum availability. Minimum availability is dictated by the parameters specified in the deployment strategy. type: Progressing with status: \"True\" means that your Deployment is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum required new replicas are available (see the Reason of the condition for the particulars - in our case reason: NewReplicaSetAvailable means that the Deployment is complete). You can check if a Deployment has failed to progress by using kubectl rollout status . kubectl rollout status returns a non-zero exit code if the Deployment has exceeded the progression deadline. kubectl rollout status deployment/nginx-deployment The output is similar to this: Waiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment \"nginx\" exceeded its progress deadline and the exit status from kubectl rollout is 1 (indicating an error): 1 Operating on a failed deployment All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template. Clean up Policy You can set .spec.rev",
      "chunk_index": 30
    },
    {
      "chunk_id": "57af8353d3f6bbd8e7ef5eedd18205c56b89b3b9",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "t also apply to a failed Deployment. You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template. Clean up Policy You can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10. Note: Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be able to roll back. The cleanup only starts after a Deployment reaches a complete state . If you set .spec.revisionHistoryLimit to 0, any rollout nonetheless triggers creation of a new ReplicaSet before Kubernetes removes the old one. Even with a non-zero revision history limit, you can have more ReplicaSets than the limit you configure. For example, if pods are crash looping, and there are multiple rolling updates events triggered over time, you might end up with more ReplicaSets than the .spec.revisionHistoryLimit because the Deployment never reaches a complete state. Canary Deployment If you want to roll out releases to a subset of users or servers using the Deployment, you can create multiple Deployments, one for each release, following the canary pattern described in managing resources . Writing a Deployment Spec As with all other Kube",
      "chunk_index": 31
    },
    {
      "chunk_id": "e2c37db2df44e246a606f022e199d83d27543023",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": " subset of users or servers using the Deployment, you can create multiple Deployments, one for each release, following the canary pattern described in managing resources . Writing a Deployment Spec As with all other Kubernetes configs, a Deployment needs .apiVersion , .kind , and .metadata fields. For general information about working with config files, see deploying applications , configuring containers, and using kubectl to manage resources documents. When the control plane creates new Pods for a Deployment, the .metadata.name of the Deployment is part of the basis for naming those Pods. The name of a Deployment must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label . A Deployment also needs a .spec section . Pod Template The .spec.template and .spec.selector are the only required fields of the .spec . The .spec.template is a Pod template . It has exactly the same schema as a Pod , except it is nested and does not have an apiVersion or kind . In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See selector . Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default ",
      "chunk_index": 32
    },
    {
      "chunk_id": "6d8ded7f6502c28d54cef1a06facb1cc87c81159",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See selector . Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified. Replicas .spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1. Should you manually scale a Deployment, example via kubectl scale deployment deployment --replicas=X , and then you update that Deployment based on a manifest (for example: by running kubectl apply -f deployment.yaml ), then applying that manifest overwrites the manual scaling that you previously did. If a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Deployment, don't set .spec.replicas . Instead, allow the Kubernetes control plane to manage the .spec.replicas field automatically. Selector .spec.selector is a required field that specifies a label selector for the Pods targeted by this Deployment. .spec.selector must match .spec.template.metadata.labels , or it will be rejected by the API. In API version apps/v1 , .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set. So they must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1 . A Deployment may terminate Pods whose labels match the selector if",
      "chunk_index": 33
    },
    {
      "chunk_id": "fc21732d8400a4295e539cf4d963426d756b951d",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "late.metadata.labels if not set. So they must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1 . A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total number of such Pods exceeds .spec.replicas . It brings up new Pods with .spec.template if the number of Pods is less than the desired number. Note: You should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this. If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly. Strategy .spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be \"Recreate\" or \"RollingUpdate\". \"RollingUpdate\" is the default value. Recreate Deployment All existing Pods are killed before new ones are created when .spec.strategy.type==Recreate . Note: This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any P",
      "chunk_index": 34
    },
    {
      "chunk_id": "466893d4711f9b3e44ff26cee297f229cf23f567",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ate . Note: This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an \"at most\" guarantee for your Pods, you should consider using a StatefulSet . Rolling Update Deployment The Deployment updates Pods in a rolling update fashion (gradually scale down the old ReplicaSets and scale up the new one) when .spec.strategy.type==RollingUpdate . You can specify maxUnavailable and maxSurge to control the rolling update process. Max Unavailable .spec.strategy.rollingUpdate.maxUnavailable is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by rounding down. The value cannot be 0 if .spec.strategy.rollingUpdate.maxSurge is 0. The default value is 25%. For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new",
      "chunk_index": 35
    },
    {
      "chunk_id": "3bd52a0a3ceeb7aa450f1408b35a9e90efcf0a76",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "trategy.rollingUpdate.maxSurge is 0. The default value is 25%. For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods. Max Surge .spec.strategy.rollingUpdate.maxSurge is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be 0 if maxUnavailable is 0. The absolute number is calculated from the percentage by rounding up. The default value is 25%. For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods. Here are some Rolling Update Deployment examples that use the maxUnavailable and maxSurge : apiVersion : apps/v1 kind : Deployment metadata : name : ngin",
      "chunk_index": 36
    },
    {
      "chunk_id": "67cb4576ba379362423e027f4714a5bdff15bf86",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ing at any time during the update is at most 130% of desired Pods. Here are some Rolling Update Deployment examples that use the maxUnavailable and maxSurge : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 strategy : type : RollingUpdate rollingUpdate : maxUnavailable : 1 apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 strategy : type : RollingUpdate rollingUpdate : maxSurge : 1 maxUnavailable : 1 Progress Deadline Seconds .spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back",
      "chunk_index": 37
    },
    {
      "chunk_id": "ec1be7b562b1d84225c2d95a9fa3c7be6b4b1232",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "rge : 1 maxUnavailable : 1 Progress Deadline Seconds .spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has failed progressing - surfaced as a condition with type: Progressing , status: \"False\" . and reason: ProgressDeadlineExceeded in the status of the resource. The Deployment controller will keep retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment controller will roll back a Deployment as soon as it observes such a condition. If specified, this field needs to be greater than .spec.minReadySeconds . Min Ready Seconds .spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available. This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when a Pod is considered ready, see Container Probes . Terminating Pods FEATURE STATE: Kubernetes v1.35 [beta] (enabled by default) You can see the terminating pods only if the DeploymentReplicaSetTerminatingReplicas feature gate is enabled on the API server and on the kube-controller-manager Pods that become terminating due to deletion or scale down may t",
      "chunk_index": 38
    },
    {
      "chunk_id": "656c11b6b5d8ec7fab01c25aa0d08c8b7df57ce5",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "n see the terminating pods only if the DeploymentReplicaSetTerminatingReplicas feature gate is enabled on the API server and on the kube-controller-manager Pods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional resources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas . Terminating pods can be tracked using the .status.terminatingReplicas field of the Deployment. Revision History Limit A Deployment's revision history is stored in the ReplicaSets it controls. .spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs . The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments. More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up. Paused .spec.paused is an optional boolean field for pausing and resuming a Deplo",
      "chunk_index": 39
    },
    {
      "chunk_id": "273cb63852aa29053bcf41918e314f569925f467",
      "url": "https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",
      "title": "Deployments | Kubernetes",
      "text": "ets with 0 replicas will be cleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up. Paused .spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when it is created. What's next",
      "chunk_index": 40
    },
    {
      "chunk_id": "a8e26965230326f2dd99c84004ff9129ecb4f5a3",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "Ingress Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API. FEATURE STATE: Kubernetes v1.19 [stable] An API object that manages external access to the services in a cluster, typically HTTP. Ingress may provide load balancing, SSL termination and name-based virtual hosting. Note: The Kubernetes project recommends using Gateway instead of Ingress . The Ingress API has been frozen. This means that: The Ingress API is generally available, and is subject to the stability guarantees for generally available APIs. The Kubernetes project has no plans to remove Ingress from Kubernetes. The Ingress API is no longer being developed, and will have no further changes or updates made to it. Terminology For clarity, this guide defines the following terms: Node: A worker machine in Kubernetes, part of a cluster. Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet. Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware. Cl",
      "chunk_index": 0
    },
    {
      "chunk_id": "90a81ad99bc57f58fc7ee6d0739917a1b87586c9",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": " nodes in the cluster are not part of the public internet. Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware. Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes networking model . Service: A Kubernetes Service that identifies a set of Pods using label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network. What is Ingress? Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. Here is a simple example where an Ingress sends all its traffic to one Service: Figure. Ingress An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer . Prerequisi",
      "chunk_index": 1
    },
    {
      "chunk_id": "f5ab979b24b4933d99dca2da18cb39dc9bf2f2c9",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "affic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer . Prerequisites You must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. You can choose from a number of Ingress controllers . Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently. Note: Make sure you review your Ingress controller's documentation to understand the caveats of choosing it. The Ingress resource A minimal Ingress resource example: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : minimal-ingress spec : ingressClassName : nginx-example rules : - http : paths : - path : /testpath pathType : Prefix backend : service : name : test port : number : 80 An Ingress needs apiVersion , kind , metadata and spec fields. The name of an Ingress object must be a valid DNS subdomain name . For general information about working with config files, see deploying applications , configuring containers , managing resources . Ingress controllers frequently use annotations to configure behavior. Review the documentation for your choice of ingress controller to learn which annotations are expected and / or supported. The Ingres",
      "chunk_index": 2
    },
    {
      "chunk_id": "d5afaa2eb158958cc386af08c50e348cc04e5222",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "ging resources . Ingress controllers frequently use annotations to configure behavior. Review the documentation for your choice of ingress controller to learn which annotations are expected and / or supported. The Ingress spec has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP(S) traffic. If the ingressClassName is omitted, a default Ingress class should be defined. Some ingress controllers work even without the definition of a default IngressClass. Even if you use an ingress controller that is able to operate without any IngressClass, the Kubernetes project still recommends that you define a default IngressClass. Ingress rules Each HTTP rule contains the following information: An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host. A list of paths (for example, /testpath ), each of which has an associated backend defined with a service.name and a service.port.name or service.port.number . Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service. A backend is a combination of ",
      "chunk_index": 3
    },
    {
      "chunk_id": "1059ea047a1e41d76d145ac801f00e2c163bc7f1",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "ame and a service.port.name or service.port.number . Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service. A backend is a combination of Service and port names as described in the Service doc or a custom resource backend by way of a CRD . HTTP (and HTTPS) requests to the Ingress that match the host and path of the rule are sent to the listed backend. A defaultBackend is often configured in an Ingress controller to service any requests that do not match a path in the spec. DefaultBackend An Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend is the backend that should handle requests in that case. The defaultBackend is conventionally a configuration option of the Ingress controller and is not specified in your Ingress resources. If no .spec.rules are specified, .spec.defaultBackend must be specified. If defaultBackend is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case). If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend. Resource backends A Resource backend is an ObjectRef to another Kubernetes resource within the same namespace as the Ing",
      "chunk_index": 4
    },
    {
      "chunk_id": "6d8debb301366483c1b602e3f231ddd3e227a217",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend. Resource backends A Resource backend is an ObjectRef to another Kubernetes resource within the same namespace as the Ingress object. A Resource is a mutually exclusive setting with Service, and will fail validation if both are specified. A common usage for a Resource backend is to ingress data to an object storage backend with static assets. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-resource-backend spec : defaultBackend : resource : apiGroup : k8s.example.com kind : StorageBucket name : static-assets rules : - http : paths : - path : /icons pathType : ImplementationSpecific backend : resource : apiGroup : k8s.example.com kind : StorageBucket name : icon-assets After creating the Ingress above, you can view it with the following command: kubectl describe ingress ingress-resource-backend Name: ingress-resource-backend Namespace: default Address: Default backend: APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets Rules: Host Path Backends ---- ---- -------- * /icons APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets Annotations: <none> Events: <none> Path types Each path in an Ingress is required to have a corresponding path type. Paths that do not include an explicit pathType will fail validation. There are three sup",
      "chunk_index": 5
    },
    {
      "chunk_id": "444fa5178116b44a158ed491ac0ebc4f1687068a",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "e: icon-assets Annotations: <none> Events: <none> Path types Each path in an Ingress is required to have a corresponding path type. Paths that do not include an explicit pathType will fail validation. There are three supported path types: ImplementationSpecific : With this path type, matching is up to the IngressClass. Implementations can treat this as a separate pathType or treat it identically to Prefix or Exact path types. Exact : Matches the URL path exactly and with case sensitivity. Prefix : Matches based on a URL path prefix split by / . Matching is case sensitive and done on a path element by element basis. A path element refers to the list of labels in the path split by the / separator. A request is a match for path p if every p is an element-wise prefix of p of the request path. Note: If the last element of the path is a substring of the last element in request path, it is not a match (for example: /foo/bar matches /foo/bar/baz , but does not match /foo/barbaz ). Examples Kind Path(s) Request path(s) Matches? Prefix / (all paths) Yes Exact /foo /foo Yes Exact /foo /bar No Exact /foo /foo/ No Exact /foo/ /foo No Prefix /foo /foo , /foo/ Yes Prefix /foo/ /foo , /foo/ Yes Prefix /aaa/bb /aaa/bbb No Prefix /aaa/bbb /aaa/bbb Yes Prefix /aaa/bbb/ /aaa/bbb Yes, ignores trailing slash Prefix /aaa/bbb /aaa/bbb/ Yes, matches trailing slash Prefix /aaa/bbb /aaa/bbb/ccc Yes, matc",
      "chunk_index": 6
    },
    {
      "chunk_id": "dbf39bdecfbed8d6dd5c913eb28669bab355d986",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": " /foo , /foo/ Yes Prefix /aaa/bb /aaa/bbb No Prefix /aaa/bbb /aaa/bbb Yes Prefix /aaa/bbb/ /aaa/bbb Yes, ignores trailing slash Prefix /aaa/bbb /aaa/bbb/ Yes, matches trailing slash Prefix /aaa/bbb /aaa/bbb/ccc Yes, matches subpath Prefix /aaa/bbb /aaa/bbbxyz No, does not match string prefix Prefix / , /aaa /aaa/ccc Yes, matches /aaa prefix Prefix / , /aaa , /aaa/bbb /aaa/bbb Yes, matches /aaa/bbb prefix Prefix / , /aaa , /aaa/bbb /ccc Yes, matches / prefix Prefix /aaa /ccc No, uses default backend Mixed /foo (Prefix), /foo (Exact) /foo Yes, prefers Exact Multiple matches In some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest matching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type. Hostname wildcards Hosts can be precise matches (for example “ foo.bar.com ”) or a wildcard (for example “ *.foo.com ”). Precise matches require that the HTTP host header matches the host field. Wildcard matches require the HTTP host header is equal to the suffix of the wildcard rule. Host Host header Match? *.foo.com bar.foo.com Matches based on shared suffix *.foo.com baz.bar.foo.com No match, wildcard only covers a single DNS label *.foo.com foo.com No match, wildcard only covers a single DNS label apiVersion : networking.k8s.io/v1 kind : Ingres",
      "chunk_index": 7
    },
    {
      "chunk_id": "74dd1c592eae470d3bc5f67d907decc139ba9d52",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "tches based on shared suffix *.foo.com baz.bar.foo.com No match, wildcard only covers a single DNS label *.foo.com foo.com No match, wildcard only covers a single DNS label apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : ingress-wildcard-host spec : rules : - host : \"foo.bar.com\" http : paths : - pathType : Prefix path : \"/bar\" backend : service : name : service1 port : number : 80 - host : \"*.foo.com\" http : paths : - pathType : Prefix path : \"/foo\" backend : service : name : service2 port : number : 80 Ingress class Ingresses can be implemented by different controllers, often with different configuration. Each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class. apiVersion : networking.k8s.io/v1 kind : IngressClass metadata : name : external-lb spec : controller : example.com/ingress-controller parameters : apiGroup : k8s.example.com kind : IngressParameters name : external-lb The .spec.parameters field of an IngressClass lets you reference another resource that provides configuration related to that IngressClass. The specific type of parameters to use depends on the ingress controller that you specify in the .spec.controller field of the IngressClass. IngressClass scope Depending on your ingress controller, you may be able to use ",
      "chunk_index": 8
    },
    {
      "chunk_id": "a5d1eb903ed6483e5fb4959f4227486eb3a88d22",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "he specific type of parameters to use depends on the ingress controller that you specify in the .spec.controller field of the IngressClass. IngressClass scope Depending on your ingress controller, you may be able to use parameters that you set cluster-wide, or just for one namespace. The default scope for IngressClass parameters is cluster-wide. If you set the .spec.parameters field and don't set .spec.parameters.scope , or if you set .spec.parameters.scope to Cluster , then the IngressClass refers to a cluster-scoped resource. The kind (in combination the apiGroup ) of the parameters refers to a cluster-scoped API (possibly a custom resource), and the name of the parameters identifies a specific cluster scoped resource for that API. For example: --- apiVersion : networking.k8s.io/v1 kind : IngressClass metadata : name : external-lb-1 spec : controller : example.com/ingress-controller parameters : # The parameters for this IngressClass are specified in a # ClusterIngressParameter (API group k8s.example.net) named # \"external-config-1\". This definition tells Kubernetes to # look for a cluster-scoped parameter resource. scope : Cluster apiGroup : k8s.example.net kind : ClusterIngressParameter name : external-config-1 FEATURE STATE: Kubernetes v1.23 [stable] If you set the .spec.parameters field and set .spec.parameters.scope to Namespace , then the IngressClass refers to a namesp",
      "chunk_index": 9
    },
    {
      "chunk_id": "139cd6c79b51a9b37a71e26dc8865e4fc7a1fd1c",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "ind : ClusterIngressParameter name : external-config-1 FEATURE STATE: Kubernetes v1.23 [stable] If you set the .spec.parameters field and set .spec.parameters.scope to Namespace , then the IngressClass refers to a namespaced-scoped resource. You must also set the namespace field within .spec.parameters to the namespace that contains the parameters you want to use. The kind (in combination the apiGroup ) of the parameters refers to a namespaced API (for example: ConfigMap), and the name of the parameters identifies a specific resource in the namespace you specified in namespace . Namespace-scoped parameters help the cluster operator delegate control over the configuration (for example: load balancer settings, API gateway definition) that is used for a workload. If you used a cluster-scoped parameter then either: the cluster operator team needs to approve a different team's changes every time there's a new configuration change being applied. the cluster operator must define specific access controls, such as RBAC roles and bindings, that let the application team make changes to the cluster-scoped parameters resource. The IngressClass API itself is always cluster-scoped. Here is an example of an IngressClass that refers to parameters that are namespaced: --- apiVersion : networking.k8s.io/v1 kind : IngressClass metadata : name : external-lb-2 spec : controller : example.com/ingress",
      "chunk_index": 10
    },
    {
      "chunk_id": "3fc482351ed3efabdf9105900a0a8caccbf12978",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "oped. Here is an example of an IngressClass that refers to parameters that are namespaced: --- apiVersion : networking.k8s.io/v1 kind : IngressClass metadata : name : external-lb-2 spec : controller : example.com/ingress-controller parameters : # The parameters for this IngressClass are specified in an # IngressParameter (API group k8s.example.com) named \"external-config\", # that's in the \"external-configuration\" namespace. scope : Namespace apiGroup : k8s.example.com kind : IngressParameter namespace : external-configuration name : external-config Deprecated annotation Before the IngressClass resource and ingressClassName field were added in Kubernetes 1.18, Ingress classes were specified with a kubernetes.io/ingress.class annotation on the Ingress. This annotation was never formally defined, but was widely supported by Ingress controllers. The newer ingressClassName field on Ingresses is a replacement for that annotation, but is not a direct equivalent. While the annotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field is a reference to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller. Default IngressClass You can mark a particular IngressClass as default for your cluster. Setting the ingressclass.kubernetes.io/is-default-class annotation to t",
      "chunk_index": 11
    },
    {
      "chunk_id": "8ea053f1898b8635c833112ffea168a39f701fcb",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "onfiguration, including the name of the Ingress controller. Default IngressClass You can mark a particular IngressClass as default for your cluster. Setting the ingressclass.kubernetes.io/is-default-class annotation to true on an IngressClass resource will ensure that new Ingresses without an ingressClassName field specified will be assigned this default IngressClass. Caution: If you have more than one IngressClass marked as the default for your cluster, the admission controller prevents creating new Ingress objects that don't have an ingressClassName specified. You can resolve this by ensuring that at most 1 IngressClass is marked as default in your cluster. Start by defining a default IngressClass. It is recommended though, to specify the default IngressClass: apiVersion : networking.k8s.io/v1 kind : IngressClass metadata : labels : app.kubernetes.io/component : controller name : example-class annotations : ingressclass.kubernetes.io/is-default-class : \"true\" spec : controller : k8s.io/example-class Types of Ingress Ingress backed by a single Service There are existing Kubernetes concepts that allow you to expose a single Service (see alternatives ). You can also do this with an Ingress by specifying a default backend with no rules. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : test-ingress spec : defaultBackend : service : name : test port : number : 80 ",
      "chunk_index": 12
    },
    {
      "chunk_id": "7ca2c48a08f8d993986da0943b6c419e4737cfa4",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "lso do this with an Ingress by specifying a default backend with no rules. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : test-ingress spec : defaultBackend : service : name : test port : number : 80 If you create it using kubectl apply -f you should be able to view the state of the Ingress you added: kubectl get ingress test-ingress NAME CLASS HOSTS ADDRESS PORTS AGE test-ingress external-lb * 203.0.113.123 80 59s Where 203.0.113.123 is the IP allocated by the Ingress controller to satisfy this Ingress. Note: Ingress controllers and load balancers may take a minute or two to allocate an IP address. Until that time, you often see the address listed as <pending> . Simple fanout A fanout configuration routes traffic from a single IP address to more than one Service, based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers down to a minimum. For example, a setup like: Figure. Ingress Fan Out It would require an Ingress such as: apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : simple-fanout-example spec : rules : - host : foo.bar.com http : paths : - path : /foo pathType : Prefix backend : service : name : service1 port : number : 4200 - path : /bar pathType : Prefix backend : service : name : service2 port : number : 8080 When you create the Ingress with kubectl apply -f : kubectl describe ingress simpl",
      "chunk_index": 13
    },
    {
      "chunk_id": "27b73729ad4b1a62f2a670de75aad3a6367503ba",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": ": service : name : service1 port : number : 4200 - path : /bar pathType : Prefix backend : service : name : service2 port : number : 8080 When you create the Ingress with kubectl apply -f : kubectl describe ingress simple-fanout-example Name: simple-fanout-example Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:4200 (10.8.0.90:4200) /bar service2:8080 (10.8.0.91:8080) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 22s loadbalancer-controller default/test The Ingress controller provisions an implementation-specific load balancer that satisfies the Ingress, as long as the Services ( service1 , service2 ) exist. When it has done so, you can see the address of the load balancer at the Address field. Name based virtual hosting Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address. Figure. Ingress Name Based Virtual hosting The following Ingress tells the backing load balancer to route requests based on the Host header . apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : name-virtual-host-ingress spec : rules : - host : foo.bar.com http : paths : - pathType : Prefix path : \"/\" backend : service : name : service1 port : number : 80 - host : bar.foo.com http : paths : - pat",
      "chunk_index": 14
    },
    {
      "chunk_id": "6e2f3750994d6718631f86164f32213750e219d8",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "tadata : name : name-virtual-host-ingress spec : rules : - host : foo.bar.com http : paths : - pathType : Prefix path : \"/\" backend : service : name : service1 port : number : 80 - host : bar.foo.com http : paths : - pathType : Prefix path : \"/\" backend : service : name : service2 port : number : 80 If you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress controller can be matched without a name based virtual host being required. For example, the following Ingress routes traffic requested for first.bar.com to service1 , second.bar.com to service2 , and any traffic whose request host header doesn't match first.bar.com and second.bar.com to service3 . apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : name-virtual-host-ingress-no-third-host spec : rules : - host : first.bar.com http : paths : - pathType : Prefix path : \"/\" backend : service : name : service1 port : number : 80 - host : second.bar.com http : paths : - pathType : Prefix path : \"/\" backend : service : name : service2 port : number : 80 - http : paths : - pathType : Prefix path : \"/\" backend : service : name : service3 port : number : 80 TLS You can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. The Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the ",
      "chunk_index": 15
    },
    {
      "chunk_id": "6702c8de11439b648167965e8d12b01ef04a8c0f",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "ice3 port : number : 80 TLS You can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. The Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the ingress point (traffic to the Service and its Pods is in plaintext). If the TLS configuration section in an Ingress specifies different hosts, they are multiplexed on the same port according to the hostname specified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named tls.crt and tls.key that contain the certificate and private key to use for TLS. For example: apiVersion : v1 kind : Secret metadata : name : testsecret-tls namespace : default data : tls.crt : base64 encoded cert tls.key : base64 encoded key type : kubernetes.io/tls Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the client to the load balancer using TLS. You need to make sure the TLS secret you created came from a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain Name (FQDN) for https-example.foo.com . Note: Keep in mind that TLS will not work on the default rule because the certificates would have to be issued for all the possible sub-domains. Therefore, hosts in the tls section need to explicitly match the host in the rules section. apiVersion : netwo",
      "chunk_index": 16
    },
    {
      "chunk_id": "e99ded3e0209b48fdb582ae7a0468b9b880e763e",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "t work on the default rule because the certificates would have to be issued for all the possible sub-domains. Therefore, hosts in the tls section need to explicitly match the host in the rules section. apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - https-example.foo.com secretName : testsecret-tls rules : - host : https-example.foo.com http : paths : - path : / pathType : Prefix backend : service : name : service1 port : number : 80 Note: There is a gap between TLS features supported by various ingress controllers. You should refer to the documentation for the ingress controller(s) you've chosen to understand how TLS works in your environment. Load balancing An Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service. It's also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as readiness probes that allow you to achieve the same end result. Please review the controller specific documentation to see how t",
      "chunk_index": 17
    },
    {
      "chunk_id": "17b5c2a4fd6dcd87f0c1df553dbd1609daab14ab",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "t exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as readiness probes that allow you to achieve the same end result. Please review the controller specific documentation to see how they handle health checks. Updating an Ingress To update an existing Ingress to add a new Host, you can update it by editing the resource: kubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 35s loadbalancer-controller default/test kubectl edit ingress test This pops up an editor with the existing configuration in YAML format. Modify it to include the new Host: spec : rules : - host : foo.bar.com http : paths : - backend : service : name : service1 port : number : 80 path : /foo pathType : Prefix - host : bar.baz.com http : paths : - backend : service : name : service2 port : number : 80 path : /foo pathType : Prefix .. After you save your changes, kubectl updates the resource in the API server, which tells the Ingress controller to reconfigure the load balancer. Verify this: kubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:808",
      "chunk_index": 18
    },
    {
      "chunk_id": "5cb6cca29466075c9952fecf24d7e5f8600e8e58",
      "url": "https://kubernetes.io/docs/concepts/services-networking/ingress/",
      "title": "Ingress | Kubernetes",
      "text": "which tells the Ingress controller to reconfigure the load balancer. Verify this: kubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends ---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) bar.baz.com /foo service2:80 (10.8.0.91:80) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ADD 45s loadbalancer-controller default/test You can achieve the same outcome by invoking kubectl replace -f on a modified Ingress YAML file. Failing across availability zones Techniques for spreading traffic across failure domains differ between cloud providers. Please check the documentation of the relevant Ingress controller for details. Alternatives You can expose a Service in multiple ways that don't directly involve the Ingress resource: What's next",
      "chunk_index": 19
    },
    {
      "chunk_id": "4eee7ed9ca6f396ec10c84a871b795ca2ddf6e78",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "Secrets A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image . Using a Secret means that you don't need to include confidential data in your application code. Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing sensitive data to nonvolatile storage. Secrets are similar to ConfigMaps but are specifically intended to hold confidential data. Caution: Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment. In order to safely use Secrets, take at least the following steps: Enable Encryption at Rest for Secrets. Enable or configure RBAC rules with least-privilege access to Secrets. Restrict Secret access to specific containers. Consider ",
      "chunk_index": 0
    },
    {
      "chunk_id": "497f50081cec89fb8c452a6f1395052ef3163ef5",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "afely use Secrets, take at least the following steps: Enable Encryption at Rest for Secrets. Enable or configure RBAC rules with least-privilege access to Secrets. Restrict Secret access to specific containers. Consider using external Secret store providers . For more guidelines to manage and improve the security of your Secrets, refer to Good practices for Kubernetes Secrets . See Information security for Secrets for more details. Uses for Secrets You can use Secrets for purposes such as the following: The Kubernetes control plane also uses Secrets; for example, bootstrap token Secrets are a mechanism to help automate node registration. Use case: dotfiles in a secret volume You can make your data \"hidden\" by defining a key that begins with a dot. This key represents a dotfile or \"hidden\" file. For example, when the following Secret is mounted into a volume, secret-volume , the volume will contain a single file, called .secret-file , and the dotfile-test-container will have this file present at the path /etc/secret-volume/.secret-file . Note: Files beginning with dot characters are hidden from the output of ls -l ; you must use ls -la to see them when listing directory contents. apiVersion : v1 kind : Secret metadata : name : dotfile-secret data : .secret-file : dmFsdWUtMg0KDQo= --- apiVersion : v1 kind : Pod metadata : name : secret-dotfiles-pod spec : volumes : - name : secre",
      "chunk_index": 1
    },
    {
      "chunk_id": "370ce981a55e8f09bd484553ecef929bfa3b3666",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "rectory contents. apiVersion : v1 kind : Secret metadata : name : dotfile-secret data : .secret-file : dmFsdWUtMg0KDQo= --- apiVersion : v1 kind : Pod metadata : name : secret-dotfiles-pod spec : volumes : - name : secret-volume secret : secretName : dotfile-secret containers : - name : dotfile-test-container image : registry.k8s.io/busybox command : - ls - \"-l\" - \"/etc/secret-volume\" volumeMounts : - name : secret-volume readOnly : true mountPath : \"/etc/secret-volume\" Use case: Secret visible to one container in a Pod Consider a program that needs to handle HTTP requests, do some complex business logic, and then sign some messages with an HMAC. Because it has complex application logic, there might be an unnoticed remote file reading exploit in the server, which could expose the private key to an attacker. This could be divided into two processes in two containers: a frontend container which handles user interaction and business logic, but which cannot see the private key; and a signer container that can see the private key, and responds to simple signing requests from the frontend (for example, over localhost networking). With this partitioned approach, an attacker now has to trick the application server into doing something rather arbitrary, which may be harder than getting it to read a file. Alternatives to Secrets Rather than using a Secret to protect confidential data, yo",
      "chunk_index": 2
    },
    {
      "chunk_id": "d47f05f911d9544a122a1088905698a18d0fbcf2",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "acker now has to trick the application server into doing something rather arbitrary, which may be harder than getting it to read a file. Alternatives to Secrets Rather than using a Secret to protect confidential data, you can pick from alternatives. Here are some of your options: If your cloud-native component needs to authenticate to another application that you know is running within the same Kubernetes cluster, you can use a ServiceAccount and its tokens to identify your client. There are third-party tools that you can run, either within or outside your cluster, that manage sensitive data. For example, a service that Pods access over HTTPS, that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount token). For authentication, you can implement a custom signer for X.509 certificates, and use CertificateSigningRequests to let that custom signer issue certificates to Pods that need them. You can use a device plugin to expose node-local encryption hardware to a specific Pod. For example, you can schedule trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band. You can also combine two or more of those options, including the option to use Secret objects themselves. For example: implement (or deploy) an operator that fetches short-lived session tokens from an external service, and then creates Secrets based on tho",
      "chunk_index": 3
    },
    {
      "chunk_id": "71c82c4788558cf3c62f6a0e96b77411411babe6",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "those options, including the option to use Secret objects themselves. For example: implement (or deploy) an operator that fetches short-lived session tokens from an external service, and then creates Secrets based on those short-lived session tokens. Pods running in your cluster can make use of the session tokens, and operator ensures they are valid. This separation means that you can run Pods that are unaware of the exact mechanisms for issuing and refreshing those session tokens. Types of Secret When creating a Secret, you can specify its type using the type field of the Secret resource, or certain equivalent kubectl command line flags (if available). The Secret type is used to facilitate programmatic handling of the Secret data. Kubernetes provides several built-in types for some common usage scenarios. These types vary in terms of the validations performed and the constraints Kubernetes imposes on them. Built-in Type Usage Opaque arbitrary user-defined data kubernetes.io/service-account-token ServiceAccount token kubernetes.io/dockercfg serialized ~/.dockercfg file kubernetes.io/dockerconfigjson serialized ~/.docker/config.json file kubernetes.io/basic-auth credentials for basic authentication kubernetes.io/ssh-auth credentials for SSH authentication kubernetes.io/tls data for a TLS client or server bootstrap.kubernetes.io/token bootstrap token data You can define and use y",
      "chunk_index": 4
    },
    {
      "chunk_id": "a14e7fe6415aaaea7997757032f779955f548eec",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "credentials for basic authentication kubernetes.io/ssh-auth credentials for SSH authentication kubernetes.io/tls data for a TLS client or server bootstrap.kubernetes.io/token bootstrap token data You can define and use your own Secret type by assigning a non-empty string as the type value for a Secret object (an empty string is treated as an Opaque type). Kubernetes doesn't impose any constraints on the type name. However, if you are using one of the built-in types, you must meet all the requirements defined for that type. If you are defining a type of Secret that's for public use, follow the convention and structure the Secret type to have your domain name before the name, separated by a / . For example: cloud-hosting.example.net/cloud-api-credentials . Opaque Secrets Opaque is the default Secret type if you don't explicitly specify a type in a Secret manifest. When you create a Secret using kubectl , you must use the generic subcommand to indicate an Opaque Secret type. For example, the following command creates an empty Secret of type Opaque : kubectl create secret generic empty-secret kubectl get secret empty-secret The output looks like: NAME TYPE DATA AGE empty-secret Opaque 0 2m6s The DATA column shows the number of data items stored in the Secret. In this case, 0 means you have created an empty Secret. ServiceAccount token Secrets A kubernetes.io/service-account-token t",
      "chunk_index": 5
    },
    {
      "chunk_id": "a8e028505cc4b92a13b3985e0e335642eb49e994",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "empty-secret Opaque 0 2m6s The DATA column shows the number of data items stored in the Secret. In this case, 0 means you have created an empty Secret. ServiceAccount token Secrets A kubernetes.io/service-account-token type of Secret is used to store a token credential that identifies a ServiceAccount . This is a legacy mechanism that provides long-lived ServiceAccount credentials to Pods. In Kubernetes v1.22 and later, the recommended approach is to obtain a short-lived, automatically rotating ServiceAccount token by using the TokenRequest API instead. You can get these short-lived tokens using the following methods: Note: You should only create a ServiceAccount token Secret if you can't use the TokenRequest API to obtain a token, and the security exposure of persisting a non-expiring token credential in a readable API object is acceptable to you. For instructions, see Manually create a long-lived API token for a ServiceAccount . When using this Secret type, you need to ensure that the kubernetes.io/service-account.name annotation is set to an existing ServiceAccount name. If you are creating both the ServiceAccount and the Secret objects, you should create the ServiceAccount object first. After the Secret is created, a Kubernetes controller fills in some other fields such as the kubernetes.io/service-account.uid annotation, and the token key in the data field, which is popula",
      "chunk_index": 6
    },
    {
      "chunk_id": "0c3479d39b650f8ad5907d177628f2a1bc601178",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " ServiceAccount object first. After the Secret is created, a Kubernetes controller fills in some other fields such as the kubernetes.io/service-account.uid annotation, and the token key in the data field, which is populated with an authentication token. The following example configuration declares a ServiceAccount token Secret: apiVersion : v1 kind : Secret metadata : name : secret-sa-sample annotations : kubernetes.io/service-account.name : \"sa-name\" type : kubernetes.io/service-account-token data : extra : YmFyCg== After creating the Secret, wait for Kubernetes to populate the token key in the data field. See the ServiceAccount documentation for more information on how ServiceAccounts work. You can also check the automountServiceAccountToken field and the serviceAccountName field of the Pod for information on referencing ServiceAccount credentials from within Pods. Docker config Secrets If you are creating a Secret to store credentials for accessing a container image registry, you must use one of the following type values for that Secret: kubernetes.io/dockercfg : store a serialized ~/.dockercfg which is the legacy format for configuring Docker command line. The Secret data field contains a .dockercfg key whose value is the content of a base64 encoded ~/.dockercfg file. kubernetes.io/dockerconfigjson : store a serialized JSON that follows the same format rules as the ~/.docke",
      "chunk_index": 7
    },
    {
      "chunk_id": "caf4e086afec70773e9fd08a5146bdcc5831dd91",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "he Secret data field contains a .dockercfg key whose value is the content of a base64 encoded ~/.dockercfg file. kubernetes.io/dockerconfigjson : store a serialized JSON that follows the same format rules as the ~/.docker/config.json file, which is a new format for ~/.dockercfg . The Secret data field must contain a .dockerconfigjson key for which the value is the content of a base64 encoded ~/.docker/config.json file. Below is an example for a kubernetes.io/dockercfg type of Secret: apiVersion : v1 kind : Secret metadata : name : secret-dockercfg type : kubernetes.io/dockercfg data : .dockercfg : | eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo= Note: If you do not want to perform the base64 encoding, you can choose to use the stringData field instead. When you create Docker config Secrets using a manifest, the API server checks whether the expected key exists in the data field, and it verifies if the value provided can be parsed as a valid JSON. The API server doesn't validate if the JSON actually is a Docker config file. You can also use kubectl to create a Secret for accessing a container registry, such as when you don't have a Docker configuration file: kubectl create secret docker-registry secret-tiger-docker \\ --docker-email = tiger@acme.example \\ --docker-username = tiger \\ --docker-password = pass1234 \\ --docker-server = my-registry.example",
      "chunk_index": 8
    },
    {
      "chunk_id": "ce5bee05f9e26252b2a4eb5a71dc2b68b3f455bc",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "ocker configuration file: kubectl create secret docker-registry secret-tiger-docker \\ --docker-email = tiger@acme.example \\ --docker-username = tiger \\ --docker-password = pass1234 \\ --docker-server = my-registry.example:5000 This command creates a Secret of type kubernetes.io/dockerconfigjson . Retrieve the .data.dockerconfigjson field from that new Secret and decode the data: kubectl get secret secret-tiger-docker -o jsonpath = '{.data.*}' | base64 -d The output is equivalent to the following JSON document (which is also a valid Docker configuration file): { \"auths\" : { \"my-registry.example:5000\" : { \"username\" : \"tiger\" , \"password\" : \"pass1234\" , \"email\" : \"tiger@acme.example\" , \"auth\" : \"dGlnZXI6cGFzczEyMzQ=\" } } } Caution: The auth value there is base64 encoded; it is obscured but not secret. Anyone who can read that Secret can learn the registry access bearer token. It is suggested to use credential providers to dynamically and securely provide pull secrets on-demand. Basic authentication Secret The kubernetes.io/basic-auth type is provided for storing credentials needed for basic authentication. When using this Secret type, the data field of the Secret must contain one of the following two keys: username : the user name for authentication password : the password or token for authentication Both values for the above two keys are base64 encoded strings. You can alternativ",
      "chunk_index": 9
    },
    {
      "chunk_id": "ea02075c01414b6a912027a42eb3a28d106dc882",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "ust contain one of the following two keys: username : the user name for authentication password : the password or token for authentication Both values for the above two keys are base64 encoded strings. You can alternatively provide the clear text content using the stringData field in the Secret manifest. The following manifest is an example of a basic authentication Secret: apiVersion : v1 kind : Secret metadata : name : secret-basic-auth type : kubernetes.io/basic-auth stringData : username : admin # required field for kubernetes.io/basic-auth password : t0p-Secret # required field for kubernetes.io/basic-auth Note: The stringData field for a Secret does not work well with server-side apply. The basic authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for basic authentication. However, using the defined and public Secret type ( kubernetes.io/basic-auth ) helps other people to understand the purpose of your Secret, and sets a convention for what key names to expect. SSH authentication Secrets The builtin type kubernetes.io/ssh-auth is provided for storing data used in SSH authentication. When using this Secret type, you will have to specify a ssh-privatekey key-value pair in the data (or stringData ) field as the SSH credential to use. The following manifest is an example of a Secret used for SSH public/private key au",
      "chunk_index": 10
    },
    {
      "chunk_id": "13f9fee4c4967a4cfa3827e5e2d65c3743e76502",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "s Secret type, you will have to specify a ssh-privatekey key-value pair in the data (or stringData ) field as the SSH credential to use. The following manifest is an example of a Secret used for SSH public/private key authentication: apiVersion : v1 kind : Secret metadata : name : secret-ssh-auth type : kubernetes.io/ssh-auth data : # the data is abbreviated in this example ssh-privatekey : | UG91cmluZzYlRW1vdGljb24lU2N1YmE= The SSH authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for SSH authentication. However, using the defined and public Secret type ( kubernetes.io/ssh-auth ) helps other people to understand the purpose of your Secret, and sets a convention for what key names to expect. The Kubernetes API verifies that the required keys are set for a Secret of this type. Caution: SSH private keys do not establish trusted communication between an SSH client and host server on their own. A secondary means of establishing trust is needed to mitigate \"man in the middle\" attacks, such as a known_hosts file added to a ConfigMap. TLS Secrets The kubernetes.io/tls Secret type is for storing a certificate and its associated key that are typically used for TLS. One common use for TLS Secrets is to configure encryption in transit for an Ingress , but you can also use it with other resources or directly in your workload. Wh",
      "chunk_index": 11
    },
    {
      "chunk_id": "a40cdb4f158f20ddaea9beeb54c59fd7bac0f9ad",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " and its associated key that are typically used for TLS. One common use for TLS Secrets is to configure encryption in transit for an Ingress , but you can also use it with other resources or directly in your workload. When using this type of Secret, the tls.key and the tls.crt key must be provided in the data (or stringData ) field of the Secret configuration, although the API server doesn't actually validate the values for each key. As an alternative to using stringData , you can use the data field to provide the base64 encoded certificate and private key. For details, see Constraints on Secret names and data . The following YAML contains an example config for a TLS Secret: apiVersion : v1 kind : Secret metadata : name : secret-tls type : kubernetes.io/tls data : # values are base64 encoded, which obscures them but does NOT provide # any useful level of confidentiality # Replace the following values with your own base64-encoded certificate and key. tls.crt : \"REPLACE_WITH_BASE64_CERT\" tls.key : \"REPLACE_WITH_BASE64_KEY\" The TLS Secret type is provided only for convenience. You can create an Opaque type for credentials used for TLS authentication. However, using the defined and public Secret type ( kubernetes.io/tls ) helps ensure the consistency of Secret format in your project. The API server verifies if the required keys are set for a Secret of this type. To create a TLS Sec",
      "chunk_index": 12
    },
    {
      "chunk_id": "e9410c69553e933e3200e91fd9aa412c694b0ca5",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " the defined and public Secret type ( kubernetes.io/tls ) helps ensure the consistency of Secret format in your project. The API server verifies if the required keys are set for a Secret of this type. To create a TLS Secret using kubectl , use the tls subcommand: kubectl create secret tls my-tls-secret \\ --cert = path/to/cert/file \\ --key = path/to/key/file The public/private key pair must exist before hand. The public key certificate for --cert must be .PEM encoded and must match the given private key for --key . Bootstrap token Secrets The bootstrap.kubernetes.io/token Secret type is for tokens used during the node bootstrap process. It stores tokens used to sign well-known ConfigMaps. A bootstrap token Secret is usually created in the kube-system namespace and named in the form bootstrap-token-<token-id> where <token-id> is a 6 character string of the token ID. As a Kubernetes manifest, a bootstrap token Secret might look like the following: apiVersion : v1 kind : Secret metadata : name : bootstrap-token-5emitj namespace : kube-system type : bootstrap.kubernetes.io/token data : auth-extra-groups : c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4= expiration : MjAyMC0wOS0xM1QwNDozOToxMFo= token-id : NWVtaXRq token-secret : a3E0Z2lodnN6emduMXAwcg== usage-bootstrap-authentication : dHJ1ZQ== usage-bootstrap-signing : dHJ1ZQ== A bootstrap token Secret has the follo",
      "chunk_index": 13
    },
    {
      "chunk_id": "330ccd616fc19c1869047c9eabce320bc0886474",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "4= expiration : MjAyMC0wOS0xM1QwNDozOToxMFo= token-id : NWVtaXRq token-secret : a3E0Z2lodnN6emduMXAwcg== usage-bootstrap-authentication : dHJ1ZQ== usage-bootstrap-signing : dHJ1ZQ== A bootstrap token Secret has the following keys specified under data : token-id : A random 6 character string as the token identifier. Required. token-secret : A random 16 character string as the actual token Secret. Required. description : A human-readable string that describes what the token is used for. Optional. expiration : An absolute UTC time using RFC3339 specifying when the token should be expired. Optional. usage-bootstrap-<usage> : A boolean flag indicating additional usage for the bootstrap token. auth-extra-groups : A comma-separated list of group names that will be authenticated as in addition to the system:bootstrappers group. You can alternatively provide the values in the stringData field of the Secret without base64 encoding them: apiVersion : v1 kind : Secret metadata : # Note how the Secret is named name : bootstrap-token-5emitj # A bootstrap token Secret usually resides in the kube-system namespace namespace : kube-system type : bootstrap.kubernetes.io/token stringData : auth-extra-groups : \"system:bootstrappers:kubeadm:default-node-token\" expiration : \"2020-09-13T04:39:10Z\" # This token ID is used in the name token-id : \"5emitj\" token-secret : \"kq4gihvszzgn1p0r\" # This token ca",
      "chunk_index": 14
    },
    {
      "chunk_id": "61da620c8f5c13b78d7bdcf898a3ce2384d26b9b",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "ngData : auth-extra-groups : \"system:bootstrappers:kubeadm:default-node-token\" expiration : \"2020-09-13T04:39:10Z\" # This token ID is used in the name token-id : \"5emitj\" token-secret : \"kq4gihvszzgn1p0r\" # This token can be used for authentication usage-bootstrap-authentication : \"true\" # and it can be used for signing usage-bootstrap-signing : \"true\" Note: The stringData field for a Secret does not work well with server-side apply. Working with Secrets Creating a Secret There are several options to create a Secret: Constraints on Secret names and data The name of a Secret object must be a valid DNS subdomain name . You can specify the data and/or the stringData field when creating a configuration file for a Secret. The data and the stringData fields are optional. The values for all keys in the data field have to be base64-encoded strings. If the conversion to base64 string is not desirable, you can choose to specify the stringData field instead, which accepts arbitrary strings as values. The keys of data and stringData must consist of alphanumeric characters, - , _ or . . All key-value pairs in the stringData field are internally merged into the data field. If a key appears in both the data and the stringData field, the value specified in the stringData field takes precedence. Size limit Individual Secrets are limited to 1MiB in size. This is to discourage creation of very la",
      "chunk_index": 15
    },
    {
      "chunk_id": "634bc73021d75784540f3131c4d07015102f1180",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " a key appears in both the data and the stringData field, the value specified in the stringData field takes precedence. Size limit Individual Secrets are limited to 1MiB in size. This is to discourage creation of very large Secrets that could exhaust the API server and kubelet memory. However, creation of many smaller Secrets could also exhaust memory. You can use a resource quota to limit the number of Secrets (or other resources) in a namespace. Editing a Secret You can edit an existing Secret unless it is immutable . To edit a Secret, use one of the following methods: You can also edit the data in a Secret using the Kustomize tool . However, this method creates a new Secret object with the edited data. Depending on how you created the Secret, as well as how the Secret is used in your Pods, updates to existing Secret objects are propagated automatically to Pods that use the data. For more information, refer to Using Secrets as files from a Pod section. Using a Secret Secrets can be mounted as data volumes or exposed as environment variables to be used by a container in a Pod. Secrets can also be used by other parts of the system, without being directly exposed to the Pod. For example, Secrets can hold credentials that other parts of the system should use to interact with external systems on your behalf. Secret volume sources are validated to ensure that the specified object r",
      "chunk_index": 16
    },
    {
      "chunk_id": "9bc5b13c521519dd7679c27c0bf3d173ec554712",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " to the Pod. For example, Secrets can hold credentials that other parts of the system should use to interact with external systems on your behalf. Secret volume sources are validated to ensure that the specified object reference actually points to an object of type Secret. Therefore, a Secret needs to be created before any Pods that depend on it. If the Secret cannot be fetched (perhaps because it does not exist, or due to a temporary lack of connection to the API server) the kubelet periodically retries running that Pod. The kubelet also reports an Event for that Pod, including details of the problem fetching the Secret. Optional Secrets When you reference a Secret in a Pod, you can mark the Secret as optional , such as in the following example. If an optional Secret doesn't exist, Kubernetes ignores it. apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret optional : true By default, Secrets are required. None of a Pod's containers will start until all non-optional Secrets are available. If a Pod references a specific key in a non-optional Secret and that Secret does exist, but is missing the named key, the Pod fails during startup. Using Secrets as files from a Pod If you want to access data from a Secret in a ",
      "chunk_index": 17
    },
    {
      "chunk_id": "e59a8dc0c36742cf0ed3cbef1d997731d4f4123b",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " references a specific key in a non-optional Secret and that Secret does exist, but is missing the named key, the Pod fails during startup. Using Secrets as files from a Pod If you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes make the value of that Secret be available as a file inside the filesystem of one or more of the Pod's containers. For instructions, refer to Create a Pod that has access to the secret data through a Volume . When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks this and updates the data in the volume, using an eventually-consistent approach. Note: A container using a Secret as a subPath volume mount does not receive automated Secret updates. The kubelet keeps a cache of the current keys and values for the Secrets that are used in volumes for pods on that node. You can configure the way that the kubelet detects changes from the cached values. The configMapAndSecretChangeDetectionStrategy field in the kubelet configuration controls which strategy the kubelet uses. The default strategy is Watch . Updates to Secrets can be either propagated by an API watch mechanism (the default), based on a cache with a defined time-to-live, or polled from the cluster API server on each kubelet synchronisation loop. As a result, the total delay from the moment when the Secret is updated to the moment ",
      "chunk_index": 18
    },
    {
      "chunk_id": "a83ba0116465ace49ada429db9431ac137886a42",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "e default), based on a cache with a defined time-to-live, or polled from the cluster API server on each kubelet synchronisation loop. As a result, the total delay from the moment when the Secret is updated to the moment when new keys are projected to the Pod can be as long as the kubelet sync period + cache propagation delay, where the cache propagation delay depends on the chosen cache type (following the same order listed in the previous paragraph, these are: watch propagation delay, the configured cache TTL, or zero for direct polling). Using Secrets as environment variables To use a Secret in an environment variable in a Pod: For each container in your Pod specification, add an environment variable for each Secret key that you want to use to the env[].valueFrom.secretKeyRef field. Modify your image and/or command line so that the program looks for values in the specified environment variables. For instructions, refer to Define container environment variables using Secret data . It's important to note that the range of characters allowed for environment variable names in pods is restricted . If any keys do not meet the rules, those keys are not made available to your container, though the Pod is allowed to start. Container image pull Secrets If you want to fetch container images from a private repository, you need a way for the kubelet on each node to authenticate to that re",
      "chunk_index": 19
    },
    {
      "chunk_id": "af466c1823698099750cb74fb4c9ef6d094a36ae",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": " to your container, though the Pod is allowed to start. Container image pull Secrets If you want to fetch container images from a private repository, you need a way for the kubelet on each node to authenticate to that repository. You can configure image pull Secrets to make this possible. These Secrets are configured at the Pod level. Using imagePullSecrets The imagePullSecrets field is a list of references to Secrets in the same namespace. You can use an imagePullSecrets to pass a Secret that contains a Docker (or other) image registry password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod. See the PodSpec API for more information about the imagePullSecrets field. Manually specifying an imagePullSecret You can learn how to specify imagePullSecrets from the container images documentation. Arranging for imagePullSecrets to be automatically attached You can manually create imagePullSecrets , and reference these from a ServiceAccount. Any Pods created with that ServiceAccount or created with that ServiceAccount by default, will get their imagePullSecrets field set to that of the service account. See Add ImagePullSecrets to a service account for a detailed explanation of that process. Using Secrets with static Pods You cannot use ConfigMaps or Secrets with static Pods . Immutable Secrets FEATURE STATE: Kubernetes v1.21 [stable] Kube",
      "chunk_index": 20
    },
    {
      "chunk_id": "35248d15659d9610f60ec02adf7c243f24b06419",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "rets to a service account for a detailed explanation of that process. Using Secrets with static Pods You cannot use ConfigMaps or Secrets with static Pods . Immutable Secrets FEATURE STATE: Kubernetes v1.21 [stable] Kubernetes lets you mark specific Secrets (and ConfigMaps) as immutable . Preventing changes to the data of an existing Secret has the following benefits: protects you from accidental (or unwanted) updates that could cause applications outages (for clusters that extensively use Secrets - at least tens of thousands of unique Secret to Pod mounts), switching to immutable Secrets improves the performance of your cluster by significantly reducing load on kube-apiserver. The kubelet does not need to maintain a [watch] on any Secrets that are marked as immutable. Marking a Secret as immutable You can create an immutable Secret by setting the immutable field to true . For example, apiVersion : v1 kind : Secret metadata : ... data : ... immutable : true You can also update any existing mutable Secret to make it immutable. Note: Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data field. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate these pods. Information security for Secrets Although ConfigMap and Secret wor",
      "chunk_index": 21
    },
    {
      "chunk_id": "410310b5146c22d557da2279ffee416d4e62c50a",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "ield. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate these pods. Information security for Secrets Although ConfigMap and Secret work similarly, Kubernetes applies some additional protection for Secret objects. Secrets often hold values that span a spectrum of importance, many of which can cause escalations within Kubernetes (e.g. service account tokens) and to external systems. Even if an individual app can reason about the power of the Secrets it expects to interact with, other apps within the same namespace can render those assumptions invalid. A Secret is only sent to a node if a Pod on that node requires it. For mounting Secrets into Pods, the kubelet stores a copy of the data into a tmpfs so that the confidential data is not written to durable storage. Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy of the confidential data from the Secret. There may be several containers in a Pod. By default, containers you define only have access to the default ServiceAccount and its related Secret. You must explicitly define environment variables or map a volume into a container in order to provide access to any other Secret. There may be Secrets for several Pods on the same node. However, only the Secrets that a Pod requests are potentially visible within its ",
      "chunk_index": 22
    },
    {
      "chunk_id": "9113c936e3788af08fab235c885313a6b3ca2709",
      "url": "https://kubernetes.io/docs/concepts/configuration/secret/",
      "title": "Secrets | Kubernetes",
      "text": "s or map a volume into a container in order to provide access to any other Secret. There may be Secrets for several Pods on the same node. However, only the Secrets that a Pod requests are potentially visible within its containers. Therefore, one Pod does not have access to the Secrets of another Pod. Configure least-privilege access to Secrets To enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets. Warning: Any containers that run with privileged: true on a node can access all Secrets used on that node. What's next",
      "chunk_index": 23
    },
    {
      "chunk_id": "7b4730e4b27a27a37edb4e1492ae92f441f9d15e",
      "url": "https://kubernetes.io/docs/concepts/security/",
      "title": "Security | Kubernetes",
      "text": "Security Concepts for keeping your cloud-native workload secure. This section of the Kubernetes documentation aims to help you learn to run workloads more securely, and about the essential aspects of keeping a Kubernetes cluster secure. Kubernetes is based on a cloud-native architecture, and draws on advice from the CNCF about good practice for cloud native information security. Read Cloud Native Security and Kubernetes for the broader context about how to secure your cluster and the applications that you're running on it. Kubernetes security mechanisms Kubernetes includes several APIs and security controls, as well as ways to define policies that can form part of how you manage information security. Control plane protection A key security mechanism for any Kubernetes cluster is to control access to the Kubernetes API . Kubernetes expects you to configure and use TLS to provide data encryption in transit within the control plane, and between the control plane and its clients. You can also enable encryption at rest for the data stored within Kubernetes control plane; this is separate from using encryption at rest for your own workloads' data, which might also be a good idea. Secrets The Secret API provides basic protection for configuration values that require confidentiality. Workload protection Enforce Pod security standards to ensure that Pods and their containers are isolate",
      "chunk_index": 0
    },
    {
      "chunk_id": "fe95e1c936c5c44f7febdb3a429c0066091b74e2",
      "url": "https://kubernetes.io/docs/concepts/security/",
      "title": "Security | Kubernetes",
      "text": "a good idea. Secrets The Secret API provides basic protection for configuration values that require confidentiality. Workload protection Enforce Pod security standards to ensure that Pods and their containers are isolated appropriately. You can also use RuntimeClasses to define custom isolation if you need it. Network policies let you control network traffic between Pods, or between Pods and the network outside your cluster. You can deploy security controls from the wider ecosystem to implement preventative or detective controls around Pods, their containers, and the images that run in them. Admission control Admission controllers are plugins that intercept Kubernetes API requests and can validate or mutate the requests based on specific fields in the request. Thoughtfully designing these controllers helps to avoid unintended disruptions as Kubernetes APIs change across version updates. For design considerations, see Admission Webhook Good Practices . Auditing Kubernetes audit logging provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster. The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself. Cloud provider security Note: Items on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those third-par",
      "chunk_index": 1
    },
    {
      "chunk_id": "2b30591d8cd4f38bb278aac5fe80feb903ba0a1d",
      "url": "https://kubernetes.io/docs/concepts/security/",
      "title": "Security | Kubernetes",
      "text": "hat use the Kubernetes API, and by the control plane itself. Cloud provider security Note: Items on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. To add a vendor, product or project to this list, read the content guide before submitting a change. More information. If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. Here are links to some of the popular cloud providers' security documentation: Policies You can define security policies using Kubernetes-native mechanisms, such as NetworkPolicy (declarative control over network packet filtering) or ValidatingAdmissionPolicy (declarative restrictions on what changes someone can make using the Kubernetes API). However, you can also rely on policy implementations from the wider ecosystem around Kubernetes. Kubernetes provides extension mechanisms to let those ecosystem projects implement their own policy controls on source code review, container image approval, API access controls, networking, and more. For more information about policy mechanisms and Kubernetes, read Policies . What's next Learn about related Kubernetes security topics: Learn the context: Get certified: Read more in this section: Items on this page refer to third party produc",
      "chunk_index": 2
    },
    {
      "chunk_id": "b93f64cb3aaa20f4e68b95c1e5aa468742886c37",
      "url": "https://kubernetes.io/docs/concepts/security/",
      "title": "Security | Kubernetes",
      "text": "ut policy mechanisms and Kubernetes, read Policies . What's next Learn about related Kubernetes security topics: Learn the context: Get certified: Read more in this section: Items on this page refer to third party products or projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. See the CNCF website guidelines for more details. You should read the content guide before proposing a change that adds an extra third-party link.",
      "chunk_index": 3
    },
    {
      "chunk_id": "7cc1a4644521cf4854fc013af0116d5a412260cf",
      "url": "https://helm.sh/docs/intro/quickstart/",
      "title": "Quickstart Guide",
      "text": "This guide covers how you can quickly get started using Helm. Prerequisites ​ The following prerequisites are required for a successful and properly secured use of Helm. A Kubernetes cluster Deciding what security configurations to apply to your installation, if any Installing and configuring Helm. Install Kubernetes or have access to a cluster ​ You must have Kubernetes installed. For the latest release of Helm, we recommend the latest stable release of Kubernetes, which in most cases is the second-latest minor release. You should also have a local configured copy of kubectl . See the Helm Version Support Policy for the maximum version skew supported between Helm and Kubernetes. Install Helm ​ Download a binary release of the Helm client. You can use tools like homebrew , or look at the official releases page . For more details, or for other options, see the installation guide . Initialize a Helm Chart Repository ​ Once you have Helm ready, you can add a chart repository. Check Artifact Hub for available Helm chart repositories. $ helm repo add bitnami https://charts.bitnami.com/bitnami Once this is installed, you will be able to list the charts you can install: $ helm search repo bitnami NAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... bitnami/airflow 8.0.2 2.0.0 Apache Airflow is a platform to pr",
      "chunk_index": 0
    },
    {
      "chunk_id": "4984ac84b593275d92343c0de68d265597d2e677",
      "url": "https://helm.sh/docs/intro/quickstart/",
      "title": "Quickstart Guide",
      "text": ": $ helm search repo bitnami NAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.9 0.0.9 DEPRECATED Chart with custom templates used in ... bitnami/airflow 8.0.2 2.0.0 Apache Airflow is a platform to programmaticall... bitnami/apache 8.2.3 2.4.46 Chart for Apache HTTP Server bitnami/aspnet-core 1.2.3 3.1.9 ASP.NET Core is an open-source framework create... # ... and many more Install an Example Chart ​ To install a chart, you can run the helm install command. Helm has several ways to find and install a chart, but the easiest is to use the bitnami charts. $ helm repo update # Make sure we get the latest list of charts $ helm install bitnami/mysql --generate-name NAME: mysql-1612624192 LAST DEPLOYED: Sat Feb 6 16:09:56 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ... In the example above, the bitnami/mysql chart was released, and the name of our new release is mysql-1612624192 . You get a simple idea of the features of this MySQL chart by running helm show chart bitnami/mysql . Or you could run helm show all bitnami/mysql to get all information about the chart. Whenever you install a chart, a new release is created. So one chart can be installed multiple times into the same cluster. And each can be independently managed and upgraded. The helm install command is a very powerful command with many capabilities. To learn more about it",
      "chunk_index": 1
    },
    {
      "chunk_id": "b0753a8fb3a440b3d72c5614d4461e2ca9411ca7",
      "url": "https://helm.sh/docs/intro/quickstart/",
      "title": "Quickstart Guide",
      "text": ". So one chart can be installed multiple times into the same cluster. And each can be independently managed and upgraded. The helm install command is a very powerful command with many capabilities. To learn more about it, check out the Using Helm Guide Learn About Releases ​ It's easy to see what has been released using Helm: $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mysql-1612624192 default 1 2021-02-06 16:09:56.283059 +0100 CET deployed mysql-8.3.0 8.0.23 The helm list (or helm ls ) function will show you a list of all deployed releases. Uninstall a Release ​ To uninstall a release, use the helm uninstall command: $ helm uninstall mysql-1612624192 release \"mysql-1612624192\" uninstalled This will uninstall mysql-1612624192 from Kubernetes, which will remove all resources associated with the release as well as the release history. If the flag --keep-history is provided, release history will be kept. You will be able to request information about that release: $ helm status mysql-1612624192 Status: UNINSTALLED ... Because Helm tracks your releases even after you've uninstalled them, you can audit a cluster's history, and even undelete a release (with helm rollback ). Reading the Help Text ​ To learn more about the available Helm commands, use helm help or type a command followed by the -h flag:",
      "chunk_index": 2
    },
    {
      "chunk_id": "1c0b758066f339afe5687c93ab35a1a7f06ef173",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "Dockerfile reference Open Markdown Ask Docs AI Claude Open in Claude Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile. The Dockerfile supports the following instructions: Instruction Description ADD Add local or remote files and directories. ARG Use build-time variables. CMD Specify default commands. COPY Copy files and directories. ENTRYPOINT Specify default executable. ENV Set environment variables. EXPOSE Describe which ports your application is listening on. FROM Create a new build stage from a base image. HEALTHCHECK Check a container's health on startup. LABEL Add metadata to an image. MAINTAINER Specify the author of an image. ONBUILD Specify instructions for when the image is used in a build. RUN Execute build commands. SHELL Set the default shell of an image. STOPSIGNAL Specify the system call signal for exiting a container. USER Set user and group ID. VOLUME Create volume mounts. WORKDIR Change working directory. Here is the format of the Dockerfile: The instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily. Docker runs instructions in a Dockerfile in order. A Dockerfile must b",
      "chunk_index": 0
    },
    {
      "chunk_id": "b93a59a81484d8e4163098e610283f0c9d6d6726",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "he Dockerfile: The instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily. Docker runs instructions in a Dockerfile in order. A Dockerfile must begin with a FROM instruction . This may be after parser directives , comments , and globally scoped ARGs . The FROM instruction specifies the base image from which you are building. FROM may only be preceded by one or more ARG instructions, which declare arguments that are used in FROM lines in the Dockerfile. BuildKit treats lines that begin with # as a comment, unless the line is a valid parser directive . A # marker anywhere else in a line is treated as an argument. This allows statements like: Comment lines are removed before the Dockerfile instructions are executed. The comment in the following example is removed before the shell executes the echo command. The following examples is equivalent. Comments don't support line continuation characters. Note on whitespace For backward compatibility, leading whitespace before comments ( # ) and instructions (such as RUN ) are ignored, but discouraged. Leading whitespace is not preserved in these cases, and the following examples are therefore equivalent: Whitespace in instruction arguments, however, isn't ignored. The following example prints hello world with leading whitespace as specified: Parser directives are op",
      "chunk_index": 1
    },
    {
      "chunk_id": "522c96d6a1a932a4083a18f205e9f3058467f258",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s, and the following examples are therefore equivalent: Whitespace in instruction arguments, however, isn't ignored. The following example prints hello world with leading whitespace as specified: Parser directives are optional, and affect the way in which subsequent lines in a Dockerfile are handled. Parser directives don't add layers to the build, and don't show up as build steps. Parser directives are written as a special type of comment in the form # directive=value . A single directive may only be used once. The following parser directives are supported: Once a comment, empty line or builder instruction has been processed, BuildKit no longer looks for parser directives. Instead it treats anything formatted as a parser directive as a comment and doesn't attempt to validate if it might be a parser directive. Therefore, all parser directives must be at the top of a Dockerfile. Parser directive keys, such as syntax or check , aren't case-sensitive, but they're lowercase by convention. Values for a directive are case-sensitive and must be written in the appropriate case for the directive. For example, #check=skip=jsonargsrecommended is invalid because the check name must use Pascal case, not lowercase. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives. Due to these rules, the followi",
      "chunk_index": 2
    },
    {
      "chunk_id": "97424b7a6e8a4e008c095d15e3c5b4ac77051104",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ame must use Pascal case, not lowercase. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives. Due to these rules, the following examples are all invalid: Invalid due to line continuation: Invalid due to appearing twice: Treated as a comment because it appears after a builder instruction: Treated as a comment because it appears after a comment that isn't a parser directive: The following unknowndirective is treated as a comment because it isn't recognized. The known syntax directive is treated as a comment because it appears after a comment that isn't a parser directive. Non line-breaking whitespace is permitted in a parser directive. Hence, the following lines are all treated identically: Use the syntax parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation. Most users will want to set this parser directive to docker/dockerfile:1 , which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build. For more information about how the parser directive works, see Custom Dockerf",
      "chunk_index": 3
    },
    {
      "chunk_id": "41f9b19504026b91409f653794cf8d6257ab9e76",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s parser directive to docker/dockerfile:1 , which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build. For more information about how the parser directive works, see Custom Dockerfile syntax . Or The escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \\ . The escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the escape parser directive is included in a Dockerfile, escaping is not performed in a RUN command, except at the end of a line. Setting the escape character to ` is especially useful on Windows , where \\ is the directory path separator. ` is consistent with Windows PowerShell . Consider the following example which would fail in a non-obvious way on Windows. The second \\ at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first \\ . Similarly, the \\ at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction: Results in: One solution to the above would be to use / as the target of both the COPY ",
      "chunk_index": 4
    },
    {
      "chunk_id": "18ffcdc43f7698f542466fa6609f2823046d2af9",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction: Results in: One solution to the above would be to use / as the target of both the COPY instruction, and dir . However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support / as the path separator. By adding the escape parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows: Results in: The check directive is used to configure how build checks are evaluated. By default, all checks are run, and failures are treated as warnings. You can disable specific checks using #check=skip=<check-name> . To specify multiple checks to skip, separate them with a comma: To disable all checks, use #check=skip=all . By default, builds with failing build checks exit with a zero status code despite warnings. To make the build fail on warnings, set #check=error=true . When using the check directive, with error=true option, it is recommended to pin the Dockerfile syntax to a specific version. Otherwise, your build may start to fail when new checks are added in the future versions. To combine both the skip and error options, use a semi-colon to separate them: To see all available checks, see the build checks",
      "chunk_index": 5
    },
    {
      "chunk_id": "e8eb8190ceb84174e07ba1c34bba363e822f0e82",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": ". Otherwise, your build may start to fail when new checks are added in the future versions. To combine both the skip and error options, use a semi-colon to separate them: To see all available checks, see the build checks reference . Note that the checks available depend on the Dockerfile syntax version. To make sure you're getting the most up-to-date checks, use the syntax directive to specify the Dockerfile syntax version to the latest stable version. Environment variables (declared with the ENV statement ) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally. Environment variables are notated in the Dockerfile either with $variable_name or ${variable_name} . They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like ${foo}_bar . The ${variable_name} syntax also supports a few of the standard bash modifiers as specified below: ${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result. ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string. The following variable replacements are supported in a pre-release version of Dockerfile syntax, ",
      "chunk_index": 6
    },
    {
      "chunk_id": "c5ab6502b3dd258da9f01b7d93f323d137c41ef8",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "{variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string. The following variable replacements are supported in a pre-release version of Dockerfile syntax, when using the # syntax=docker/dockerfile-upstream:master syntax directive in your Dockerfile: ${variable#pattern} removes the shortest match of pattern from variable , seeking from the start of the string. ${variable##pattern} removes the longest match of pattern from variable , seeking from the start of the string. ${variable%pattern} removes the shortest match of pattern from variable , seeking backwards from the end of the string. ${variable%%pattern} removes the longest match of pattern from variable , seeking backwards from the end of the string. ${variable/pattern/replacement} replace the first occurrence of pattern in variable with replacement ${variable//pattern/replacement} replaces all occurrences of pattern in variable with replacement In all cases, word can be any string, including additional environment variables. pattern is a glob pattern where ? matches any single character and * any number of characters (including zero). To match literal ? and * , use a backslash escape: \\? and \\* . You can escape whole variable names by adding a \\ before the variable: \\$foo or \\${foo} , for example, will translate to $foo and ${foo} literals respectively. Examp",
      "chunk_index": 7
    },
    {
      "chunk_id": "9a76d664d2e5e4e8c7884f0ef6c3710ec6eb2db8",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " literal ? and * , use a backslash escape: \\? and \\* . You can escape whole variable names by adding a \\ before the variable: \\$foo or \\${foo} , for example, will translate to $foo and ${foo} literals respectively. Example (parsed representation is displayed after the # ): Environment variables are supported by the following list of instructions in the Dockerfile: ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD (when combined with one of the supported instructions above) You can also use environment variables with RUN , CMD , and ENTRYPOINT instructions, but in those cases the variable substitution is handled by the command shell, not the builder. Note that instructions using the exec form don't invoke a command shell automatically. See Variable substitution . Environment variable substitution use the same value for each variable throughout the entire instruction. Changing the value of a variable only takes effect in subsequent instructions. Consider the following example: The value of def becomes hello The value of ghi becomes bye You can use .dockerignore file to exclude files and directories from the build context. For more information, see .dockerignore file . The RUN , CMD , and ENTRYPOINT instructions all have two possible forms: INSTRUCTION [\"executable\",\"param1\",\"param2\"] (exec form) INSTRUCTION command param1 param2 (shell form) The exec form make",
      "chunk_index": 8
    },
    {
      "chunk_id": "c6f4edcec74a004ee24aed8bfcd7ee58ef6e5a1a",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "see .dockerignore file . The RUN , CMD , and ENTRYPOINT instructions all have two possible forms: INSTRUCTION [\"executable\",\"param1\",\"param2\"] (exec form) INSTRUCTION command param1 param2 (shell form) The exec form makes it possible to avoid shell string munging, and to invoke commands using a specific command shell, or any other executable. It uses a JSON array syntax, where each element in the array is a command, flag, or argument. The shell form is more relaxed, and emphasizes ease of use, flexibility, and readability. The shell form automatically uses a command shell, whereas the exec form does not. The exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes ('). The exec form is best used to specify an ENTRYPOINT instruction, combined with CMD for setting default arguments that can be overridden at runtime. For more information, see ENTRYPOINT . Using the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, RUN [ \"echo\", \"$HOME\" ] won't handle variable substitution for $HOME . If you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ] . When using the exec form and executing a shell directly, as in the case for the s",
      "chunk_index": 9
    },
    {
      "chunk_id": "34a9b730351e971a53c48ef9eeaea640209fe2df",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ] . When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder. In exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way: The correct syntax for this example is: Unlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the escape character (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines: They're equivalent to the following line: You can also use heredocs with the shell form to break up supported commands. For more information about heredocs, see Here-documents . You can change the default shell using the SHELL command. For example: For more information, see SHELL . Or Or The FROM instruction initializes a new build stage",
      "chunk_index": 10
    },
    {
      "chunk_id": "9a51e858d960eb1345e6274ada792fc7d158c378",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " For more information about heredocs, see Here-documents . You can change the default shell using the SHELL command. For example: For more information, see SHELL . Or Or The FROM instruction initializes a new build stage and sets the base image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction. The image can be any valid image. ARG is the only instruction that may precede FROM in the Dockerfile. See Understand how ARG and FROM interact . FROM can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new FROM instruction. Each FROM instruction clears any state created by previous instructions. Optionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM <name> , COPY --from=<name> , and RUN --mount=type=bind,from=<name> instructions to refer to the image built in this stage. The tag or digest values are optional. If you omit either of them, the builder assumes a latest tag by default. The builder returns an error if it can't find the tag value. The optional --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image. For example, linux/amd64 , linux/arm64 , or windows/am",
      "chunk_index": 11
    },
    {
      "chunk_id": "3fc187cc834ff07d3d464242da1007dcf4a95448",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "n error if it can't find the tag value. The optional --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image. For example, linux/amd64 , linux/arm64 , or windows/amd64 . By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example automatic platform ARGs allow you to force a stage to native build platform ( --platform=$BUILDPLATFORM ), and use it to cross-compile to the target platform inside the stage. FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM . An ARG declared before a FROM is outside of a build stage, so it can't be used in any instruction after a FROM . To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage: The RUN instruction will execute any commands to create a new layer on top of the current image. The added layer is used in the next step in the Dockerfile. RUN has two forms: For more information about the differences between these two forms, see shell or exec forms . The shell form is most commonly used, and lets you break up longer instructions into multiple lines, either using newline escapes , or with heredocs : The available [OPTIONS] for the RUN instruction are: The cache for RUN instructions i",
      "chunk_index": 12
    },
    {
      "chunk_id": "ab3aada6f70b92e9fe58dc8d82211bb6b238e6cf",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s most commonly used, and lets you break up longer instructions into multiple lines, either using newline escapes , or with heredocs : The available [OPTIONS] for the RUN instruction are: The cache for RUN instructions isn't invalidated automatically during the next build. The cache for an instruction like RUN apt-get dist-upgrade -y will be reused during the next build. The cache for RUN instructions can be invalidated by using the --no-cache flag, for example docker build --no-cache . See the Dockerfile Best Practices guide for more information. The cache for RUN instructions can be invalidated by ADD and COPY instructions. Not yet available in stable syntax, use docker/dockerfile:1-labs version. It also needs BuildKit 0.20.0 or later. RUN --device allows build to request CDI devices to be available to the build step. The use of --device is protected by the device entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement device flag or in buildkitd config , and for a build request with --allow device flag . The device name is provided by the CDI specification registered in BuildKit. In the following example, multiple devices are registered in the CDI specification for the vendor1.com/device vendor. The device name format is flexible and accepts various patterns to support multiple device configurations: vendor1.com/device : re",
      "chunk_index": 13
    },
    {
      "chunk_id": "61d39de89308a27579ec9eb8426d260d7366bcf3",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ltiple devices are registered in the CDI specification for the vendor1.com/device vendor. The device name format is flexible and accepts various patterns to support multiple device configurations: vendor1.com/device : request the first device found for this vendor vendor1.com/device=foo : request a specific device vendor1.com/device=* : request all devices for this vendor class1 : request devices by org.mobyproject.buildkit.device.class annotation Annotations are supported by the CDI specification since 0.6.0. To automatically allow all devices registered in the CDI specification, you can set the org.mobyproject.buildkit.device.autoallow annotation. You can also set this annotation for a specific device. In this example we use the --device flag to run llama.cpp inference using an NVIDIA GPU device through CDI: RUN --mount allows you to create filesystem mounts that the build can access. This can be used to: Create bind mount to the host filesystem or other build stages Access build secrets or ssh-agent sockets Use a persistent package management cache to speed up your build The supported mount types are: Type Description bind (default) Bind-mount context directories (read-only). cache Mount a temporary directory to cache directories for compilers and package managers. tmpfs Mount a tmpfs in the build container. secret Allow the build container to access secure files such as pri",
      "chunk_index": 14
    },
    {
      "chunk_id": "b46e5f711070ba7de77443f22085602bdce4d405",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ries (read-only). cache Mount a temporary directory to cache directories for compilers and package managers. tmpfs Mount a tmpfs in the build container. secret Allow the build container to access secure files such as private keys without baking them into the image or build cache. ssh Allow the build container to access SSH keys via SSH agents, with support for passphrases. This mount type allows binding files or directories to the build container. A bind mount is read-only by default. Option Description target , dst , destination Mount path. source Source path in the from . Defaults to the root of the from . from Build stage, context, or image name for the root of the source. Defaults to the build context. rw , readwrite Allow writes on the mount. Written data will be discarded. This mount type allows the build container to cache directories for compilers and package managers. Option Description id Optional ID to identify separate/different caches. Defaults to value of target . target , dst , destination Mount path. ro , readonly Read-only if set. sharing One of shared , private , or locked . Defaults to shared . A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount. from Build stage, context, or image name to use as a base of the cache ",
      "chunk_index": 15
    },
    {
      "chunk_id": "2b0dffdc5ecf6bba85955d4ed207b3a59d0dc0b9",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount. from Build stage, context, or image name to use as a base of the cache mount. Defaults to empty directory. source Subpath in the from to mount. Defaults to the root of the from . mode File mode for new cache directory in octal. Default 0755 . uid User ID for new cache directory. Default 0 . gid Group ID for new cache directory. Default 0 . Contents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed. Apt needs exclusive access to its data, so the caches use the option sharing=locked , which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use sharing=private if you prefer to have each build create another cache directory in this case. This mount type allows mounting tmpfs in the build container. Option Description target , dst , destination Mount path. size Specify an upper limit on the size of the filesystem. This mount type allows the build container to acce",
      "chunk_index": 16
    },
    {
      "chunk_id": "e0832dbb27120a4ca713be268477f57f5f6d46d8",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "t type allows mounting tmpfs in the build container. Option Description target , dst , destination Mount path. size Specify an upper limit on the size of the filesystem. This mount type allows the build container to access secret values, such as tokens or private keys, without baking them into the image. By default, the secret is mounted as a file. You can also mount the secret as an environment variable by setting the env option. Option Description id ID of the secret. Defaults to basename of the target path. target , dst , destination Mount the secret to the specified path. Defaults to /run/secrets/ + id if unset and if env is also unset. env Mount the secret to an environment variable instead of a file, or both. (since Dockerfile v1.10.0) required If set to true , the instruction errors out when the secret is unavailable. Defaults to false . mode File mode for secret file in octal. Default 0400 . uid User ID for secret file. Default 0 . gid Group ID for secret file. Default 0 . The following example takes the secret API_KEY and mounts it as an environment variable with the same name. Assuming that the API_KEY environment variable is set in the build environment, you can build this with the following command: This mount type allows the build container to access SSH keys via SSH agents, with support for passphrases. Option Description id ID of SSH agent socket or key. Defaults",
      "chunk_index": 17
    },
    {
      "chunk_id": "467bd8532dcb42836f85d04e020636bfa59f6c97",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "nt, you can build this with the following command: This mount type allows the build container to access SSH keys via SSH agents, with support for passphrases. Option Description id ID of SSH agent socket or key. Defaults to \"default\". target , dst , destination SSH agent socket path. Defaults to /run/buildkit/ssh_agent.${N} . required If set to true , the instruction errors out when the key is unavailable. Defaults to false . mode File mode for socket in octal. Default 0600 . uid User ID for socket. Default 0 . gid Group ID for socket. Default 0 . You can also specify a path to *.pem file on the host directly instead of $SSH_AUTH_SOCK . However, pem files with passphrases are not supported. RUN --network allows control over which networking environment the command is run in. The supported network types are: Type Description default (default) Run in the default network. none Run with no network access. host Run in the host's network environment. Equivalent to not supplying a flag at all, the command is run in the default network for the build. The command is run with no network access ( lo is still available, but is isolated to this process) pip will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage. The command is run in the host's network environment (similar to docker build --network=host , but on a per-instruction",
      "chunk_index": 18
    },
    {
      "chunk_id": "fccaaaf942cbc487aa28990becf23fd89090ea60",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " to install the packages provided in the tarfile, which can be controlled by an earlier build stage. The command is run in the host's network environment (similar to docker build --network=host , but on a per-instruction basis) The use of --network=host is protected by the network.host entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement network.host flag or in buildkitd config , and for a build request with --allow network.host flag . The default security mode is sandbox . With --security=insecure , the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running docker run --privileged . In order to access this feature, entitlement security.insecure should be enabled when starting the buildkitd daemon with --allow-insecure-entitlement security.insecure flag or in buildkitd config , and for a build request with --allow security.insecure flag . Default sandbox mode can be activated via --security=sandbox , but that is no-op. The CMD instruction sets the command to be executed when running a container from an image. You can specify CMD instructions using shell or exec forms : CMD [\"executable\",\"param1\",\"param2\"] (exec form) CMD [\"param1\",\"param2\"] (exec form, as default parameters to ENTRYPOINT ) CMD command param1 param2 (",
      "chunk_index": 19
    },
    {
      "chunk_id": "71f6b580b1937c09765ff8aa69be6e38146f7f3b",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "n image. You can specify CMD instructions using shell or exec forms : CMD [\"executable\",\"param1\",\"param2\"] (exec form) CMD [\"param1\",\"param2\"] (exec form, as default parameters to ENTRYPOINT ) CMD command param1 param2 (shell form) There can only be one CMD instruction in a Dockerfile. If you list more than one CMD , only the last one takes effect. The purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well. If you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD . See ENTRYPOINT . If the user specifies arguments to docker run then they will override the default specified in CMD , but still use the default ENTRYPOINT . If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified in the exec form . Don't confuse RUN with CMD . RUN actually runs a command and commits the result; CMD doesn't execute anything at build time, but specifies the intended command for the image. The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. A few usage examples: An ",
      "chunk_index": 20
    },
    {
      "chunk_id": "267a36591cc15bfff0f08145189ee9957add5829",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " the image. The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. A few usage examples: An image can have more than one label. You can specify multiple labels on a single line. Prior to Docker 1.10, this decreased the size of the final image, but this is no longer the case. You may still choose to specify multiple labels in a single instruction, in one of the following two ways: Be sure to use double quotes and not single quotes. Particularly when you are using string interpolation (e.g. LABEL example=\"foo-$ENV_VAR\" ), single quotes will take the string as is without unpacking the variable's value. Labels included in base images (images in the FROM line) are inherited by your image. If a label already exists but with a different value, the most-recently-applied value overrides any previously-set value. To view an image's labels, use the docker image inspect command. You can use the --format option to show just the labels; The MAINTAINER instruction sets the Author field of the generated images. The LABEL instruction is a much more flexible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with docker inspect . To set a label corresponding to the MAINTAINER field you cou",
      "chunk_index": 21
    },
    {
      "chunk_id": "135729dbd6f13cc187820daffb3c6a00090d64ae",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with docker inspect . To set a label corresponding to the MAINTAINER field you could use: This will then be visible from docker inspect with the other labels. The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if you don't specify a protocol. The EXPOSE instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports. By default, EXPOSE assumes TCP. You can also specify UDP: To expose on both TCP and UDP, include two lines: In this case, if you use -P with docker run , the port will be exposed once for TCP and once for UDP. Remember that -P uses an ephemeral high-ordered host port on the host, so TCP and UDP doesn't use the same port. Regardless of the EXPOSE settings, you can override them at runtime by using the -p flag. For example To set up port redirection on the host ",
      "chunk_index": 22
    },
    {
      "chunk_id": "3adb2620f512172cbef275ca0a44911b20011e6d",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " high-ordered host port on the host, so TCP and UDP doesn't use the same port. Regardless of the EXPOSE settings, you can override them at runtime by using the -p flag. For example To set up port redirection on the host system, see using the -P flag . The docker network command supports creating networks for communication among containers without the need to expose or publish specific ports, because the containers connected to the network can communicate with each other over any port. For detailed information, see the overview of this feature . The ENV instruction sets the environment variable <key> to the value <value> . This value will be in the environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value will be interpreted for other environment variables, so quote characters will be removed if they are not escaped. Like command line parsing, quotes and backslashes can be used to include spaces within values. Example: The ENV instruction allows for multiple <key>=<value> ... variables to be set at one time, and the example below will yield the same net results in the final image: The environment variables set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect , and change them using docker run --env <key>=<value> . A stage inherits any environment variab",
      "chunk_index": 23
    },
    {
      "chunk_id": "04584ea577dace80c3c372751e884e390d424143",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect , and change them using docker run --env <key>=<value> . A stage inherits any environment variables that were set using ENV by its parent stage or any ancestor. Refer to the multi-stage builds section in the manual for more information. Environment variable persistence can cause unexpected side effects. For example, setting ENV DEBIAN_FRONTEND=noninteractive changes the behavior of apt-get , and may confuse users of your image. If an environment variable is only needed during build, and not in the final image, consider setting a value for a single command instead: Or using ARG , which is not persisted in the final image: Alternative syntax The ENV instruction also allows an alternative syntax ENV <key> <value> , omitting the = . For example: This syntax does not allow for multiple environment-variables to be set in a single ENV instruction, and can be confusing. For example, the following sets a single environment variable ( ONE ) with value \"TWO= THREE=world\" : The alternative syntax is supported for backward compatibility, but discouraged for the reasons outlined above, and may be removed in a future release. ADD has two forms. The latter form is required for paths containing whitespace. The available [OPTIONS] are: The ADD instruction copies new files o",
      "chunk_index": 24
    },
    {
      "chunk_id": "60caa3984da8d438c87fb0d50d6000a639bdb3e4",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " for the reasons outlined above, and may be removed in a future release. ADD has two forms. The latter form is required for paths containing whitespace. The available [OPTIONS] are: The ADD instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest> . Files and directories can be copied from the build context, a remote URL, or a Git repository. The ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY . You can specify multiple source files or directories with ADD . The last argument must always be the destination. For example, to add two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container: If you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ). To add files from a remote location, you can specify a URL or the address of a Git repository as the source. For example: BuildKit detects the type of <src> and processes it accordingly. Any relative or local path that doesn't begin with a http:// , https:// , or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file ",
      "chunk_index": 25
    },
    {
      "chunk_id": "c6272868f3a11db8185344a90c7b8523a66cfdef",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ttp:// , https:// , or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file at ./file.txt to the root of the filesystem in the build container. Specifying a source path with a leading slash or one that navigates outside the build context, such as ADD ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making ADD something/ /something equivalent to ADD something /something . If the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised. If the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised. If you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build",
      "chunk_index": 26
    },
    {
      "chunk_id": "a3cd567fa82629f36c0afacd68202305e24901f6",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised. If you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the ADD instruction to copy remote files. You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build. For local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules. For example, to add all files and directories in the root of the build context ending with .png : In the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts . When adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following; When using a local tar archive as the source for ADD , and the archive is in a recognized compression format ( gzip , bzip2 or xz , or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ ADD --unpack flag]. When a directory is extracted, it ",
      "chunk_index": 27
    },
    {
      "chunk_id": "9ef643c2275c8149264fb18a180400eb727f1047",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ip , bzip2 or xz , or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ ADD --unpack flag]. When a directory is extracted, it has the same behavior as tar -x . The result is the union of: Whatever existed at the destination path, and The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis. Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination. In the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a Last-Modified header, the timestamp from that header will be used to set the mtime on the destination file. However, like any other file processed during an ADD , mtime isn't included in the determination of whether or not the file has changed and the cache should be updated. If remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ ADD --unpack flag]. If the destination ends with a trailing sl",
      "chunk_index": 28
    },
    {
      "chunk_id": "bbbb222e0c361c2573128a3401c8215c72030e3b",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ged and the cache should be updated. If remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ ADD --unpack flag]. If the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, ADD http://example.com/foobar / would create the file /foobar . The URL must have a nontrivial path so that an appropriate filename can be discovered ( http://example.com doesn't work). If the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, ADD http://example.com/foo /bar creates the file /bar . If your URL files are protected using authentication, you need to use RUN wget , RUN curl or use another tool from within the container as the ADD instruction doesn't support authentication. To use a Git repository as the source for ADD , you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image. You can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the docs directory of the v0.14.1 tag of the buildkit repository: For more information about Git URL fragments, see URL fragments . When adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the",
      "chunk_index": 29
    },
    {
      "chunk_id": "9bea54f4fbd2da92d16f49afdef82fa2ab2da606",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "the v0.14.1 tag of the buildkit repository: For more information about Git URL fragments, see URL fragments . When adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755. When using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile: To build this Dockerfile, pass the --ssh flag to the docker build to mount the SSH agent socket to the build. For example: For more information about building with secrets, see Build secrets . If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage. Trailing slashes are significant. For example, ADD test.txt /abs creates a file at /abs , whereas ADD test.txt /abs/ creates /abs/test.txt . If the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container. If destination doesn't exist, it's created, along with all missing directories in its path. If the source is a file, and the destination doesn't end with a tra",
      "chunk_index": 30
    },
    {
      "chunk_id": "c5b04569433a182e1b7072cc368618b06f7c6f83",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "as relative to the working directory of the build container. If destination doesn't exist, it's created, along with all missing directories in its path. If the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file. When <src> is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the .git directory by default. The --keep-git-dir=true flag lets you preserve the .git directory. The --checksum flag lets you verify the checksum of a remote resource. The checksum is formatted as sha256:<hash> . SHA-256 is the only supported hash algorithm. The --checksum flag only supports HTTP(S) sources. See COPY --chown --chmod . See COPY --link . See COPY --exclude . The --unpack flag controls whether or not to automatically unpack tar archives (including compressed formats like gzip or bzip2 ) when adding them to the image. Local tar archives are unpacked by default, whereas remote tar archives (where src is a URL) are downloaded without unpacking. COPY has two forms. The latter form is required for paths containing whitespace. The available [OPTIONS] are: The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest> . Files and directories can be copied from the build contex",
      "chunk_index": 31
    },
    {
      "chunk_id": "54cba01716e6a6d148771ee9cdd5160a625ff20c",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ce. The available [OPTIONS] are: The COPY instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest> . Files and directories can be copied from the build context, build stage, named context, or an image. The ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY . You can specify multiple source files or directories with COPY . The last argument must always be the destination. For example, to copy two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container: If you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ). COPY accepts a flag --from=<name> that lets you specify the source location to be a build stage, context, or image. The following example copies files from a stage named build : For more information about copying from named sources, see the --from flag . When copying source files from the build context, paths are interpreted as relative to the root of the context. Specifying a source path with a leading slash or one that navigates outside the build context, such as COPY ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path a",
      "chunk_index": 32
    },
    {
      "chunk_id": "c617ab97163c756b5facdec46099f5ac0b40f447",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ource path with a leading slash or one that navigates outside the build context, such as COPY ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making COPY something/ /something equivalent to COPY something /something . If the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised. If the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised. If you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the COPY instruction to copy files from other stages, named contexts, or images, using the --from flag . You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive wi",
      "chunk_index": 33
    },
    {
      "chunk_id": "86d2535d1db1581a033a7630df2671f1289c15c9",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "m other stages, named contexts, or images, using the --from flag . You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build. When using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755. For local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules. For example, to add all files and directories in the root of the build context ending with .png : In the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts . When adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following; If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage. Trailing slashes are significant. For example, COPY test.txt /abs creates a file at /abs , whereas COPY test.txt /abs/ creates /abs/tes",
      "chunk_index": 34
    },
    {
      "chunk_id": "6f7eca1ca73bb87dc8e012b1dbb01a8fffe3c909",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "pied into the specified destination relative to the root of the current build stage. Trailing slashes are significant. For example, COPY test.txt /abs creates a file at /abs , whereas COPY test.txt /abs/ creates /abs/test.txt . If the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container. If destination doesn't exist, it's created, along with all missing directories in its path. If the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file. By default, the COPY instruction copies files from the build context. The COPY --from flag lets you copy files from an image, a build stage, or a named context instead. To copy from a build stage in a multi-stage build , specify the name of the stage you want to copy from. You specify stage names using the AS keyword with the FROM instruction. You can also copy files directly from named contexts (specified with --build-context <name>=<source> ) or images. The following example copies an nginx.conf file from the official Nginx image. The source path of COPY --from is always resolved from filesystem root of the image or stage that you specify. Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951 . The --chown and --chmod features are only suppor",
      "chunk_index": 35
    },
    {
      "chunk_id": "4983ba9c6cce89a07d4abffb9f4ddba21f64cd61",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ways resolved from filesystem root of the image or stage that you specify. Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951 . The --chown and --chmod features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of /etc/passwd and /etc/group for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers. All files and directories copied from the build context are created with a UID and GID of 0 unless the optional --chown flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the --chown flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem /etc/passwd and /etc/group files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the --chown flag: If the container root filesystem doesn't contain either /etc/passwd or /etc/group files and either user or group names a",
      "chunk_index": 36
    },
    {
      "chunk_id": "12e00cc253c9f8ec3c653eec6a5b7d97bc2b51ba",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "nteger UID or GID respectively. The following examples show valid definitions for the --chown flag: If the container root filesystem doesn't contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the COPY operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content. With the Dockerfile syntax version 1.10.0 and later, the --chmod flag supports variable interpolation, which lets you define the permission bits using build arguments: Enabling this flag in COPY or ADD commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed. When --link is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state. Is equivalent of doing two builds: and and merging all the layers of both images together. Use --link to reuse already built layers in subsequent builds with --cache-from even if the previous layers have changed. This is especially important for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the l",
      "chunk_index": 37
    },
    {
      "chunk_id": "625f9589a3ea39a3f932411f108317081b71f321",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "rtant for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order. The same behavior where BuildKit can avoid pulling down the base image can also happen when using --link and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the COPY commands and push them to the registry directly on top of the layers of the base image. When using --link the COPY/ADD commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories. If you don't rel",
      "chunk_index": 38
    },
    {
      "chunk_id": "e742e396bf9bfde1f782df3de8fc36635d6df053",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "e the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories. If you don't rely on the behavior of following symlinks in the destination path, using --link is always recommended. The performance of --link is equivalent or better than the default behavior and, it creates much better conditions for cache reuse. The --parents flag preserves parent directories for src entries. This flag defaults to false . This behavior is similar to the Linux cp utility's --parents or rsync --relative flag. As with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash ( ./ ) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with --from where the source paths need to be absolute. Note that, without the --parents flag specified, any filename collision will fail the Linux cp operation with an explicit error message ( cp: will not overwrite just-created './x/a.txt' with './y/a.txt' ), where the Buildkit will silently overwrite the target file at the destination. While it is possible to preserve the directory structure for COPY instructions consisting of only one src entry, usually it is more beneficial to keep the layer",
      "chunk_index": 39
    },
    {
      "chunk_id": "b4caf217043d27fbcffc5a3a525725f473722ab3",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "will silently overwrite the target file at the destination. While it is possible to preserve the directory structure for COPY instructions consisting of only one src entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the --parents flag, the Buildkit is capable of packing multiple COPY instructions together, keeping the directory structure intact. The --exclude flag lets you specify a path expression for files to be excluded. The path expression follows the same format as <src> , supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a .txt extension: You can specify the --exclude option multiple times for a COPY instruction. Multiple --excludes are files matching its patterns not to be copied, even if the files paths match the pattern specified in <src> . To add all files starting with \"hom\", excluding files with either .txt or .md extensions: An ENTRYPOINT allows you to configure a container that will run as an executable. ENTRYPOINT has two possible forms: For more information about the different forms, see Shell and exec form . The following command starts a container from the nginx with its default content, listening on port 80: Command line arguments to docker run <image> will be appended after all elements in an exec form",
      "chunk_index": 40
    },
    {
      "chunk_id": "f1c6f495cd4916b10caf31e4fc945640c0e48bfc",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ell and exec form . The following command starts a container from the nginx with its default content, listening on port 80: Command line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT , and will override all elements specified using CMD . This allows arguments to be passed to the entry point, i.e., docker run <image> -d will pass the -d argument to the entry point. You can override the ENTRYPOINT instruction using the docker run --entrypoint flag. The shell form of ENTRYPOINT prevents any CMD command line arguments from being used. It also starts your ENTRYPOINT as a subcommand of /bin/sh -c , which does not pass signals. This means that the executable will not be the container's PID 1 , and will not receive Unix signals. In this case, your executable doesn't receive a SIGTERM from docker stop <container> . Only the last ENTRYPOINT instruction in the Dockerfile will have an effect. You can use the exec form of ENTRYPOINT to set fairly stable default commands and arguments and then use either form of CMD to set additional defaults that are more likely to be changed. When you run the container, you can see that top is the only process: To examine the result further, you can use docker exec : And you can gracefully request top to shut down using docker stop test . The following Dockerfile shows using the ENTRYPOINT to run Apache in t",
      "chunk_index": 41
    },
    {
      "chunk_id": "5c924f26e8aaba0e7988638f9f9bb18dbefd87a3",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s the only process: To examine the result further, you can use docker exec : And you can gracefully request top to shut down using docker stop test . The following Dockerfile shows using the ENTRYPOINT to run Apache in the foreground (i.e., as PID 1 ): If you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using exec and gosu commands: Lastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the ENTRYPOINT script receives the Unix signals, passes them on, and then does some more work: If you run this image with docker run -it --rm -p 80:80 --name test apache , you can then examine the container's processes with docker exec , or docker top , and then ask the script to stop Apache: You can override the ENTRYPOINT setting using --entrypoint , but this can only set the binary to exec (no sh -c will be used). You can specify a plain string for the ENTRYPOINT and it will execute in /bin/sh -c . This form will use shell processing to substitute shell environment variables, and will ignore any CMD or docker run command line arguments. To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec : When you run this image, you'l",
      "chunk_index": 42
    },
    {
      "chunk_id": "eddb77a82e5fee1449a3a57b57cce24ab315c67c",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ll ignore any CMD or docker run command line arguments. To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec : When you run this image, you'll see the single PID 1 process: Which exits cleanly on docker stop : If you forget to add exec to the beginning of your ENTRYPOINT : You can then run it (giving it a name for the next step): You can see from the output of top that the specified ENTRYPOINT is not PID 1 . If you then run docker stop test , the container will not exit cleanly - the stop command will be forced to send a SIGKILL after the timeout: Both CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation. Dockerfile should specify at least one of CMD or ENTRYPOINT commands. ENTRYPOINT should be defined when using the container as an executable. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container. CMD will be overridden when running the container with alternative arguments. The table below shows what command is executed for different ENTRYPOINT / CMD combinations: No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [\"exec_entry\", \"p1_entry\"] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [\"exe",
      "chunk_index": 43
    },
    {
      "chunk_id": "0c946767d11d3fb2b16c2a9eeb0cc0cb60daac57",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "cuted for different ENTRYPOINT / CMD combinations: No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [\"exec_entry\", \"p1_entry\"] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [\"exec_cmd\", \"p1_cmd\"] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value. The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, VOLUME [\"/var/log/\"] , or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db . For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation. The docker run command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet: This Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume.",
      "chunk_index": 44
    },
    {
      "chunk_id": "c1aa4bcc182751613fe6771f186942dd9075390b",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "se image. For example, consider the following Dockerfile snippet: This Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume. Keep the following things in mind about volumes in the Dockerfile. Volumes on Windows-based containers : When using Windows-based containers, the destination of a volume inside the container must be one of: a non-existing or empty directory a drive other than C: Changing the volume from within the Dockerfile : If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept. JSON formatting : The list is parsed as a JSON array. You must enclose words with double quotes ( \" ) rather than single quotes ( ' ). The host directory is declared at container run-time : The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container. or The USER instruction sets the user name (or UID) and optionally t",
      "chunk_index": 45
    },
    {
      "chunk_id": "06a1d4d2d0163d42448f68e70dd278290c380657",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "kerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container. or The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use as the default user and group for the remainder of the current stage. The specified user is used for RUN instructions and at runtime, runs the relevant ENTRYPOINT and CMD commands. Note that when specifying a group for the user, the user will have only the specified group membership. Any other configured group memberships will be ignored. When the user doesn't have a primary group then the image (or the next instructions) will be run with the root group. On Windows, the user must be created first if it's not a built-in account. This can be done with the net user command called as part of a Dockerfile. The WORKDIR instruction sets the working directory for any RUN , CMD , ENTRYPOINT , COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn't exist, it will be created even if it's not used in any subsequent Dockerfile instruction. The WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example: The output of the final pwd command in this Dockerfile would be /a/b/c . The WORKDIR instruction can r",
      "chunk_index": 46
    },
    {
      "chunk_id": "5c7ffb624692d56df63a89a86bd3f7fcac7b4ae9",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "le. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example: The output of the final pwd command in this Dockerfile would be /a/b/c . The WORKDIR instruction can resolve environment variables previously set using ENV . You can only use environment variables explicitly set in the Dockerfile. For example: The output of the final pwd command in this Dockerfile would be /path/$DIRNAME If not specified, the default working directory is / . In practice, if you aren't building a Dockerfile from scratch ( FROM scratch ), the WORKDIR may likely be set by the base image you're using. Therefore, to avoid unintended operations in unknown directories, it's best practice to set your WORKDIR explicitly. The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag. It isn't recommended to use build arguments for passing secrets such as user credentials, API tokens, etc. Build arguments are visible in the docker history command and in max mode provenance attestations, which are attached to the image by default if you use the Buildx GitHub Actions and your GitHub repository is public. Refer to the RUN --mount=type=secret section to learn about secure ways to use secrets when building images. A Dockerfile may include one or more ARG instruc",
      "chunk_index": 47
    },
    {
      "chunk_id": "e6cb02fcc77794e398052f7a224f70085bff4ae7",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "he Buildx GitHub Actions and your GitHub repository is public. Refer to the RUN --mount=type=secret section to learn about secure ways to use secrets when building images. A Dockerfile may include one or more ARG instructions. For example, the following is a valid Dockerfile: An ARG instruction can optionally include a default value: If an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default. An ARG variable comes into effect from the line on which it is declared in the Dockerfile. For example, consider this Dockerfile: A user builds this file by calling: The USER instruction on line 2 evaluates to the some_user fallback, because the username variable is not yet declared. The username variable is declared on line 3, and available for reference in Dockerfile instruction from that point onwards. The USER instruction on line 4 evaluates to what_user , since at that point the username argument has a value of what_user which was passed on the command line. Prior to its definition by an ARG instruction, any use of a variable results in an empty string. An ARG variable declared within a build stage is automatically inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the ARG instruction, or they must bot",
      "chunk_index": 48
    },
    {
      "chunk_id": "a1ebdbe4e18cc431293e79ed62b3b720c1742af9",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ally inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the ARG instruction, or they must both be based on a shared base stage in the same Dockerfile where the variable is declared. For more information, refer to variable scoping . You can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction. Then, assume this image is built with this command: In this case, the RUN instruction uses v1.0.0 instead of the ARG setting passed by the user: v2.0.1 This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition. Using the example above but a different ENV specification you can create more useful interactions between ARG and ENV instructions: Unlike an ARG instruction, ENV values are always persisted in the built image. Consider a docker build without the --build-arg flag: Using this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction. The var",
      "chunk_index": 49
    },
    {
      "chunk_id": "355007edbe4114f423e48fcbb52e4a9b1e59d257",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "er a docker build without the --build-arg flag: Using this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction. The variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. Variable expansion is only supported for a limited set of Dockerfile instructions. Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile. HTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy ALL_PROXY all_proxy To use these, pass them on the command line using the --build-arg flag, for example: By default, these pre-defined variables are excluded from the output of docker history . Excluding them reduces the risk of accidentally leaking sensitive authentication information in an HTTP_PROXY variable. For example, consider building the following Dockerfile using --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com In this case, the value of the HTTP_PROXY variable is not available in the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com , a subsequent build does not result in a cache miss. If you need to override",
      "chunk_index": 50
    },
    {
      "chunk_id": "60b198b72657297b98faf33b73559edb1f86348f",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com , a subsequent build does not result in a cache miss. If you need to override this behaviour then you may do so by adding an ARG statement in the Dockerfile as follows: When building this Dockerfile, the HTTP_PROXY is preserved in the docker history , and changing its value invalidates the build cache. This feature is only available when using the BuildKit backend. BuildKit supports a predefined set of ARG variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the --platform flag on docker build . The following ARG variables are set automatically: TARGETPLATFORM - platform of the build result. Eg linux/amd64 , linux/arm/v7 , windows/amd64 . TARGETOS - OS component of TARGETPLATFORM TARGETARCH - architecture component of TARGETPLATFORM TARGETVARIANT - variant component of TARGETPLATFORM BUILDPLATFORM - platform of the node performing the build. BUILDOS - OS component of BUILDPLATFORM BUILDARCH - architecture component of BUILDPLATFORM BUILDVARIANT - variant component of BUILDPLATFORM These arguments are defined in the global scope so are not automatically available inside build stages or for your RU",
      "chunk_index": 51
    },
    {
      "chunk_id": "6bee78ceb71077c48265551da6eebfcb6bd265ad",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": " BUILDARCH - architecture component of BUILDPLATFORM BUILDVARIANT - variant component of BUILDPLATFORM These arguments are defined in the global scope so are not automatically available inside build stages or for your RUN commands. To expose one of these arguments inside the build stage redefine it without value. For example: Arg Type Description BUILDKIT_BUILD_NAME String Override the build name shown in buildx history command and Docker Desktop Builds view . BUILDKIT_CACHE_MOUNT_NS String Set optional cache ID namespace. BUILDKIT_CONTEXT_KEEP_GIT_DIR Bool Trigger Git context to keep the .git directory. BUILDKIT_HISTORY_PROVENANCE_V1 Bool Enable SLSA Provenance v1 for build history record. BUILDKIT_INLINE_CACHE Bool Inline cache metadata to image config or not. BUILDKIT_MULTI_PLATFORM Bool Opt into deterministic output regardless of multi-platform output or not. BUILDKIT_SANDBOX_HOSTNAME String Set the hostname (default buildkitsandbox ) BUILDKIT_SYNTAX String Set frontend image SOURCE_DATE_EPOCH Int Set the Unix timestamp for created image and layers. More info from reproducible builds . Supported since Dockerfile 1.5, BuildKit 0.11 When using a Git context, .git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build: ARG variables are not persisted into the built image as ENV variables are. However, ARG vari",
      "chunk_index": 52
    },
    {
      "chunk_id": "986a90d3bfb6acce605777003729dd9259f9d8f5",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build: ARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile. For example, consider these two Dockerfile: If you specify --build-arg CONT_IMG_VER=<value> on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. ARG CONT_IMG_VER causes the RUN line to be identified as the same as running CONT_IMG_VER=<value> echo hello , so if the <value> changes, you get a cache miss. Consider another example under the same command line: In this example, the cache miss occurs on line 3. The miss happens because the variable's value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value. If an ENV ins",
      "chunk_index": 53
    },
    {
      "chunk_id": "44f2bef716bd7ada2bbdc36ad68e0ee112dba279",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "iss happens because the variable's value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value. If an ENV instruction overrides an ARG instruction of the same name, like this Dockerfile: Line 3 doesn't cause a cache miss because the value of CONT_IMG_VER is a constant ( hello ). As a result, the environment variables and values used on the RUN (line 4) doesn't change between builds. The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile. This is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration. For example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can't just call ADD and RUN now, because you don't yet have access to the application source code, and it will be different for each application build. You could si",
      "chunk_index": 54
    },
    {
      "chunk_id": "539448fd9ca6d7363857f1bfa64b7c9df41592a2",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ght require a build script to be called after that. You can't just call ADD and RUN now, because you don't yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that's inefficient, error-prone and difficult to update because it mixes with application-specific code. The solution is to use ONBUILD to register advance instructions to run later, during the next build stage. Here's how it works: When it encounters an ONBUILD instruction, the builder adds a trigger to the metadata of the image being built. The instruction doesn't otherwise affect the current build. At the end of the build, a list of all triggers is stored in the image manifest, under the key OnBuild . They can be inspected with the docker inspect command. Later the image may be used as a base for a new build, using the FROM instruction. As part of processing the FROM instruction, the downstream builder looks for ONBUILD triggers, and executes them in the same order they were registered. If any of the triggers fail, the FROM instruction is aborted which in turn causes the build to fail. If all triggers succeed, the FROM instruction completes and the build continues as usual. Triggers are cleared from the final image after being executed. In other words t",
      "chunk_index": 55
    },
    {
      "chunk_id": "bd2d066a4fad0b1346e5f56fb4a08236783b0dc8",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "s aborted which in turn causes the build to fail. If all triggers succeed, the FROM instruction completes and the build continues as usual. Triggers are cleared from the final image after being executed. In other words they aren't inherited by \"grand-children\" builds. For example you might add something like this: As of Dockerfile syntax 1.11, you can use ONBUILD with instructions that copy or mount files from other stages, images, or build contexts. For example: If the source of from is a build stage, the stage must be defined in the Dockerfile where ONBUILD gets triggered. If it's a named context, that context must be passed to the downstream build. Chaining ONBUILD instructions using ONBUILD ONBUILD isn't allowed. The ONBUILD instruction may not trigger FROM or MAINTAINER instructions. The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a signal name in the format SIG<NAME> , for instance SIGKILL , or an unsigned number that matches a position in the kernel's syscall table, for instance 9 . The default is SIGTERM if not defined. The image's default stopsignal can be overridden per container, using the --stop-signal flag on docker run and docker create . The HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NON",
      "chunk_index": 56
    },
    {
      "chunk_id": "23725c581c575210baaf60f272c169d59252a01a",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "ing the --stop-signal flag on docker run and docker create . The HEALTHCHECK instruction has two forms: HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) The HEALTHCHECK instruction tells Docker how to test a container to check that it's still working. This can detect cases such as a web server stuck in an infinite loop and unable to handle new connections, even though the server process is still running. When a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting . Whenever a health check passes, it becomes healthy (whatever state it was previously in). After a certain number of consecutive failures, it becomes unhealthy . The options that can appear before CMD are: --interval=DURATION (default: 30s ) --timeout=DURATION (default: 30s ) --start-period=DURATION (default: 0s ) --start-interval=DURATION (default: 5s ) --retries=N (default: 3 ) The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes. If a single run of the check takes longer than timeout seconds then the check is considered to have failed. The process performing the check is abruptly stopped with a SIGKILL . It takes retries c",
      "chunk_index": 57
    },
    {
      "chunk_id": "c916b67b5ccd8c32799e4d879b9b48a9acdc311b",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "evious check completes. If a single run of the check takes longer than timeout seconds then the check is considered to have failed. The process performing the check is abruptly stopped with a SIGKILL . It takes retries consecutive failures of the health check for the container to be considered unhealthy . start period provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries. start interval is the time between health checks during the start period. This option requires Docker Engine version 25.0 or later. There can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect. The command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running ) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details). The command's exit status indicates the health status of the container. The possible values are: 0: success - the container is healthy and ready for use 1: unhealthy - the container isn't working correctly 2: reserved - don't use this exit code For example, to check eve",
      "chunk_index": 58
    },
    {
      "chunk_id": "47f60afd7ff6d4cbe7f88685ffad300ce7a819cf",
      "url": "https://docs.docker.com/engine/reference/builder/",
      "title": "Dockerfile reference",
      "text": "tus of the container. The possible values are: 0: success - the container is healthy and ready for use 1: unhealthy - the container isn't working correctly 2: reserved - don't use this exit code For example, to check every five minutes or so that a web-server is able to serve the site's main page within three seconds: To help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with docker inspect . Such output should be kept short (only the first 4096 bytes are stored currently). When the health status of a container changes, a health_status event is generated with the new status. The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"] , and on Windows is [\"cmd\", \"/S\", \"/C\"] . The SHELL instruction must be written in JSON form in a Dockerfile. The SHELL instruction is particularly useful on Windows where there are two commonly used and quite different native shells: cmd and powershell , as well as alternate shells available including sh . The SHELL instruction can appear multiple times. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. For example: The following instructions can be affected by the SHELL instruction when the shell f",
      "chunk_index": 59
    },
    {
      "chunk_id": "6d6b9525cdb96d1e94155aceed5b5d118e7f2eef",
      "url": "https://developer.hashicorp.com/terraform/language",
      "title": "Configuration Language",
      "text": "This is the documentation for Terraform's configuration language. It is relevant to users of Terraform CLI , HCP Terraform , and Terraform Enterprise . Terraform's language is its primary user interface. Configuration files you write in Terraform language tell Terraform what plugins to install, what infrastructure to create, and what data to fetch. Terraform language also lets you define dependencies between resources and create multiple similar resources from a single configuration block. Hands-on: Try the Write Terraform Configuration tutorials. The main purpose of the Terraform language is declaring resources , which represent infrastructure objects. All other language features exist only to make the definition of resources more flexible and convenient. A Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure. A configuration can consist of multiple files and directories. The syntax of the Terraform language consists of only a few basic elements: resource \"aws_vpc\" \"main\" { cidr_block = var.base_cidr_block } < BLOCK TYPE > \"<BLOCK LABEL>\" \"<BLOCK LABEL>\" { # Block body < IDENTIFIER > = < EXPRESSION > # Argument } Blocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels,",
      "chunk_index": 0
    },
    {
      "chunk_id": "4ce9f7e42dbcb45f13e6934076f9d6f77fc98933",
      "url": "https://developer.hashicorp.com/terraform/language",
      "title": "Configuration Language",
      "text": " IDENTIFIER > = < EXPRESSION > # Argument } Blocks are containers for other content and usually represent the configuration of some kind of object, like a resource. Blocks have a block type, can have zero or more labels, and have a body that contains any number of arguments and nested blocks. Most of Terraform's features are controlled by top-level blocks in a configuration file. Arguments assign a value to a name. They appear within blocks. Expressions represent a value, either literally or by referencing and combining other values. They appear as values for arguments, or within other expressions. The Terraform language is declarative, describing an intended goal rather than the steps to reach that goal. The ordering of blocks and the files they are organized into are generally not significant; Terraform only considers implicit and explicit relationships between resources when determining an order of operations. The following example describes a simple network topology for Amazon Web Services, just to give a sense of the overall structure and syntax of the Terraform language. Similar configurations can be created for other virtual network services, using resource types defined by other providers, and a practical network configuration will often contain additional elements not shown here. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 1.0.4\" } }",
      "chunk_index": 1
    },
    {
      "chunk_id": "7b6155e5a77dec7c0f7f04424f83a8f2312aac2a",
      "url": "https://developer.hashicorp.com/terraform/language",
      "title": "Configuration Language",
      "text": "ce types defined by other providers, and a practical network configuration will often contain additional elements not shown here. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 1.0.4\" } } } variable \"aws_region\" {} variable \"base_cidr_block\" { description = \"A /16 CIDR range definition, such as 10.1.0.0/16, that the VPC will use\" default = \"10.1.0.0/16\" } variable \"availability_zones\" { description = \"A list of availability zones in which to create subnets\" type = list ( string ) } provider \"aws\" { region = var.aws_region } resource \"aws_vpc\" \"main\" { # Referencing the base_cidr_block variable allows the network address # to be changed without modifying the configuration. cidr_block = var.base_cidr_block } resource \"aws_subnet\" \"az\" { # Create one subnet for each given availability zone. count = length(var . availability_zones) # For each subnet, use one of the specified availability zones. availability_zone = var.availability_zones[count.index] # By referencing the aws_vpc.main object, Terraform knows that the subnet # must be created only after the VPC is created. vpc_id = aws_vpc.main.id # Built-in functions and operators can be used for simple transformations of # values, such as computing a subnet address. Here we create a /20 prefix for # each subnet, using consecutive addresses for each availability zone, # such as 10.1.16.0/20 . cidr_blo",
      "chunk_index": 2
    },
    {
      "chunk_id": "f26dd924866e08497a5d95dc1460492764286199",
      "url": "https://developer.hashicorp.com/terraform/language",
      "title": "Configuration Language",
      "text": "be used for simple transformations of # values, such as computing a subnet address. Here we create a /20 prefix for # each subnet, using consecutive addresses for each availability zone, # such as 10.1.16.0/20 . cidr_block = cidrsubnet(aws_vpc . main . cidr_block , 4 , count . index + 1 ) }",
      "chunk_index": 3
    },
    {
      "chunk_id": "1ed17fbb869585128a4d0bb1d7920da96519c359",
      "url": "https://developer.hashicorp.com/terraform/tutorials",
      "title": "Tutorials | Terraform | HashiCorp Developer",
      "text": "Try HCP Terraform Sign up for HCP Terraform for free and start managing infrastructure with your team. Build, change, and destroy infrastructure with Terraform. Start here to learn the basics of Terraform with your favorite cloud provider. Create, manage, and destroy AWS infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Build, change, and destroy Azure infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Collaborate on version-controlled configuration using HCP Terraform. Follow this track to build, change, and destroy infrastructure using remote runs and state. Build, change, and destroy Docker infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Build, change, and destroy Google Cloud Platform (GCP) infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Build, change, and destroy a virtual cloud network and subnet on Oracle Cloud Infrastructure (OCI) using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Try the newest tutorials for common Terraform tasks and use cases. Write Terraform Tests Write tests ",
      "chunk_index": 0
    },
    {
      "chunk_id": "3df4481376b44e1dc7964c1a9605b028455eb70d",
      "url": "https://developer.hashicorp.com/terraform/tutorials",
      "title": "Tutorials | Terraform | HashiCorp Developer",
      "text": ") using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Try the newest tutorials for common Terraform tasks and use cases. Write Terraform Tests Write tests to validate the behavior of your Terraform module configuration. Enable self-service workflows with Vault-backed dynamic credentials Manage dynamic credentials with an HCP Terraform project. Create trust relationships for a single workspace, an entire project, and provision infrastructure with a no-code module. Use health assessments to detect infrastructure drift Enable and use HCP Terraform health assessments to detect infrastructure changes outside the Terraform workflow. Manage hosted Apache Kafka with the Confluent Terraform provider Use the Confluent provider to create Apache Kafka clusters, topics, and service accounts using Terraform. Detect infrastructure drift and enforce policies Use HCP Terraform to enforce policies and detect infrastructure configuration drift. Create and use no-code modules Use HCP Terraform no-code modules to let users deploy resources without writing Terraform configuration. Prepare for Terraform certifications with our prep materials. Prepare for the Terraform Associate 004 Certification Exam Prepare for the Terraform Associate exam. Choose to follow an in-depth guide or to review select exam topics depending on the kind of prep",
      "chunk_index": 1
    },
    {
      "chunk_id": "8e8e46466883da7fef2256b3b2d4bcba7c2cd07c",
      "url": "https://developer.hashicorp.com/terraform/tutorials",
      "title": "Tutorials | Terraform | HashiCorp Developer",
      "text": "th our prep materials. Prepare for the Terraform Associate 004 Certification Exam Prepare for the Terraform Associate exam. Choose to follow an in-depth guide or to review select exam topics depending on the kind of preparation support you need. Then review sample questions to learn what to expect on exam day. Prepare for the Terraform Authoring and Operations Professional Certification Exam Prepare for your Terraform Professional certification exam. Choose to follow an in-depth guide, or review select exam topics depending on the kind of preparation support you need. Use the Terraform Plugin Framework to build providers that use common Go conventions. Call APIs with Custom Framework Providers In these tutorials, learn how Terraform uses providers to interact with target APIs. Then, build a custom provider based on the new Terraform Plugin Framework.",
      "chunk_index": 2
    },
    {
      "chunk_id": "254475f29666f5d78a82effff40633c6e3dc5e25",
      "url": "https://redis.io/docs/latest/develop/data-types/",
      "title": "Redis data types",
      "text": "Redis data types Overview of data types supported by Redis Redis is a data structure server. At its core, Redis provides a collection of native data types that help you solve a wide variety of problems, from caching to queuing to event processing . Below is a short description of each data type, with links to broader overviews and command references. Each overview includes a comprehensive tutorial with code samples. Data types Redis Open Source implements the following data types: See Compare data types for advice on which of the general-purpose data types is best for common tasks. Strings Redis strings are the most basic Redis data type, representing a sequence of bytes. For more information, see: Lists Redis lists are lists of strings sorted by insertion order. For more information, see: Sets Redis sets are unordered collections of unique strings that act like the sets from your favorite programming language (for example, Java HashSets , Python sets , and so on). With a Redis set, you can add, remove, and test for existence in O(1) time (in other words, regardless of the number of set elements). For more information, see: Hashes Redis hashes are record types modeled as collections of field-value pairs. As such, Redis hashes resemble Python dictionaries , Java HashMaps , and Ruby hashes . For more information, see: Sorted sets Redis sorted sets are collections of unique string",
      "chunk_index": 0
    },
    {
      "chunk_id": "bfa6a14a6114772858685be0b658d1353cf3077a",
      "url": "https://redis.io/docs/latest/develop/data-types/",
      "title": "Redis data types",
      "text": "odeled as collections of field-value pairs. As such, Redis hashes resemble Python dictionaries , Java HashMaps , and Ruby hashes . For more information, see: Sorted sets Redis sorted sets are collections of unique strings that maintain order by each string's associated score. For more information, see: Vector sets Redis vector sets are a specialized data type designed for managing high-dimensional vector data, enabling fast and efficient vector similarity search within Redis. Vector sets are optimized for use cases involving machine learning, recommendation systems, and semantic search, where each vector represents a data point in multi-dimensional space. Vector sets supports the HNSW (hierarchical navigable small world) algorithm, allowing you to store, index, and query vectors based on the cosine similarity metric. With vector sets, Redis provides native support for hybrid search, combining vector similarity with structured filters . For more information, see: Streams A Redis stream is a data structure that acts like an append-only log. Streams help record events in the order they occur and then syndicate them for processing. For more information, see: Geospatial indexes Redis geospatial indexes are useful for finding locations within a given geographic radius or bounding box. For more information, see: Bitmaps Redis bitmaps let you perform bitwise operations on strings. For ",
      "chunk_index": 1
    },
    {
      "chunk_id": "e6a56fea0226891e66e23391164338ec9eaeef82",
      "url": "https://redis.io/docs/latest/develop/data-types/",
      "title": "Redis data types",
      "text": "ial indexes Redis geospatial indexes are useful for finding locations within a given geographic radius or bounding box. For more information, see: Bitmaps Redis bitmaps let you perform bitwise operations on strings. For more information, see: Bitfields Redis bitfields efficiently encode multiple counters in a string value. Bitfields provide atomic get, set, and increment operations and support different overflow policies. For more information, see: JSON Redis JSON provides structured, hierarchical arrays and key-value objects that match the popular JSON text file format. You can import JSON text into Redis objects and access, modify, and query individual data elements. For more information, see: Probabilistic data types These data types let you gather and calculate statistics in a way that is approximate but highly efficient. The following types are available: HyperLogLog The Redis HyperLogLog data structures provide probabilistic estimates of the cardinality (i.e., number of elements) of large sets. For more information, see: Bloom filter Redis Bloom filters let you check for the presence or absence of an element in a set. For more information, see: Cuckoo filter Redis Cuckoo filters let you check for the presence or absence of an element in a set. They are similar to Bloom filters but with slightly different trade-offs between features and performance. For more information, s",
      "chunk_index": 2
    },
    {
      "chunk_id": "e3a219d55b34bcf6f7cf75fb9d5e2789ce9f4430",
      "url": "https://redis.io/docs/latest/develop/data-types/",
      "title": "Redis data types",
      "text": "er Redis Cuckoo filters let you check for the presence or absence of an element in a set. They are similar to Bloom filters but with slightly different trade-offs between features and performance. For more information, see: t-digest Redis t-digest structures estimate percentiles from a stream of data values. For more information, see: Top-K Redis Top-K structures estimate the ranking of a data point within a stream of values. For more information, see: Count-min sketch Redis Count-min sketch estimate the frequency of a data point within a stream of values. For more information, see: Time series Redis time series structures let you store and query timestamped data points. For more information, see: Adding extensions To extend the features provided by the included data types, use one of these options: Write your own custom server-side functions in Lua . Write your own Redis module using the modules API or check out the community-supported modules .",
      "chunk_index": 3
    },
    {
      "chunk_id": "1b9e8926366fceeaf2cbd672acf2c04c3c3e6666",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": "{\"acl_categories\":[\"@write\",\"@string\",\"@slow\"],\"arguments\":[{\"display_text\":\"key\",\"key_spec_index\":0,\"name\":\"key\",\"type\":\"key\"},{\"display_text\":\"value\",\"name\":\"value\",\"type\":\"string\"},{\"arguments\":[{\"display_text\":\"nx\",\"name\":\"nx\",\"token\":\"NX\",\"type\":\"pure-token\"},{\"display_text\":\"xx\",\"name\":\"xx\",\"token\":\"XX\",\"type\":\"pure-token\"},{\"display_text\":\"ifeq-value\",\"name\":\"ifeq-value\",\"since\":\"8.4.0\",\"token\":\"IFEQ\",\"type\":\"string\"},{\"display_text\":\"ifne-value\",\"name\":\"ifne-value\",\"since\":\"8.4.0\",\"token\":\"IFNE\",\"type\":\"string\"},{\"display_text\":\"ifdeq-digest\",\"name\":\"ifdeq-digest\",\"since\":\"8.4.0\",\"token\":\"IFDEQ\",\"type\":\"integer\"},{\"display_text\":\"ifdne-digest\",\"name\":\"ifdne-digest\",\"since\":\"8.4.0\",\"token\":\"IFDNE\",\"type\":\"integer\"}],\"name\":\"condition\",\"optional\":true,\"since\":\"2.6.12\",\"type\":\"oneof\"},{\"display_text\":\"get\",\"name\":\"get\",\"optional\":true,\"since\":\"6.2.0\",\"token\":\"GET\",\"type\":\"pure-token\"},{\"arguments\":[{\"display_text\":\"seconds\",\"name\":\"seconds\",\"since\":\"2.6.12\",\"token\":\"EX\",\"type\":\"integer\"},{\"display_text\":\"milliseconds\",\"name\":\"milliseconds\",\"since\":\"2.6.12\",\"token\":\"PX\",\"type\":\"integer\"},{\"display_text\":\"unix-time-seconds\",\"name\":\"unix-time-seconds\",\"since\":\"6.2.0\",\"token\":\"EXAT\",\"type\":\"unix-time\"},{\"display_text\":\"unix-time-milliseconds\",\"name\":\"unix-time-milliseconds\",\"since\":\"6.2.0\",\"token\":\"PXAT\",\"type\":\"unix-time\"},{\"display_text\":\"keepttl\",\"name\":\"keepttl\",\"since\":\"6",
      "chunk_index": 0
    },
    {
      "chunk_id": "7c1ce94d950b79b17cad0ed72d8a12ff89d64616",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": "\"6.2.0\",\"token\":\"EXAT\",\"type\":\"unix-time\"},{\"display_text\":\"unix-time-milliseconds\",\"name\":\"unix-time-milliseconds\",\"since\":\"6.2.0\",\"token\":\"PXAT\",\"type\":\"unix-time\"},{\"display_text\":\"keepttl\",\"name\":\"keepttl\",\"since\":\"6.0.0\",\"token\":\"KEEPTTL\",\"type\":\"pure-token\"}],\"name\":\"expiration\",\"optional\":true,\"type\":\"oneof\"}],\"arity\":-3,\"categories\":[\"docs\",\"develop\",\"stack\",\"oss\",\"rs\",\"rc\",\"oss\",\"kubernetes\",\"clients\"],\"command_flags\":[\"write\",\"denyoom\"],\"complexity\":\"O(1)\",\"description\":\"Sets the string value of a key, ignoring its type. The key is created if it doesn't exist.\",\"duplicateOf\":\"head:data-ai-metadata\",\"group\":\"string\",\"key_specs\":[{\"RW\":true,\"access\":true,\"begin_search\":{\"spec\":{\"index\":1},\"type\":\"index\"},\"find_keys\":{\"spec\":{\"keystep\":1,\"lastkey\":0,\"limit\":0},\"type\":\"range\"},\"notes\":\"RW and ACCESS due to the optional `GET` argument\",\"update\":true,\"variable_flags\":true}],\"location\":\"body\",\"since\":\"1.0.0\",\"syntax_fmt\":\"SET key value [NX | XX | IFEQÂ ifeq-value | IFNEÂ ifne-value |\\n IFDEQÂ ifdeq-digest | IFDNEÂ ifdne-digest] [GET] [EXÂ seconds |\\n PXÂ milliseconds | EXATÂ unix-time-seconds |\\n PXATÂ unix-time-milliseconds | KEEPTTL]\",\"title\":\"SET\",\"tableOfContents\":{\"sections\":[{\"id\":\"options\",\"title\":\"Options\"},{\"id\":\"hash-digest\",\"title\":\"Hash Digest\"},{\"id\":\"examples\",\"title\":\"Examples\",\"children\":[{\"id\":\"code-examples\",\"title\":\"Code examples\"}]},{\"id\":\"patterns\",\"titl",
      "chunk_index": 1
    },
    {
      "chunk_id": "3b06a0cf85b1036007eb4c5d6360d830e4ca068a",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": "Contents\":{\"sections\":[{\"id\":\"options\",\"title\":\"Options\"},{\"id\":\"hash-digest\",\"title\":\"Hash Digest\"},{\"id\":\"examples\",\"title\":\"Examples\",\"children\":[{\"id\":\"code-examples\",\"title\":\"Code examples\"}]},{\"id\":\"patterns\",\"title\":\"Patterns\"},{\"id\":\"redis-enterprise-and-redis-cloud-compatibility\",\"title\":\"Redis Enterprise and Redis Cloud compatibility\"},{\"id\":\"return-information\",\"title\":\"Return information\"}]},\"codeExamples\":[]} SET Available since: Redis Open Source 1.0.0 Time complexity: O(1) ACL categories: @write , @string , @slow , Compatibility: Redis Enterprise and Redis Cloud compatibility Set key to hold the string value . If key already holds a value, it is overwritten, regardless of its type. Any previous time to live associated with the key is discarded on successful SET operation. Options The SET command supports a set of options that modify its behavior: NX -- Only set the key if it does not already exist. XX -- Only set the key if it already exists. IFEQ ifeq-value -- Set the keyâs value and expiration only if its current value is equal to ifeq-value . If the key doesnât exist, it wonât be created. IFNE ifne-value -- Set the keyâs value and expiration only if its current value is not equal to ifne-value . If the key doesnât exist, it will be created. IFDEQ ifeq-digest -- Set the keyâs value and expiration only if the hash digest of its current value is equal",
      "chunk_index": 2
    },
    {
      "chunk_id": "fea5bbf30cda9252665e548c8e078326cf716122",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": "ation only if its current value is not equal to ifne-value . If the key doesnât exist, it will be created. IFDEQ ifeq-digest -- Set the keyâs value and expiration only if the hash digest of its current value is equal to ifeq-digest . If the key doesnât exist, it wonât be created. See the Hash Digest section below for more information. IFDNE ifne-digest -- Set the keyâs value and expiration only if the hash digest of its current value is not equal to ifne-digest . If the key doesnât exist, it will be created. See the Hash Digest section below for more information. GET -- Return the old string stored at key, or nil if key did not exist. An error is returned and SET aborted if the value stored at key is not a string. EX seconds -- Set the specified expire time, in seconds (a positive integer). PX milliseconds -- Set the specified expire time, in milliseconds (a positive integer). EXAT timestamp-seconds -- Set the specified Unix time at which the key will expire, in seconds (a positive integer). PXAT timestamp-milliseconds -- Set the specified Unix time at which the key will expire, in milliseconds (a positive integer). KEEPTTL -- Retain the time to live associated with the key. Note: Since the SET command options can replace SETNX , SETEX , PSETEX , GETSET , it is possible that in future versions of Redis these commands will be deprecated and finally removed. Hash Dige",
      "chunk_index": 3
    },
    {
      "chunk_id": "b0e4d3e738b89be3eaa89744044102aafdfb6060",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": " associated with the key. Note: Since the SET command options can replace SETNX , SETEX , PSETEX , GETSET , it is possible that in future versions of Redis these commands will be deprecated and finally removed. Hash Digest A hash digest is a fixed-size numerical representation of a string value, computed using the XXH3 hash algorithm. Redis uses this hash digest for efficient comparison operations without needing to compare the full string content. You can retrieve a key's hash digest using the DIGEST command, which returns it as a hexadecimal string that you can use with the IFDEQ and IFDNE options, and also the DELEX command's IFDEQ and IFDNE options. Examples Code examples \"\"\" Code samples for data structure store quickstart pages: https://redis.io/docs/latest/develop/get-started/data-store/ \"\"\" import redis r = redis . Redis ( host = \"localhost\" , port = 6379 , db = 0 , decode_responses = True ) res = r . set ( \"bike:1\" , \"Process 134\" ) print ( res ) # >>> True res = r . get ( \"bike:1\" ) print ( res ) # >>> \"Process 134\" import { createClient } from 'redis' ; const client = createClient (); client . on ( 'error' , err => console . log ( 'Redis Client Error' , err )); await client . connect (). catch ( console . error ); await client . set ( 'bike:1' , 'Process 134' ); const value = await client . get ( 'bike:1' ); console . log ( value ); // returns 'Process 134' await cli",
      "chunk_index": 4
    },
    {
      "chunk_id": "3044d3edf9b41ccf99c41a8a886001e6f9ce2d05",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": " , err )); await client . connect (). catch ( console . error ); await client . set ( 'bike:1' , 'Process 134' ); const value = await client . get ( 'bike:1' ); console . log ( value ); // returns 'Process 134' await client . close (); package io.redis.examples ; import redis.clients.jedis.RedisClient ; public class SetGetExample { public void run () { RedisClient jedis = RedisClient . create ( \"redis://localhost:6379\" ); String status = jedis . set ( \"bike:1\" , \"Process 134\" ); if ( \"OK\" . equals ( status )) System . out . println ( \"Successfully added a bike.\" ); String value = jedis . get ( \"bike:1\" ); if ( value != null ) System . out . println ( \"The name of the bike is: \" + value + \".\" ); jedis . close (); } } package example_commands_test import ( \"context\" \"fmt\" \"github.com/redis/go-redis/v9\" ) func ExampleClient_Set_and_get () { ctx := context . Background () rdb := redis . NewClient ( & redis . Options { Addr : \"localhost:6379\" , Password : \"\" , // no password docs DB : 0 , // use default DB }) err := rdb . Set ( ctx , \"bike:1\" , \"Process 134\" , 0 ). Err () if err != nil { panic ( err ) } fmt . Println ( \"OK\" ) value , err := rdb . Get ( ctx , \"bike:1\" ). Result () if err != nil { panic ( err ) } fmt . Printf ( \"The name of the bike is %s\" , value ) } using NRedisStack.Tests ; using StackExchange.Redis ; public class SetGetExample { public void Run () { var muxer = Co",
      "chunk_index": 5
    },
    {
      "chunk_id": "870761a5ece05a50ba6200bdc6239e8539bd1bb0",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": " ). Result () if err != nil { panic ( err ) } fmt . Printf ( \"The name of the bike is %s\" , value ) } using NRedisStack.Tests ; using StackExchange.Redis ; public class SetGetExample { public void Run () { var muxer = ConnectionMultiplexer . Connect ( \"localhost:6379\" ); var db = muxer . GetDatabase (); bool status = db . StringSet ( \"bike:1\" , \"Process 134\" ); if ( status ) Console . WriteLine ( \"Successfully added a bike.\" ); var value = db . StringGet ( \"bike:1\" ); if ( value . HasValue ) Console . WriteLine ( \"The name of the bike is: \" + value + \".\" ); } } Patterns Note: The following pattern is discouraged in favor of the Redlock algorithm which is only a bit more complex to implement, but offers better guarantees and is fault tolerant. The command SET resource-name anystring NX EX max-lock-time is a simple way to implement a locking system with Redis. A client can acquire the lock if the above command returns OK (or retry after some time if the command returns Nil), and remove the lock just using DEL . The lock will be auto-released after the expire time is reached. It is possible to make this system more robust modifying the unlock schema as follows: Instead of setting a fixed string, set a non-guessable large random string, called token. Instead of releasing the lock with DEL , send a script that only removes the key if the value matches. This avoids that a client will",
      "chunk_index": 6
    },
    {
      "chunk_id": "d85e9c298ef071e8044221a60dbd8624da9e7157",
      "url": "https://redis.io/docs/latest/commands/set/",
      "title": "SET | Docs",
      "text": "tead of setting a fixed string, set a non-guessable large random string, called token. Instead of releasing the lock with DEL , send a script that only removes the key if the value matches. This avoids that a client will try to release the lock after the expire time deleting the key created by another client that acquired the lock later. An example of unlock script would be similar to the following: if redis.call(\"get\",KEYS[1]) == ARGV[1] then return redis.call(\"del\",KEYS[1]) else return 0 end The script should be called with EVAL ...script... 1 resource-name token-value Redis Enterprise and Redis Cloud compatibility Redis Enterprise Redis Cloud Notes â Standard â Active-Active â Standard â Active-Active Return information History Starting with Redis version 2.6.12: Added the EX , PX , NX and XX options. Starting with Redis version 6.0.0: Added the KEEPTTL option. Starting with Redis version 6.2.0: Added the GET , EXAT and PXAT option. Starting with Redis version 7.0.0: Allowed the NX and GET options to be used together. Starting with Redis version 8.4.0: Added 'IFEQ', 'IFNE', 'IFDEQ', 'IFDNE' options.",
      "chunk_index": 7
    },
    {
      "chunk_id": "9f1e2a73097016763d6dea8bc945dd69d9bb034b",
      "url": "https://www.mongodb.com/docs/manual/introduction/",
      "title": "Introduction to MongoDB",
      "text": "You can create a MongoDB database in the following environments: To learn more about creating a MongoDB database with the Atlas UI, see Get Started with Atlas. A record in MongoDB is a document, which is a data structure composed of field and value pairs. MongoDB documents are similar to JSON objects. The values of fields may include other documents, arrays, and arrays of documents. The advantages of using documents are: Documents correspond to native data types in many programming languages. Embedded documents and arrays reduce need for expensive joins. Dynamic schema supports fluent polymorphism. MongoDB stores documents in collections . Collections are analogous to tables in relational databases. In addition to collections, MongoDB supports: MongoDB provides high performance data persistence. In particular, MongoDB's replication facility, called replica set , provides: automatic failover data redundancy. A replica set is a group of MongoDB servers that maintain the same data set, providing redundancy and increasing data availability. MongoDB provides horizontal scalability as part of its core functionality: Sharding distributes data across a cluster of machines. Starting in 3.4, MongoDB supports creating zones of data based on the shard key . In a balanced cluster, MongoDB directs reads and writes covered by a zone only to those shards inside the zone. See the Zones manual p",
      "chunk_index": 0
    },
    {
      "chunk_id": "30ef52bee43466c0c29281924be3dc479c29c0ca",
      "url": "https://www.mongodb.com/docs/manual/introduction/",
      "title": "Introduction to MongoDB",
      "text": "es. Starting in 3.4, MongoDB supports creating zones of data based on the shard key . In a balanced cluster, MongoDB directs reads and writes covered by a zone only to those shards inside the zone. See the Zones manual page for more information. MongoDB supports multiple storage engines: In addition, MongoDB provides pluggable storage engine API that allows third parties to develop storage engines for MongoDB.",
      "chunk_index": 1
    },
    {
      "chunk_id": "9744bb0b4c6af1482b22406484ca3787807df29b",
      "url": "https://www.postgresql.org/docs/current/tutorial.html",
      "title": "Part I. Tutorial",
      "text": "Welcome to the PostgreSQL Tutorial. The tutorial is intended to give an introduction to PostgreSQL , relational database concepts, and the SQL language. We assume some general knowledge about how to use computers and no particular Unix or programming experience is required. This tutorial is intended to provide hands-on experience with important aspects of the PostgreSQL system. It makes no attempt to be a comprehensive treatment of the topics it covers. After you have successfully completed this tutorial you will want to read the Part II section to gain a better understanding of the SQL language, or Part IV for information about developing applications with PostgreSQL . Those who provision and manage their own PostgreSQL installation should also read Part III .",
      "chunk_index": 0
    },
    {
      "chunk_id": "cad6d7842250bc0caa8ae12fa49d282f149e958f",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "{\"acl_categories\":[\"@write\",\"@stream\",\"@fast\"],\"arguments\":[{\"display_text\":\"key\",\"key_spec_index\":0,\"name\":\"key\",\"type\":\"key\"},{\"display_text\":\"nomkstream\",\"name\":\"nomkstream\",\"optional\":true,\"since\":\"6.2.0\",\"token\":\"NOMKSTREAM\",\"type\":\"pure-token\"},{\"arguments\":[{\"arguments\":[{\"display_text\":\"maxlen\",\"name\":\"maxlen\",\"token\":\"MAXLEN\",\"type\":\"pure-token\"},{\"display_text\":\"minid\",\"name\":\"minid\",\"since\":\"6.2.0\",\"token\":\"MINID\",\"type\":\"pure-token\"}],\"name\":\"strategy\",\"type\":\"oneof\"},{\"arguments\":[{\"display_text\":\"equal\",\"name\":\"equal\",\"token\":\"=\",\"type\":\"pure-token\"},{\"display_text\":\"approximately\",\"name\":\"approximately\",\"token\":\"~\",\"type\":\"pure-token\"}],\"name\":\"operator\",\"optional\":true,\"type\":\"oneof\"},{\"display_text\":\"threshold\",\"name\":\"threshold\",\"type\":\"string\"},{\"display_text\":\"count\",\"name\":\"count\",\"optional\":true,\"since\":\"6.2.0\",\"token\":\"LIMIT\",\"type\":\"integer\"}],\"name\":\"trim\",\"optional\":true,\"type\":\"block\"},{\"arguments\":[{\"display_text\":\"auto-id\",\"name\":\"auto-id\",\"token\":\"*\",\"type\":\"pure-token\"},{\"display_text\":\"id\",\"name\":\"id\",\"type\":\"string\"}],\"name\":\"id-selector\",\"type\":\"oneof\"},{\"arguments\":[{\"display_text\":\"field\",\"name\":\"field\",\"type\":\"string\"},{\"display_text\":\"value\",\"name\":\"value\",\"type\":\"string\"}],\"multiple\":true,\"name\":\"data\",\"type\":\"block\"}],\"arity\":-5,\"categories\":[\"docs\",\"develop\",\"stack\",\"oss\",\"rs\",\"rc\",\"oss\",\"kubernetes\",\"clients\"],\"command_flags\":[\"write\",\"",
      "chunk_index": 0
    },
    {
      "chunk_id": "232390ab72e458b1b22c5af11dc75d598421a615",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "lay_text\":\"value\",\"name\":\"value\",\"type\":\"string\"}],\"multiple\":true,\"name\":\"data\",\"type\":\"block\"}],\"arity\":-5,\"categories\":[\"docs\",\"develop\",\"stack\",\"oss\",\"rs\",\"rc\",\"oss\",\"kubernetes\",\"clients\"],\"command_flags\":[\"write\",\"denyoom\",\"fast\"],\"complexity\":\"O(1) when adding a new entry, O(N) when trimming where N being the number of entries evicted.\",\"description\":\"Appends a new message to a stream. Creates the key if it doesn't exist.\",\"duplicateOf\":\"head:data-ai-metadata\",\"group\":\"stream\",\"key_specs\":[{\"RW\":true,\"begin_search\":{\"spec\":{\"index\":1},\"type\":\"index\"},\"find_keys\":{\"spec\":{\"keystep\":1,\"lastkey\":0,\"limit\":0},\"type\":\"range\"},\"notes\":\"UPDATE instead of INSERT because of the optional trimming feature\",\"update\":true}],\"location\":\"body\",\"since\":\"5.0.0\",\"syntax_fmt\":\"XADD key [NOMKSTREAM] [KEEPREF | DELREF | ACKED] [\\u003cMAXLEN | MINID\\u003e [= | ~] threshold\\n [LIMITÂ count]] \\u003c* | id\\u003e field value [field value ...]\",\"title\":\"XADD\",\"tableOfContents\":{\"sections\":[{\"id\":\"required-arguments\",\"title\":\"Required arguments\"},{\"id\":\"optional-arguments\",\"title\":\"Optional arguments\"},{\"id\":\"specifying-a-stream-id-as-an-argument\",\"title\":\"Specifying a Stream ID as an argument\"},{\"id\":\"capped-streams\",\"title\":\"Capped streams\"},{\"id\":\"additional-information-about-streams\",\"title\":\"Additional information about streams\"},{\"id\":\"examples\",\"title\":\"Examples\"},{\"id\":\"redis-enterprise-and",
      "chunk_index": 1
    },
    {
      "chunk_id": "bec46abd2e3a5f933b925574a14d01317cba9d67",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "an argument\"},{\"id\":\"capped-streams\",\"title\":\"Capped streams\"},{\"id\":\"additional-information-about-streams\",\"title\":\"Additional information about streams\"},{\"id\":\"examples\",\"title\":\"Examples\"},{\"id\":\"redis-enterprise-and-redis-cloud-compatibility\",\"title\":\"Redis Enterprise and Redis Cloud compatibility\"},{\"id\":\"return-information\",\"title\":\"Return information\"}]},\"codeExamples\":[]} XADD Available since: Redis Open Source 5.0.0 Time complexity: O(1) when adding a new entry, O(N) when trimming where N being the number of entries evicted. ACL categories: @write , @stream , @fast , Compatibility: Redis Enterprise and Redis Cloud compatibility Appends the specified stream entry to the stream at the specified key . If the key does not exist, XADD will create a new key with the given stream value as a side effect of running this command. You can turn off key creation with the NOMKSTREAM option. Required arguments key The name of the stream key. id The stream entry ID. Use * to auto-generate a unique ID, or specify a well-formed ID in the format <ms>-<seq> (for example, 1526919030474-55 ). field value [field value ...] One or more field-value pairs that make up the stream entry. You must provide at least one field-value pair. Optional arguments NOMKSTREAM Prevents the creation of a new stream if the key does not exist. Available since Redis 6.2.0. KEEPREF | DELREF | ACKED Specifies how ",
      "chunk_index": 2
    },
    {
      "chunk_id": "9a0ce45f8d83d7d6902223c7acde36f145c4bb89",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "m entry. You must provide at least one field-value pair. Optional arguments NOMKSTREAM Prevents the creation of a new stream if the key does not exist. Available since Redis 6.2.0. KEEPREF | DELREF | ACKED Specifies how to handle consumer group references when trimming. If there are no consumer groups, these arguments have no effect. Available since Redis 8.2. If no option is specified, KEEPREF is used by default. Unlike the XDELEX and XACKDEL commands where one of these options is required, here they are optional to maintain backward compatibility: KEEPREF (default): When trimming, removes entries from the stream according to the specified strategy ( MAXLEN or MINID ), regardless of whether they are referenced by any consumer groups, but preserves existing references to these entries in all consumer groups' PEL (Pending Entries List). DELREF : When trimming, removes entries from the stream according to the specified strategy and also removes all references to these entries from all consumer groups' PEL. ACKED : When trimming, only removes entries that were read and acknowledged by all consumer groups. Note that if the number of referenced entries is larger than MAXLEN , trimming will still stop at the limit. MAXLEN | MINID [= | ~] threshold [LIMIT count]> Trims the stream to maintain a specific size or remove old entries: MAXLEN | MINID The trimming strategy: MAXLEN : Evicts e",
      "chunk_index": 3
    },
    {
      "chunk_id": "5e465f83373c0a14b77bf97d59f4c280487c4c50",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": " MAXLEN , trimming will still stop at the limit. MAXLEN | MINID [= | ~] threshold [LIMIT count]> Trims the stream to maintain a specific size or remove old entries: MAXLEN | MINID The trimming strategy: MAXLEN : Evicts entries as long as the stream's length exceeds the specified threshold MINID : Evicts entries with IDs lower than the specified threshold (available since Redis 6.2.0) = | ~ The trimming operator: = : Exact trimming (default) - trims to the exact threshold ~ : Approximate trimming - more efficient, may leave slightly more entries than the threshold threshold The trimming threshold: For MAXLEN : threshold is a non-negative integer specifying the maximum number of entries that may remain in the stream after trimming. Redis enforces this by removing the oldest entries - that is, the entries with the lowest stream IDs - so that only the newest entries are kept. For MINID : threshold is a stream ID. All entries whose IDs are less than threshold are trimmed. All entries with IDs greater than or equal to threshold are kept. LIMIT count Limits the number of entries to examine during trimming. Available since Redis 6.2.0. When not specified, Redis uses a default value of 100 * the number of entries in a macro node. Specifying 0 disables the limiting mechanism entirely. Each entry consists of a list of field-value pairs. Redis stores the field-value pairs in the same order",
      "chunk_index": 4
    },
    {
      "chunk_id": "784f6e269b58c220b1cb4b5307f8a3c5f21cb358",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "efault value of 100 * the number of entries in a macro node. Specifying 0 disables the limiting mechanism entirely. Each entry consists of a list of field-value pairs. Redis stores the field-value pairs in the same order you provide them. Commands that read the stream, such as XRANGE or XREAD , return the fields and values in exactly the same order you added them with XADD . Note: XADD is the only Redis command that can add data to a stream. However, other commands, such as XDEL and XTRIM , can remove data from a stream. Specifying a Stream ID as an argument A stream entry ID identifies a specific entry inside a stream. XADD auto-generates a unique ID for you if you specify the * character (asterisk) as the ID argument. However, you can also specify a well-formed ID to add the new entry with that exact ID, though this is useful only in rare cases. Specify IDs using two numbers separated by a - character: 1526919030474-55 Both numbers are 64-bit integers. When Redis auto-generates an ID, the first part is the Unix time in milliseconds of the Redis instance generating the ID. The second part is a sequence number used to distinguish IDs generated in the same millisecond. You can also specify an incomplete ID that consists only of the milliseconds part, which Redis interprets as a zero value for the sequence part. To have only the sequence part automatically generated, specify the ",
      "chunk_index": 5
    },
    {
      "chunk_id": "7bedbcfd4da1983d0fecc232deea2ff01823bccf",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "econd. You can also specify an incomplete ID that consists only of the milliseconds part, which Redis interprets as a zero value for the sequence part. To have only the sequence part automatically generated, specify the milliseconds part followed by the - separator and the * character: > XADD mystream 1526919030474-55 message \"Hello,\" \"1526919030474-55\" > XADD mystream 1526919030474-* message \" World!\" \"1526919030474-56\" Redis guarantees that IDs are always incremental: the ID of any entry you insert will be greater than any previous ID, so entries are totally ordered inside a stream. To guarantee this property, if the current top ID in the stream has a time greater than the current local time of the instance, Redis uses the top entry time instead and increments the sequence part of the ID. This may happen when, for instance, the local clock jumps backward, or after a failover when the new master has a different absolute time. When you specify an explicit ID to XADD , the minimum valid ID is 0-1 , and you must specify an ID that is greater than any other ID currently inside the stream, otherwise the command fails and returns an error. Specifying explicit IDs is usually useful only if you have another system generating unique IDs (for instance an SQL table) and you want the Redis stream IDs to match those from your other system. Capped streams XADD incorporates the same semantic",
      "chunk_index": 6
    },
    {
      "chunk_id": "fd96f252f2d5906d5c866eeecb9d429ea46b8eca",
      "url": "https://redis.io/docs/latest/commands/xadd/",
      "title": "XADD | Docs",
      "text": "usually useful only if you have another system generating unique IDs (for instance an SQL table) and you want the Redis stream IDs to match those from your other system. Capped streams XADD incorporates the same semantics as the XTRIM command - refer to its documentation page for more information. This allows you to add new entries and keep the stream's size in check with a single call to XADD , effectively capping the stream with an arbitrary threshold. Although exact trimming is possible and is the default, due to the internal representation of streams, it is more efficient to add an entry and trim the stream with XADD using almost exact trimming (the ~ argument). For example, calling XADD in the following form: XADD mystream MAXLEN ~ 1000 * ... entry fields here ... This adds a new entry but also evicts old entries so that the stream contains only 1000 entries, or at most a few tens more. Additional information about streams For more information about Redis streams, see the introduction to Redis Streams document . Examples Redis Enterprise and Redis Cloud compatibility Redis Enterprise Redis Cloud Notes â Standard â Active-Active â Standard â Active-Active Return information History Starting with Redis version 6.2.0: Added the NOMKSTREAM option, MINID trimming strategy and the LIMIT option. Starting with Redis version 7.0.0: Added support for the <ms>-* explicit ID form.",
      "chunk_index": 7
    },
    {
      "chunk_id": "4759cb4d97cb52519350283d40b39bebfb8f3e2b",
      "url": "https://airflow.apache.org/docs/",
      "title": "Documentation | Apache Airflow",
      "text": "Documentation Apache Airflow Core, which includes webserver, scheduler, CLI and other components that are needed for minimal Airflow installation. Read the documentation » Apache Airflow CTL (airflowctl) is a command-line interface (CLI) for Apache Airflow that interacts exclusively with the Airflow REST API. It provides a secure, auditable, and consistent way to manage Airflow deployments â without direct access to the metadata database. Read the documentation » The Task SDK provides python-native interfaces for defining DAGs, executing tasks in isolated subprocesses and interacting with Airflow resources (e.g., Connections, Variables, XComs, Metrics, Logs, and OpenLineage events) at runtime. The goal of task-sdk is to decouple DAG authoring from Airflow internals (Scheduler, API Server, etc.), providing a forward-compatible, stable interface for writing and maintaining DAGs across Airflow versions. Read the documentation » Airflow has an official Dockerfile and Docker image published in DockerHub as a convenience package for installation. You can extend and customize the image according to your requirements and use it in your own deployments. Read the documentation » Airflow has an official Helm Chart that will help you set up your own Airflow on a cloud/on-prem Kubernetes environment and leverage its scalable nature to support a large group of users. Thanks to Kubernetes, ",
      "chunk_index": 0
    },
    {
      "chunk_id": "67003b99887eff09a35c7351aca1ef60b2df770e",
      "url": "https://airflow.apache.org/docs/",
      "title": "Documentation | Apache Airflow",
      "text": "ntation » Airflow has an official Helm Chart that will help you set up your own Airflow on a cloud/on-prem Kubernetes environment and leverage its scalable nature to support a large group of users. Thanks to Kubernetes, we are not tied to a specific cloud provider. Read the documentation » Airflow releases official Python API client that can be used to easily interact with Airflow REST API from Python code. See the client repository Providers packages include integrations with third party projects. They are versioned and released independently of the Apache Airflow core. Read the documentation » Active providers Suspended providers These providers are currently suspended from releases and we are not actively testing their compatibility with latest Airflow releases. You can still use the released versions of these providers if you need to and in case the reason for suspension is resolved, the provider might be resumed by a PR of a community member who will resolve the suspension reason. It the provider is suspended for quite some time, the community might make a decision about removing it. More about the suspension/resuming process can be found in the Community provider’s lifecycle documentation page. No suspended providers at the moment Removed providers These providers are no longer supported and have been removed from the codebase, you can however still use the released versi",
      "chunk_index": 1
    },
    {
      "chunk_id": "255cd91707aac46bb180b2d9e3f1093f12589d43",
      "url": "https://airflow.apache.org/docs/",
      "title": "Documentation | Apache Airflow",
      "text": "rovider’s lifecycle documentation page. No suspended providers at the moment Removed providers These providers are no longer supported and have been removed from the codebase, you can however still use the released versions of these providers if you need to. More about the removal process can be found in the Community provider’s lifecycle documentation page.",
      "chunk_index": 2
    },
    {
      "chunk_id": "ed1c08e3198b0fc1828028cc17f45624d60b4f80",
      "url": "https://www.rabbitmq.com/getstarted.html",
      "title": "RabbitMQ Tutorials",
      "text": "These tutorials cover the basics of creating messaging applications using RabbitMQ. You need to have the RabbitMQ server installed to go through the tutorials, please see the installation guide or use the community Docker image . Executable versions of these tutorials are open source , as is this website . There are two groups of tutorials: This tutorials target RabbitMQ 4.x. Queue tutorials ​ This section covers the default RabbitMQ protocol, AMQP 0-9-1. AMQP 0-9-1 Overview ​ Once you have been through the tutorials (or if you want to skip ahead), you may wish to read an Introduction to RabbitMQ Concepts and take a look at the Compatibility and Conformance page to find relevant resources to learn more about AMQP 1.0 and AMQP 0-9-1, the two core protocols implemented by RabbitMQ. Stream tutorials ​ This section covers RabbitMQ streams . Stream Overview and Blog Posts ​ Once you have been through the tutorials (or if you want to skip ahead), you may wish to read the RabbitMQ stream documentation and browse our stream blog posts . Getting Help ​ If you have any questions or comments regarding RabbitMQ, feel free to ask them on GitHub Discussion or RabbitMQ community Discord server . Tutorials in Other Languages ​ The tutorials here use a number of popular technologies, however, there are ports available for many more languages and client libraries, for example: We also maintain a",
      "chunk_index": 0
    },
    {
      "chunk_id": "dbc7dead6361d7b0c5bd6ab1b7070b80868b8ee0",
      "url": "https://www.rabbitmq.com/getstarted.html",
      "title": "RabbitMQ Tutorials",
      "text": "ty Discord server . Tutorials in Other Languages ​ The tutorials here use a number of popular technologies, however, there are ports available for many more languages and client libraries, for example: We also maintain a list of community-developed clients and developer tools for various protocols RabbitMQ supports.",
      "chunk_index": 1
    },
    {
      "chunk_id": "97d14f4dbda355d814161f7eef87a64c1d0ecd96",
      "url": "https://spark.apache.org/docs/latest/",
      "title": "Overview - Spark 4.1.0 Documentation",
      "text": "Downloading Get Spark from the downloads page of the project website. This documentation is for Spark version 4.1.0. Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions. Users can also download a “Hadoop free” binary and run Spark with any Hadoop version by augmenting Spark’s classpath . Scala and Java users can include Spark in their projects using its Maven coordinates and Python users can install Spark from PyPI. If you’d like to build Spark from source, visit Building Spark . Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS), and it should run on any platform that runs a supported version of Java. This should include JVMs on x86_64 and ARM64. It’s easy to run locally on one machine — all you need is to have java installed on your system PATH , or the JAVA_HOME environment variable pointing to a Java installation. Spark runs on Java 17/21, Scala 2.13, Python 3.10+, and R 3.5+ (Deprecated). When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. Since Spark 4.0.0, it’s Scala 2.13. Running the Examples and Shell Spark comes with several sample programs. Python, Scala, Java, and R examples are in the examples/src/main directory. To run Spark interactively in a Python interpreter, use bin/pyspark : ./bin/pyspark --master \"lo",
      "chunk_index": 0
    },
    {
      "chunk_id": "47ed577524dba9af2292645c42307fd33d7e9da1",
      "url": "https://spark.apache.org/docs/latest/",
      "title": "Overview - Spark 4.1.0 Documentation",
      "text": "Shell Spark comes with several sample programs. Python, Scala, Java, and R examples are in the examples/src/main directory. To run Spark interactively in a Python interpreter, use bin/pyspark : ./bin/pyspark --master \"local[2]\" Sample applications are provided in Python. For example: ./bin/spark-submit examples/src/main/python/pi.py 10 To run one of the Scala or Java sample programs, use bin/run-example <class> [params] in the top-level Spark directory. (Behind the scenes, this invokes the more general spark-submit script for launching applications). For example, ./bin/run-example SparkPi 10 You can also run Spark interactively through a modified version of the Scala shell. This is a great way to learn the framework. ./bin/spark-shell --master \"local[2]\" The --master option specifies the master URL for a distributed cluster , or local to run locally with one thread, or local[N] to run locally with N threads. You should start by using local for testing. For a full list of options, run the Spark shell with the --help option. Since version 1.4, Spark has provided an R API (only the DataFrame APIs are included). To run Spark interactively in an R interpreter, use bin/sparkR : ./bin/sparkR --master \"local[2]\" Example applications are also provided in R. For example: ./bin/spark-submit examples/src/main/r/dataframe.R Running Spark Client Applications Anywhere with Spark Connect Spark",
      "chunk_index": 1
    },
    {
      "chunk_id": "9a592b89e913c2b7f32836ee330fa79606f6423a",
      "url": "https://spark.apache.org/docs/latest/",
      "title": "Overview - Spark 4.1.0 Documentation",
      "text": "n/sparkR : ./bin/sparkR --master \"local[2]\" Example applications are also provided in R. For example: ./bin/spark-submit examples/src/main/r/dataframe.R Running Spark Client Applications Anywhere with Spark Connect Spark Connect is a new client-server architecture introduced in Spark 3.4 that decouples Spark client applications and allows remote connectivity to Spark clusters. The separation between client and server allows Spark and its open ecosystem to be leveraged from anywhere, embedded in any application. In Spark 3.4, Spark Connect provides DataFrame API coverage for PySpark and DataFrame/Dataset API support in Scala. To learn more about Spark Connect and how to use it, see Spark Connect Overview . Launching on a Cluster The Spark cluster mode overview explains the key concepts in running on a cluster. Spark can run both by itself, or over several existing cluster managers. It currently provides several options for deployment: Where to Go from Here Programming Guides: API Docs: Deployment Guides: Other Documents: External Resources:",
      "chunk_index": 2
    },
    {
      "chunk_id": "d40eccc216adacf119a84eff662bd8771df4e8d1",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "dbt transforms raw warehouse data into trusted data products. You write simple SQL select statements, and dbt handles the heavy lifting by creating modular, maintainable data models that power analytics, operations, and AI, replacing the need for complex and fragile transformation code. dbt is the industry standard for data transformation, helping you get more work done while producing higher quality results. You can use dbt to modularize and centralize your analytics code, while also providing your data team with guardrails typically found in software engineering workflows. Collaborate on data models, version them, and test and document your queries before safely deploying them to production, with monitoring and visibility. dbt brings software engineering best practices like version control, testing, modularity, CI/CD, and documentation to analytics workflows — helping teams build production-grade data pipelines backed by a 100,000+ member community . dbt works alongside your ingestion, visualization, and other data tools, so you can transform data directly in your cloud data platform. Read more about why we want to enable analysts to work more like software engineers in The dbt Viewpoint . Learn how other data practitioners around the world are using dbt by joining the dbt Community . Use dbt to quickly and collaboratively transform data and deploy analytics code following so",
      "chunk_index": 0
    },
    {
      "chunk_id": "08e1e8be34f96e25df9bc0b2f892801510a1b8f0",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "gineers in The dbt Viewpoint . Learn how other data practitioners around the world are using dbt by joining the dbt Community . Use dbt to quickly and collaboratively transform data and deploy analytics code following software engineering best practices like version control, modularity, portability, CI/CD, and documentation. This means anyone on the data team comfortable with SQL can safely contribute to production-grade data pipelines. The dbt platform offers the fastest, most reliable, and scalable way to deploy dbt. Allowing data teams to optimize their data transformation by developing, testing, scheduling, and investigating data models using a single, fully managed service through a web-based user interface (UI). You can learn about plans and pricing on www.getdbt.com . Learn more about the dbt platform features and try one of the dbt quickstarts . The dbt Fusion Engine is the next-generation dbt engine, designed to deliver data teams a lightning-fast development experience, intelligent cost savings, and improved governance. For more information, refer to the dbt Fusion Engine , supported features , and the installation instructions pages. dbt Core is an open-source tool that enables data practitioners to transform data and is suitable for users who prefer to manually set up dbt and locally maintain it. You can install dbt Core through the command line. Learn more with the",
      "chunk_index": 1
    },
    {
      "chunk_id": "c654f1cd8641fc13148022a0606a3570ecd3365a",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "en-source tool that enables data practitioners to transform data and is suitable for users who prefer to manually set up dbt and locally maintain it. You can install dbt Core through the command line. Learn more with the quickstart for dbt Core . Avoid writing boilerplate DML and DDL by managing transactions, dropping tables, and managing schema changes. Write business logic with just a SQL select statement, or a Python DataFrame, that returns the dataset you need, and dbt takes care of materialization . Build up reusable, or modular, data models that can be referenced in subsequent work instead of starting at the raw data with every analysis. Dramatically reduce the time your queries take to run: Leverage metadata to find long-running models that you want to optimize and use incremental models which dbt makes easy to configure and use. Write DRY er code by leveraging macros , hooks , and package management . No longer copy and paste SQL, which can lead to errors when logic changes. Instead, build reusable data models that get pulled into subsequent models and analysis. Change a model once and that change will propagate to all its dependencies. Publish the canonical version of a particular data model, encapsulating all complex business logic. All analysis on top of this model will incorporate the same business logic without needing to reimplement it. Use mature source control p",
      "chunk_index": 2
    },
    {
      "chunk_id": "559cda23565985842da7ddb0663dad713b0849b1",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "ical version of a particular data model, encapsulating all complex business logic. All analysis on top of this model will incorporate the same business logic without needing to reimplement it. Use mature source control processes like branching, pull requests, and code reviews. Write data quality tests quickly and easily on the underlying data. Many analytic errors are caused by edge cases in the data: testing helps analysts find and handle those edge cases. As a dbt user, your main focus will be on writing models (select queries) that reflect core business logic – there’s no need to write boilerplate code to create tables and views, or to define the order of execution of your models. Instead, dbt handles turning these models into objects in your warehouse for you. Feature Description Handle boilerplate code to materialize queries as relations For each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query – the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also en",
      "chunk_index": 3
    },
    {
      "chunk_id": "bf4402c7a448bb39cd4dbfd75ca11b28d18fc694",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "ions . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenc",
      "chunk_index": 4
    },
    {
      "chunk_id": "d5fd2770a977278a77e4bc7c25a04d7611943dc5",
      "url": "https://docs.getdbt.com/docs/introduction",
      "title": "What is dbt? | dbt Developer Hub",
      "text": "Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .",
      "chunk_index": 5
    },
    {
      "chunk_id": "3d3ae97b2d6ab283d3219d1bd012e47f5e6fff3c",
      "url": "https://pytorch.org/docs/stable/index.html",
      "title": "PyTorch documentation",
      "text": "Stay in touch for updates, event info, and the latest news By submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. Privacy Policy .",
      "chunk_index": 0
    },
    {
      "chunk_id": "1862f4ea493cb99125cb2df0fa1496fab9b9cc39",
      "url": "https://python.langchain.com/docs/introduction/",
      "title": "LangChain overview",
      "text": "# pip install -qU langchain \"langchain[anthropic]\" from langchain.agents import create_agent def get_weather ( city : str ) -> str : \"\"\"Get weather for a given city.\"\"\" return f \"It's always sunny in { city } !\" agent = create_agent( model = \"claude-sonnet-4-5-20250929\" , tools = [get_weather], system_prompt = \"You are a helpful assistant\" , ) # Run the agent agent.invoke( { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"what is the weather in sf\" }]} )",
      "chunk_index": 0
    },
    {
      "chunk_id": "e3bf28cde44c19535f3754c4128349bd30c99f4c",
      "url": "https://docs.pytest.org/en/stable/",
      "title": "pytest documentation",
      "text": "pytest: helps you write better programs The pytest framework makes it easy to write small, readable tests, and can scale to support complex functional testing for applications and libraries. PyPI package name : pytest A quick example # content of test_sample.py def inc ( x ): return x + 1 def test_answer (): assert inc ( 3 ) == 5 To execute it: $ pytest =========================== test session starts ============================ platform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y rootdir: /home/sweet/project collected 1 item test_sample.py F [100%] ================================= FAILURES ================================= _______________________________ test_answer ________________________________ def test_answer(): > assert inc(3) == 5 E assert 4 == 5 E + where 4 = inc(3) test_sample.py :6: AssertionError ========================= short test summary info ========================== FAILED test_sample.py:: test_answer - assert 4 == 5 ============================ 1 failed in 0.12s ============================= Due to pytest ’s detailed assertion introspection, only plain assert statements are used. See Get started for a basic introduction to using pytest. Features Detailed info on failing assert statements (no need to remember self.assert* names) Auto-discovery of test modules and functions Modular fixtures for managing small or parametrized long-lived test resources Can",
      "chunk_index": 0
    },
    {
      "chunk_id": "a75be596f02f8016595c922be7e7fe686a6de4de",
      "url": "https://docs.pytest.org/en/stable/",
      "title": "pytest documentation",
      "text": " Features Detailed info on failing assert statements (no need to remember self.assert* names) Auto-discovery of test modules and functions Modular fixtures for managing small or parametrized long-lived test resources Can run unittest (including trial) test suites out of the box Python 3.10+ or PyPy 3 Rich plugin architecture, with over 1300+ external plugins and thriving community Documentation Get started - install pytest and grasp its basics in just twenty minutes How-to guides - step-by-step guides, covering a vast range of use-cases and needs Reference guides - includes the complete pytest API reference, lists of plugins and more Explanation - background, discussion of key topics, answers to higher-level questions Support pytest Open Collective is an online funding platform for open and transparent communities. It provides tools to raise money and share your finances in full transparency. It is the platform of choice for individuals and companies that want to make one-time or monthly donations directly to the project. See more details in the pytest collective . pytest for enterprise Available as part of the Tidelift Subscription. The maintainers of pytest and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, ",
      "chunk_index": 1
    },
    {
      "chunk_id": "98a3001d6c458a11f1109741fe86c8f0dbea4d88",
      "url": "https://docs.pytest.org/en/stable/",
      "title": "pytest documentation",
      "text": "d thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. Learn more. Security pytest has never been associated with a security vulnerability, but in any case, to report a security vulnerability please use the Tidelift security contact . Tidelift will coordinate the fix and disclosure.",
      "chunk_index": 2
    },
    {
      "chunk_id": "4e623bf5bce0574d517e40ae42218422c44666b6",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "Model Cards What are Model Cards? Model cards are files that accompany the models and provide handy information. Under the hood, model cards are simple Markdown files with additional metadata. Model cards are essential for discoverability, reproducibility, and sharing! You can find a model card as the README.md file in any model repo. The model card should describe: the model its intended uses & potential limitations, including biases and ethical considerations as detailed in Mitchell, 2018 the training params and experimental info (you can embed or link to an experiment tracking platform for reference) which datasets were used to train your model the model’s evaluation results The model card template is available here . How to fill out each section of the model card is described in the Annotated Model Card . Model Cards on the Hub have two key parts, with overlapping information: Model card metadata A model repo will render its README.md as a model card. The model card is a Markdown file, with a YAML section at the top that contains metadata about the model. The metadata you add to the model card supports discovery and easier use of your model. For example: Allowing users to filter models at https://huggingface.co/models . Displaying the model’s license. Adding datasets to the metadata will add a message reading Datasets used to train: to your model page and link the relevant ",
      "chunk_index": 0
    },
    {
      "chunk_id": "36ad04ab7b0f0c70ad78484d16466242834f90cc",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "owing users to filter models at https://huggingface.co/models . Displaying the model’s license. Adding datasets to the metadata will add a message reading Datasets used to train: to your model page and link the relevant datasets, if they’re available on the Hub. Dataset and language identifiers are those listed on the Datasets and Languages pages. Adding metadata to your model card There are a few different ways to add metadata to your model card including: Using the metadata UI Directly editing the YAML section of the README.md file Via the huggingface_hub Python library, see the docs for more details. Many libraries with Hub integration will automatically add metadata to the model card when you upload a model. Using the metadata UI You can add metadata to your model card using the metadata UI. To access the metadata UI, go to the model page and click on the Edit model card button in the top right corner of the model card. This will open an editor showing the model card README.md file, as well as a UI for editing the metadata. This UI will allow you to add key metadata to your model card and many of the fields will autocomplete based on the information you provide. Using the UI is the easiest way to add metadata to your model card, but it doesn’t support all of the metadata fields. If you want to add metadata that isn’t supported by the UI, you can edit the YAML section of the",
      "chunk_index": 1
    },
    {
      "chunk_id": "ae72b50b5cf69ff9b9e6513b7c95c0bd367c8f35",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "ide. Using the UI is the easiest way to add metadata to your model card, but it doesn’t support all of the metadata fields. If you want to add metadata that isn’t supported by the UI, you can edit the YAML section of the README.md file directly. Editing the YAML section of the README.md file You can also directly edit the YAML section of the README.md file. If the model card doesn’t already have a YAML section, you can add one by adding three --- at the top of the file, then include all of the relevant metadata, and close the section with another group of --- like the example below: --- language: - \"List of ISO 639-1 code for your language\" - lang1 - lang2 thumbnail: \"url to a thumbnail used in social sharing\" tags: - tag1 - tag2 license: \"any valid license identifier\" datasets: - dataset1 - dataset2 base_model: \"base model Hub identifier\" --- You can find the detailed model card metadata specification here . Specifying a library You can specify the supported libraries in the model card metadata section. Find more about our supported libraries here . The library will be specified in the following order of priority: Specifying library_name in the model card (recommended if your model is not a transformers model). This information can be added via the metadata UI or directly in the model card YAML section: Having a tag with the name of a library that is supported If it’s not spec",
      "chunk_index": 2
    },
    {
      "chunk_id": "c12dff8c000c2c3a4fc0b773e0f67e2e3d544615",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "mmended if your model is not a transformers model). This information can be added via the metadata UI or directly in the model card YAML section: Having a tag with the name of a library that is supported If it’s not specified, the Hub will try to automatically detect the library type. However, this approach is discouraged, and repo creators should use the explicit library_name as much as possible. By looking into the presence of files such as *.nemo or *.mlmodel , the Hub can determine if a model is from NeMo or CoreML. In the past, if nothing was detected and there was a config.json file, it was assumed the library was transformers . For model repos created after August 2024, this is not the case anymore, so you need to set library_name: transformers explicitly. Specifying a base model If your model is a fine-tune, an adapter, or a quantized version of a base model, you can specify the base model in the model card metadata section. This information can also be used to indicate if your model is a merge of multiple existing models. Hence, the base_model field can either be a single model ID, or a list of one or more base_models (specified by their Hub identifiers). base_model: HuggingFaceH4/zephyr-7b-beta This metadata will be used to display the base model on the model page. Users can also use this information to filter models by base model or find models that are derived from ",
      "chunk_index": 3
    },
    {
      "chunk_id": "df4f97a59770805549a5ceaa6be52bc802b50d63",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "). base_model: HuggingFaceH4/zephyr-7b-beta This metadata will be used to display the base model on the model page. Users can also use this information to filter models by base model or find models that are derived from a specific base model: For an adapter (LoRA, PEFT, etc): For a quantized version of another model: For a merge of two or more models: In the merge case, you specify a list of two or more base_models: base_model: - Endevor/InfinityRP-v1-7B - l3utterfly/mistral-7b-v0.1-layla-v4 The Hub will infer the type of relationship from the current model to the base model ( \"adapter\", \"merge\", \"quantized\", \"finetune\" ) but you can also set it explicitly if needed: base_model_relation: quantized for instance. Specifying a new version If a new version of your model is available in the Hub, you can specify it in a new_version field. For example, on l3utterfly/mistral-7b-v0.1-layla-v3 : new_version: l3utterfly/mistral-7b-v0.1-layla-v4 This metadata will be used to display a link to the latest version of a model on the model page. If the model linked in new_version also has a new_version field, the very latest version will always be displayed. Specifying a dataset You can specify the datasets used to train your model in the model card metadata section. The datasets will be displayed on the model page and users will be able to filter models by dataset. You should use the Hub datas",
      "chunk_index": 4
    },
    {
      "chunk_id": "696661cb8d6f5761f6ba2748dad95325d06e3a2e",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": " You can specify the datasets used to train your model in the model card metadata section. The datasets will be displayed on the model page and users will be able to filter models by dataset. You should use the Hub dataset identifier, which is the same as the dataset’s repo name as the identifier: datasets: - stanfordnlp/imdb - HuggingFaceFW/fineweb Specifying a task ( pipeline_tag ) You can specify the pipeline_tag in the model card metadata. The pipeline_tag indicates the type of task the model is intended for. This tag will be displayed on the model page and users can filter models on the Hub by task. This tag is also used to determine which widget to use for the model and which APIs to use under the hood. For transformers models, the pipeline tag is automatically inferred from the model’s config.json file but you can override it in the model card metadata if required. Editing this field in the metadata UI will ensure that the pipeline tag is valid. Some other libraries with Hub integration will also automatically add the pipeline tag to the model card metadata. Specifying a license You can specify the license in the model card metadata section. The license will be displayed on the model page and users will be able to filter models by license. Using the metadata UI, you will see a dropdown of the most common licenses. If required, you can also specify a custom license by add",
      "chunk_index": 5
    },
    {
      "chunk_id": "0778aa5c7fdcac23efe21e50dd89080a03a12878",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "ill be displayed on the model page and users will be able to filter models by license. Using the metadata UI, you will see a dropdown of the most common licenses. If required, you can also specify a custom license by adding other as the license value and specifying the name and a link to the license in the metadata. --- license: other license_name: coqui-public-model-license license_link: https://coqui.ai/cpml --- If the license is not available via a URL you can link to a LICENSE stored in the model repo. Evaluation Results You can specify your model’s evaluation results in a structured way in the model card metadata. Results are parsed by the Hub and displayed in a widget on the model page. Here is an example on how it looks like for the bigcode/starcoder model: The initial metadata spec was based on Papers with code’s model-index specification . This allowed us to directly index the results into Papers with code’s leaderboards when appropriate. You could also link the source from where the eval results has been computed. NEW: We have a new, simpler metadata format for eval results. Check it out in the dedicated doc page . Here is a partial example of a model-index that was describing 01-ai/Yi-34B ’s score on the ARC benchmark. The result came from the Open LLM Leaderboard which is defined as the source : --- model-index: - name: Yi-34B results: - task: type: text-generation ",
      "chunk_index": 6
    },
    {
      "chunk_id": "c638b1a6cb77aeeb55d7797810bdc35ea12ced0c",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "index that was describing 01-ai/Yi-34B ’s score on the ARC benchmark. The result came from the Open LLM Leaderboard which is defined as the source : --- model-index: - name: Yi-34B results: - task: type: text-generation dataset: name: ai2_arc type: ai2_arc metrics: - name: AI2 Reasoning Challenge (25-Shot) type: AI2 Reasoning Challenge (25-Shot) value: 64.59 source: name: Open LLM Leaderboard url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard --- For more details on how to format this data, check out the Model Card specifications . CO2 Emissions The model card is also a great place to show information about the CO 2 impact of your model. Visit our guide on tracking and reporting CO 2 emissions to learn more. Linking a Paper If the model card includes a link to a Paper page (either on HF or an Arxiv abstract/PDF), the Hugging Face Hub will extract the arXiv ID and include it in the model tags with the format arxiv:<PAPER ID> . Clicking on the tag will let you: Visit the Paper page Filter for other models on the Hub that cite the same paper. Read more about Paper pages here . Model Card text Details on how to fill out the human-readable portion of the model card (so that it may be printed out, cut+pasted, etc.) is available in the Annotated Model Card . FAQ How are model tags determined? Each model page lists all the model’s tags in the page header, belo",
      "chunk_index": 7
    },
    {
      "chunk_id": "43f465da047569fcaf37e6f1ebbca1be29b2404a",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "ortion of the model card (so that it may be printed out, cut+pasted, etc.) is available in the Annotated Model Card . FAQ How are model tags determined? Each model page lists all the model’s tags in the page header, below the model name. These are primarily computed from the model card metadata, although some are added automatically, as described in Enabling a Widget . Can I add custom tags to my model? Yes, you can add custom tags to your model by adding them to the tags field in the model card metadata. The metadata UI will suggest some popular tags, but you can add any tag you want. For example, you could indicate that your model is focused on finance by adding a finance tag. How can I indicate that my model is not suitable for all audiences You can add a not-for-all-audiences tag to your model card metadata. When this tag is present, a message will be displayed on the model page indicating that the model is not for all audiences. Users can click through this message to view the model card. How can I display different images for dark and light mode? You can display different versions of an image optimized for each theme. This is particularly useful for logos, diagrams, or screenshots that need different color schemes to maintain visibility and aesthetics across light and dark modes. To use this feature, you’ll need to provide both versions of your image. For images uploaded ",
      "chunk_index": 8
    },
    {
      "chunk_id": "8da8ab2a4509573c42e644016657a971f5dbba1d",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": " diagrams, or screenshots that need different color schemes to maintain visibility and aesthetics across light and dark modes. To use this feature, you’ll need to provide both versions of your image. For images uploaded via the markdown editor When you upload an image directly from the markdown editor (using drag-and-drop), append the URI fragment #hf-light-mode-only or #hf-dark-mode-only to the end of the image URL to specify which theme it should display in: Image only displays when viewing in light mode ![ Logo ]( https://cdn-uploads.huggingface.co/production/uploads/logo-light.png#hf-light-mode-only ) Image only displays when viewing in dark mode ![ Logo ]( https://cdn-uploads.huggingface.co/production/uploads/logo-dark.png#hf-dark-mode-only ) For already hosted images If you want to reference images that are already hosted without re-uploading them, use HTML <img> tags with the following Tailwind CSS classes to specify which theme it should display in: // Image only displays when viewing in dark mode < img class = \"hidden dark:block\" src = \"https://hf.co/logo-dark.png\" alt = \"Logo\" /> // Image only displays when viewing in light mode < img class = \"dark:hidden\" src = \"https://hf.co/logo-light.png\" alt = \"Logo\" /> Can I write LaTeX in my model card? Yes! The Hub uses the KaTeX math typesetting library to render math formulas server-side before parsing the Markdown. You have",
      "chunk_index": 9
    },
    {
      "chunk_id": "d47fa0ff087cefc24e605962d622b3ab758431dd",
      "url": "https://huggingface.co/docs/hub/en/model-cards",
      "title": "Model Cards",
      "text": "den\" src = \"https://hf.co/logo-light.png\" alt = \"Logo\" /> Can I write LaTeX in my model card? Yes! The Hub uses the KaTeX math typesetting library to render math formulas server-side before parsing the Markdown. You have to use the following delimiters: $$ ... $$ for display mode \\\\(...\\\\) for inline mode (no space between the slashes and the parenthesis). Then you’ll be able to write: LaTeX \\LaTeX L A T E ​ X M S E = ( 1 n ) ∑ i = 1 n ( y i − x i ) 2 \\mathrm{MSE} = \\left(\\frac{1}{n}\\right)\\sum_{i=1}^{n}(y_{i} - x_{i})^{2} MSE = ( n 1 ​ ) i = 1 ∑ n ​ ( y i ​ − x i ​ ) 2 E = m c 2 E=mc^2 E = m c 2 Update on GitHub",
      "chunk_index": 10
    },
    {
      "chunk_id": "27eb9dd32ecd9d048c4c0bf204f35bba4531c781",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": "LlamaIndex is the leading framework for building LLM-powered agents over your data with LLMs and workflows . Introduction What is context augmentation? What are agents and workflows? How does LlamaIndex help build them? Use cases What kind of apps can you build with LlamaIndex? Who should use it? Getting started Get started in Python or TypeScript in just 5 lines of code! LlamaCloud Managed services for LlamaIndex including LlamaParse , the world’s best document parser. Community Get help and meet collaborators on Discord, Twitter, LinkedIn, and learn how to contribute to the project. Related projects Check out our library of connectors, readers, and other integrations at LlamaHub as well as demos and starter apps like create-llama . Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. Agents range from simple question-answering to being able to sense, decide and take actions in order to complete tasks. LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task. Workflows are multi-step processes that combine one or more agents, data connectors, and other tools to complete a task. They are event-driven software that allows you to combine RAG data sources and multiple agents to create a complex application that can perform a wide variety of",
      "chunk_index": 0
    },
    {
      "chunk_id": "979c569494a0ed82dcaf1b8c870e252db92aab28",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": "ents, data connectors, and other tools to complete a task. They are event-driven software that allows you to combine RAG data sources and multiple agents to create a complex application that can perform a wide variety of tasks with reflection, error-correction, and other hallmarks of advanced LLM applications. You can then deploy these agentic workflows as production microservices. LLMs offer a natural language interface between humans and data. LLMs come pre-trained on huge amounts of publicly available data, but they are not trained on your data. Your data may be private or specific to the problem you’re trying to solve. It’s behind APIs, in SQL databases, or trapped in PDFs and slide decks. Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our tools allow you to ingest, parse, index and process your data and quickly implement complex query workflows combining data access with LLM prompting. The most popular example of context-augmentation is Retrieval-Augmented Generation or RAG , which combines context with LLMs at inference time. LlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier. We provide tools like: Data connectors ingest your ex",
      "chunk_index": 1
    },
    {
      "chunk_id": "f3f26e2e5ee824b64deb5cef9a1af27a21d826ab",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": "inference time. LlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, agents, and more. It just makes using them easier. We provide tools like: Data connectors ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more. Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume. Engines provide natural language access to your data. For example: Query engines are powerful interfaces for question-answering (e.g. a RAG flow). Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data. Agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more. Observability/Evaluation integrations that enable you to rigorously experiment, evaluate, and monitor your app in a virtuous cycle. Workflows allow you to combine all of the above into an event-driven system far more flexible than other, graph-based approaches. Some popular use cases for LlamaIndex and context augmentation in general include: Check out our use cases documentation for more examples and links to tutorials. LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their ",
      "chunk_index": 2
    },
    {
      "chunk_id": "fd0859d887bca562b813a3edad06fe3790ccb698",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": "umentation for more examples and links to tutorials. LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module — data connectors, indices, retrievers, query engines, and reranking modules — to fit their needs. LlamaIndex is available in Python (these docs) and Typescript . If you’re not sure where to start, we recommend reading how to read these docs which will point you to the right place based on your experience level. Set an environment variable called OPENAI_API_KEY with an OpenAI API key . Install the Python library: Put some documents in a folder called data , then ask questions about them with our famous 5-line starter: from llama_index.core import VectorStoreIndex, SimpleDirectoryReader documents = SimpleDirectoryReader ( \" data \" ). load_data () index = VectorStoreIndex. from_documents ( documents ) query_engine = index. as_query_engine () response = query_engine. query ( \" Some question about the data should go here \" ) If any part of this trips you up, don’t worry! Check out our more comprehensive starter tutorials using remote APIs like OpenAI or any model that runs on your laptop . If you’re an enterprise developer, check out L",
      "chunk_index": 3
    },
    {
      "chunk_id": "f78b3bcdf59198451af9d4010166af098ecad625",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": "e \" ) If any part of this trips you up, don’t worry! Check out our more comprehensive starter tutorials using remote APIs like OpenAI or any model that runs on your laptop . If you’re an enterprise developer, check out LlamaCloud . It is an end-to-end managed service for document parsing, extraction, indexing, and retrieval - allowing you to get production-quality data for your AI agent. You can sign up and get 10,000 free credits per month, sign up for one of our plans , or come talk to us if you’re interested in an enterprise solution. We offer both SaaS and self-hosted plans. You can also check out the LlamaCloud documentation for more details. Document Parsing (LlamaParse) : LlamaParse is the best-in-class document parsing solution. It’s powered by VLMs and perfect for even the most complex documents (nested tables, embedded charts/images, and more). Learn more or check out the docs . Document Extraction (LlamaExtract) : Given a human-defined or inferred schema, extract structured data from any document. Learn more or check out the docs . Indexing/Retrieval : Set up an e2e pipeline to index a collection of documents for retrieval. Connect your data source (e.g. Sharepoint, Google Drive, S3), your vector DB data sink, and we automatically handle the document processing and syncing. Learn more or check out the docs . Need help? Have a feature suggestion? Join the LlamaIndex c",
      "chunk_index": 4
    },
    {
      "chunk_id": "72ece129edfc0af892b389c115dd6819c3ed627a",
      "url": "https://docs.llamaindex.ai/en/stable/",
      "title": "Welcome to LlamaIndex 🦙 !",
      "text": ".g. Sharepoint, Google Drive, S3), your vector DB data sink, and we automatically handle the document processing and syncing. Learn more or check out the docs . Need help? Have a feature suggestion? Join the LlamaIndex community: LlamaIndex Python LlamaIndex.TS (Typescript/Javascript package): We are open-source and always welcome contributions to the project! Check out our contributing guide for full details on how to extend the core library or add an integration to a third party like an LLM, a vector store, an agent tool and more. There’s more to the LlamaIndex universe! Check out some of our other projects: llama_deploy | Deploy your agentic workflows as production microservices LlamaHub | A large (and growing!) collection of custom data connectors SEC Insights | A LlamaIndex-powered application for financial research create-llama | A CLI tool to quickly scaffold LlamaIndex projects",
      "chunk_index": 5
    },
    {
      "chunk_id": "b56582720f0610f297dc04c4ecd37e3f75e78202",
      "url": "https://playwright.dev/docs/intro",
      "title": "Installation | Playwright",
      "text": "Introduction ​ Playwright Test is an end-to-end test framework for modern web apps. It bundles test runner, assertions, isolation, parallelization and rich tooling. Playwright supports Chromium, WebKit and Firefox on Windows, Linux and macOS, locally or in CI, headless or headed, with native mobile emulation for Chrome (Android) and Mobile Safari. You will learn Installing Playwright ​ Get started by installing Playwright using one of the following methods. Using npm, yarn or pnpm ​ The command below either initializes a new project or adds Playwright to an existing one. npm init playwright@latest When prompted, choose / confirm: TypeScript or JavaScript (default: TypeScript) Tests folder name (default: tests , or e2e if tests already exists) Add a GitHub Actions workflow (recommended for CI) Install Playwright browsers (default: yes) You can re-run the command later; it does not overwrite existing tests. Using the VS Code Extension ​ You can also create and run tests with the VS Code Extension . What's Installed ​ Playwright downloads required browser binaries and creates the scaffold below. playwright.config.ts package.json package-lock.json tests/ example.spec.ts The playwright.config centralizes configuration: target browsers, timeouts, retries, projects, reporters and more. In existing projects dependencies are added to your current package.json . tests/ contains a minimal",
      "chunk_index": 0
    },
    {
      "chunk_id": "dea5971bb152f636b538ffaa8c30f685c833489b",
      "url": "https://playwright.dev/docs/intro",
      "title": "Installation | Playwright",
      "text": "c.ts The playwright.config centralizes configuration: target browsers, timeouts, retries, projects, reporters and more. In existing projects dependencies are added to your current package.json . tests/ contains a minimal starter test. Running the Example Test ​ By default tests run headless in parallel across Chromium, Firefox and WebKit (configurable in playwright.config ). Output and aggregated results display in the terminal. pnpm exec playwright test Tips: See the browser window: add --headed . Run a single project/browser: --project=chromium . Run one file: npx playwright test tests/example.spec.ts . Open testing UI: --ui . See Running Tests for details on filtering, headed mode, sharding and retries. HTML Test Reports ​ After a test run, the HTML Reporter provides a dashboard filterable by the browser, passed, failed, skipped, flaky and more. Click a test to inspect errors, attachments and steps. It auto-opens only when failures occur; open manually with the command below. npx playwright show-report yarn playwright show-report pnpm exec playwright show-report Running the Example Test in UI Mode ​ Run tests with UI Mode for watch mode, live step view, time travel debugging and more. yarn playwright test --ui pnpm exec playwright test --ui See the detailed guide on UI Mode for watch filters, step details and trace integration. Updating Playwright ​ Update Playwright and dow",
      "chunk_index": 1
    },
    {
      "chunk_id": "77af08ab879ecbb23278e3f09423a0fd65b475d0",
      "url": "https://playwright.dev/docs/intro",
      "title": "Installation | Playwright",
      "text": "travel debugging and more. yarn playwright test --ui pnpm exec playwright test --ui See the detailed guide on UI Mode for watch filters, step details and trace integration. Updating Playwright ​ Update Playwright and download new browser binaries and their dependencies: npm install -D @playwright/test@latest npx playwright install --with-deps yarn add --dev @playwright/test@latest yarn playwright install --with-deps pnpm install --save-dev @playwright/test@latest pnpm exec playwright install --with-deps Check your installed version: yarn playwright --version pnpm exec playwright --version System requirements ​ Node.js: latest 20.x, 22.x or 24.x. Windows 11+, Windows Server 2019+ or Windows Subsystem for Linux (WSL). macOS 14 (Ventura) or later. Debian 12 / 13, Ubuntu 22.04 / 24.04 (x86-64 or arm64). What's next ​",
      "chunk_index": 2
    },
    {
      "chunk_id": "e5b63287bf8d56029c9edcf7a9ae56007943165f",
      "url": "https://cp-algorithms.com/",
      "title": "Algorithms for Competitive Programming",
      "text": "Last update: April 17, 2024 Algorithms for Competitive Programming The goal of this project is to translate the wonderful resource https://e-maxx.ru/algo which provides descriptions of many algorithms and data structures especially popular in field of competitive programming. Moreover we want to improve the collected knowledge by extending the articles and adding new articles to the collection. We're an ad-free, volunteer-run website that's free for everyone. Users can contribute articles or help sponsor bounties on articles for greater algorithmic coverage. Your help is greatly appreciated. Compiled pages are published at https://cp-algorithms.com/ . Become a Contributor Changelog August, 2025: Overhaul of CP-Algorithms donation system . Please consider supporting us, so that we can grow! August, 2025: Launched a Discord server ! October, 2024: Welcome new maintainers: jxu , mhayter and kostero ! October, 15, 2024: GitHub pages based mirror is now served at https://gh.cp-algorithms.com/ , and an auxiliary competitive programming library is available at https://lib.cp-algorithms.com/ . July 16, 2024: Major overhaul of the Finding strongly connected components / Building condensation graph article. June 26, 2023: Added automatic RSS feeds for new articles and updates in articles . December 20, 2022: The repository name and the owning organizations were renamed! Now the repo is l",
      "chunk_index": 0
    },
    {
      "chunk_id": "61bb8dbd800fec49f8d978e2dedfdf05ae312c98",
      "url": "https://cp-algorithms.com/",
      "title": "Algorithms for Competitive Programming",
      "text": "Building condensation graph article. June 26, 2023: Added automatic RSS feeds for new articles and updates in articles . December 20, 2022: The repository name and the owning organizations were renamed! Now the repo is located at https://github.com/cp-algorithms/cp-algorithms . It is recommended to update the upstream link in your local repositories, if you have any. October 31, 2022: It is now possible to select and copy $\\LaTeX$ source code of formulas within the articles. June 8, 2022: Tags are enabled. Each article is now marked whether it is translated or original, overall tag info is present in the tag index . For translated articles, clicking on From: X tag would lead to the original article. June 7, 2022: Date of last commit and author list with contribution percentage is tracked for each page. June 5, 2022: Enabled content tabs and sidebar navigation. The navigation is moved to a separate page and its structure should be adjusted in navigation.md whenever a new article is created or an old one is moved. January 16, 2022: Switched to the MkDocs site generator with the Material for MkDocs theme, which give the website a more modern look, brings a couple of new features (dark mode, better search, ...), makes the website more stable (in terms of rendering math formulas), and makes it easier to contribute. New articles Full list of updates: Commit History Full list of artic",
      "chunk_index": 1
    },
    {
      "chunk_id": "5ee82728686dcdf385d2f91fc4d79e8a7912370f",
      "url": "https://cp-algorithms.com/",
      "title": "Algorithms for Competitive Programming",
      "text": "f new features (dark mode, better search, ...), makes the website more stable (in terms of rendering math formulas), and makes it easier to contribute. New articles Full list of updates: Commit History Full list of articles: Navigation",
      "chunk_index": 2
    },
    {
      "chunk_id": "ba92eb766572959eb1daa9fae03356ce9dde539a",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "Why Cypress? What you'll learn ​ The solutions Cypress provides for testing The features of Cypress App, Cypress Cloud, UI Coverage, and Cypress Accessibility Our mission and what we believe in Key differences between Cypress and other testing tools In a nutshell ​ Cypress is a next generation front end testing tool built for the modern web. We address the key pain points teams face when testing modern applications and maintaining test suites. Our users are typically developers, QA engineers, and teams looking to build web applications and increase the quality of their existing applications. Cypress provides solutions for: Cypress can test anything that runs in a browser and surface insights into how to improve the health of your test suite and the quality of your application. Products ​ Cypress App , a free, open source , locally installed app for writing and running tests. Cypress Cloud , a paid service for recording tests, surfacing test results, and providing test analytics. UI Coverage , a premium solution providing a visual overview of test coverage across every page and component of your app, offering clear insights into uncovered areas that everyone can understand. Cypress Accessibility , a premium solution providing accessibility checks, which helps detect barriers for people with disabilities using your application. Cypress is a robust solution that can improve the qu",
      "chunk_index": 0
    },
    {
      "chunk_id": "17d1bd202bf40b501b580a159a4b8b92c846033e",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "understand. Cypress Accessibility , a premium solution providing accessibility checks, which helps detect barriers for people with disabilities using your application. Cypress is a robust solution that can improve the quality of your application. First: Cypress helps you set up and start writing tests every day while you build your application locally. Test Driven Development at its best! Next: After building up a suite of tests and integrating Cypress with your CI Provider, Cypress Cloud can record your test runs surfacing exactly what happened during the test in Test Replay . You'll never have to wonder: Why did this fail? Finally: Add on Cypress Accessibility to get continuous feedback on accessibility issues and regressions, and UI Coverage to ensure you've tested every part of your application. Features ​ Below are listed some of the key features of each product. Cypress App ​ Time Travel: Cypress takes snapshots as your tests run. Hover over commands in the Command Log to see exactly what happened at each step. Debuggability: Stop guessing why your tests are failing. Debug directly from familiar tools like Developer Tools. Our readable errors and stack traces make debugging lightning fast. Automatic Waiting: Never add waits or sleeps to your tests. Cypress automatically waits for commands and assertions before moving on. No more async hell. Spies, Stubs, and Clocks: Verif",
      "chunk_index": 1
    },
    {
      "chunk_id": "2a7c039bd114cfcf6452d234bec290c4c7cd82f2",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "ces make debugging lightning fast. Automatic Waiting: Never add waits or sleeps to your tests. Cypress automatically waits for commands and assertions before moving on. No more async hell. Spies, Stubs, and Clocks: Verify and control the behavior of functions, server responses, or timers. The same functionality you love from unit testing is right at your fingertips. Network Traffic Control: Easily control, stub, and test edge cases without involving your server. You can stub network traffic however you like. Consistent Results: Our architecture doesn't use Selenium or WebDriver. Say hello to fast, consistent and reliable tests that are flake-free. Cross Browser Testing: Run tests within Firefox and Chrome-family browsers (including Edge and Electron) locally and in a Continuous Integration pipeline . Cypress Cloud ​ Test Replay: Record to Cypress Cloud and replay the test exactly as it executed during the run for zero-configuration debugging using Test Replay . Smart Orchestration: Once you're set up to record to Cypress Cloud, easily parallelize your test suite, rerun failed specs first with Spec Prioritization , and cancel test runs on failures with Auto Cancellation for tight feedback loops. Flake Detection: Discover and diagnose unreliable tests with Cypress Cloud's Flaky test management . Branch Review: Quickly identify the impact a pull request might have on your test sui",
      "chunk_index": 2
    },
    {
      "chunk_id": "ff31b4c085defd2fc09bdfd6eafdc2a68a20bcef",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "llation for tight feedback loops. Flake Detection: Discover and diagnose unreliable tests with Cypress Cloud's Flaky test management . Branch Review: Quickly identify the impact a pull request might have on your test suite in a single view using Branch Review . Compare which tests are failing, flaky, pending, added, or modified between the source and base branches and prevent the merging of low-quality code. Integrations: Integrate with GitHub , GitLab , or Bitbucket to see test results directly on every push or pull request. Cypress Cloud also integrates with Slack , Jira , and Microsoft Teams to keep your team in the loop. Test Analytics: Track test results over time with Test Analytics to identify trends, regressions, and improvements in your test suite. Use our Data Extract API to extract the data that is important to your team. UI Coverage ​ Visualize Coverage: UI Coverage provides a visual overview of test coverage across every page and component of your app, offering clear insights into uncovered areas that everyone can understand. Results API: Use the UI Coverage Results API to programmatically access test coverage data and integrate it into your existing workflows. Branch Review: Compare runs to see newly introduced elements in your application or unexpected reductions in test coverage. Cypress Accessibility ​ Accessibility Checks: Maximize the value of your existing C",
      "chunk_index": 3
    },
    {
      "chunk_id": "11ad5ebf3ba7c1cbf93e22269481b06f818f2f60",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "g workflows. Branch Review: Compare runs to see newly introduced elements in your application or unexpected reductions in test coverage. Cypress Accessibility ​ Accessibility Checks: Maximize the value of your existing Cypress tests by instantly adding thousands of accessibility checks with no setup, code changes, or performance penalty. Run-level reports: Get a detailed report of accessibility issues found in your test runs with Run-level reports . Results API: Use the Cypress Accessibility's Results API to programmatically access Accessibility results in a CI environment. Branch Review: Compare any report against a baseline to review only the new violations, without any noise from existing issues. Solutions ​ Cypress can be used to ensure several different types of test. This can provide even more confidence that your application under test is working as intended and accessible to all users. End-to-end Testing ​ Cypress was originally designed to run end-to-end (E2E) tests on anything that runs in a browser. A typical E2E test visits the application in a browser and performs actions via the UI just like a real user would. it ( 'adds todos' , ( ) => { cy . visit ( 'https://example.cypress.io/' ) cy . get ( '[data-cy=\"new-todo\"]' ) . type ( 'write tests{enter}' ) cy . get ( '[data-cy=\"todos\"]' ) . should ( 'have.length' , 1 ) } ) Component Testing ​ Cypress Component Testing pr",
      "chunk_index": 4
    },
    {
      "chunk_id": "e3fa9e6e4ad13ca0be60c687baa96b1e8267d895",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "t ( 'https://example.cypress.io/' ) cy . get ( '[data-cy=\"new-todo\"]' ) . type ( 'write tests{enter}' ) cy . get ( '[data-cy=\"todos\"]' ) . should ( 'have.length' , 1 ) } ) Component Testing ​ Cypress Component Testing provides a component workbench for you to quickly build and test components from multiple front-end UI libraries — no matter how simple or complex. Learn more about how to test components for React , Angular , Vue , and Svelte . import Button from './Button' it ( 'uses custom text for the button label' , ( ) => { cy . mount ( < Button > Click me ! < / Button > ) cy . get ( 'button' ) . should ( 'contains.text' , 'Click me!' ) } ) import ButtonComponent from './button.component' it ( 'uses custom text for the button label' , ( ) => { cy . mount ( '<app-button>Click me!</app-button>' , { declarations : [ ButtonComponent ] , } ) cy . get ( 'button' ) . should ( 'contains.text' , 'Click me!' ) } ) import Button from './Button.vue' it ( 'uses custom text for the button label' , ( ) => { cy . mount ( Button , { slots : { default : 'Click me!' , } , } ) cy . get ( 'button' ) . should ( 'contains.text' , 'Click me!' ) } ) import Button from './Button.svelte' it ( 'uses custom text for the button label' , ( ) => { cy . mount ( Button , { props : { msg : 'Click me!' } } ) cy . get ( 'button' ) . should ( 'contains.text' , 'Click me!' ) } ) Accessibility Testing ​ You can wr",
      "chunk_index": 5
    },
    {
      "chunk_id": "600c46a896ca355d309d2aeb77e20a8488f1b576",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "e' it ( 'uses custom text for the button label' , ( ) => { cy . mount ( Button , { props : { msg : 'Click me!' } } ) cy . get ( 'button' ) . should ( 'contains.text' , 'Click me!' ) } ) Accessibility Testing ​ You can write Cypress tests to check the accessibility of your application, and use plugins to run broad accessibility scans. When combined with Cypress Accessibility in Cypress Cloud, insights can be surfaced when specific accessibility standards are not met through your testing - with no configuration required. See our Accessibility Testing guide for more details. it ( 'adds todos' , ( ) => { cy . visit ( 'https://example.cypress.io/' ) cy . get ( 'img#logo' ) . should ( 'have.attr' , 'alt' , 'Cypress Logo' ) } ) UI Coverage ​ You can increase release confidence by closing testing gaps in critical app flows using UI Coverage . Leverage data-driven insights to cover untested areas, reduce incidents, and improve app quality. Other ​ Cypress can perform arbitrary HTTP calls, thus you can use it for API testing. it ( 'adds a todo' , ( ) => { cy . request ( 'POST' , '/todos' , { title : 'Write API Tests' } ) . its ( 'body' ) . should ( 'contain' , { title : 'Write API Tests' } ) } ) And through a large number of official and 3rd party plugins you can write many other types of tests! Our mission ​ Our mission is to build a thriving testing solution that enhances productivity,",
      "chunk_index": 6
    },
    {
      "chunk_id": "ac19119d19e61e6b540b67e6d8346f874dc2cff3",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "Write API Tests' } ) } ) And through a large number of official and 3rd party plugins you can write many other types of tests! Our mission ​ Our mission is to build a thriving testing solution that enhances productivity, makes testing an enjoyable experience, and generates developer happiness. We hold ourselves accountable to champion a testing process that actually works . We believe our documentation should be approachable. This means enabling our readers to understand fully not just the what but the why as well. We want to help developers build a new generation of modern applications faster, better, and without the stress and anxiety associated with managing tests. We aim to elevate the art of software development by leveraging test results to generate actionable insights for long-term stability by proactively identifying areas for improvement. We know that in order for us to be successful we must enable, nurture, and foster an ecosystem that thrives on open source. Every line of test code is an investment in your codebase , it will never be coupled to us as a paid service or company. Tests will be able to run and work independently, always . We believe testing needs a lot of ❤️ and we are here to build a tool, a service, and a community that everyone can learn and benefit from. We're solving the hardest pain points shared by every developer working on the web. We believe in",
      "chunk_index": 7
    },
    {
      "chunk_id": "558b6b4a3cd3a26773f3413af16a1396e63b6585",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "ng needs a lot of ❤️ and we are here to build a tool, a service, and a community that everyone can learn and benefit from. We're solving the hardest pain points shared by every developer working on the web. We believe in this mission and hope that you will join us to make Cypress a lasting ecosystem that makes everyone happy. Key Differences ​ Architecture ​ Most testing tools (like Selenium) operate by running outside of the browser and executing remote commands across the network. Cypress is the exact opposite. Cypress is executed in the same run loop as your application. Behind Cypress is a Node server process. Cypress and the Node process constantly communicate, synchronize, and perform tasks on behalf of each other. Having access to both parts (front and back) gives us the ability to respond to your application's events in real time, while at the same time work outside of the browser for tasks that require a higher privilege. Cypress ultimately controls the entire automation process from top to bottom, which puts it in the unique position of being able to understand everything happening in and outside of the browser. This means Cypress is capable of delivering more consistent results than any other testing tool. Because Cypress is installed locally on your machine, it can additionally tap into the operating system for automation tasks. This makes performing tasks such as t",
      "chunk_index": 8
    },
    {
      "chunk_id": "ff8434a5892a4351cc4685adcf626e8147cf1497",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "ing more consistent results than any other testing tool. Because Cypress is installed locally on your machine, it can additionally tap into the operating system for automation tasks. This makes performing tasks such as taking screenshots, recording videos , general file system operations and network operations possible. Native access ​ Because Cypress operates within your application, that means it has native access to every single object. Whether it is the window , the document , a DOM element, your application instance, a function, a timer, a service worker, or anything else - you have access to it in your Cypress tests. For instance you can: Stub the browser or your application's functions and force them to behave as needed in your test case. Expose data stores so you can programmatically alter the state of your application directly from your test code. Test edge cases like 'empty views' by forcing your server to send empty responses. Test how your application responds to errors on your server by modifying response status codes to be 500. Modify DOM elements directly - like forcing hidden elements to be shown. Use 3rd party plugins programmatically. Instead of fussing with complex UI widgets like multi selects, autocompletes, drop downs, tree views or calendars, you can call methods directly from your test code to control them. Prevent Google Analytics from loading before an",
      "chunk_index": 9
    },
    {
      "chunk_id": "89415e08cf32f59735a3eb143083804cb39b824a",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "f fussing with complex UI widgets like multi selects, autocompletes, drop downs, tree views or calendars, you can call methods directly from your test code to control them. Prevent Google Analytics from loading before any of your application code executes when testing. Get synchronous notifications whenever your application transitions to a new page or when it begins to unload. Control time by moving forward or backward so that timers or polls automatically fire without having to wait for the required time in your tests. Add your own event listeners to respond to your application. You could update your application code to behave differently when under tests in Cypress. You can control WebSocket messages from within Cypress, conditionally load 3rd party scripts, or call functions directly on your application. For Component Tests , Cypress is browser-based, allowing you to test not only your component's functionality but also styles and appearance. You can visually see your component in action and interact with it in the app. Shortcuts ​ Cypress allows for browser context to be cached with cy.session() . This means as a user, you only need to perform authentication once for the entirety of your test suite, and restore the saved session between each test. That means you do not have to visit a login page, type in a username and password and wait for the page to load and/or redirect",
      "chunk_index": 10
    },
    {
      "chunk_id": "abb9d8887b6b5885676987fe0e7b2b2f8cb9e93d",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": "n once for the entirety of your test suite, and restore the saved session between each test. That means you do not have to visit a login page, type in a username and password and wait for the page to load and/or redirect for every test you run. You can accomplish this once with cy.session() and if needed, cy.origin() . Flake resistant ​ Cypress knows and understands everything that happens in your application synchronously. It is notified the moment the page loads and the moment the page unloads. Cypress even knows how fast an element is animating and will wait for it to stop animating . Additionally, it automatically waits for elements to become visible , to become enabled , and to stop being covered . When pages begin to transition, Cypress will pause command execution until the following page is fully loaded. You can also tell Cypress to wait on specific network requests to finish. Debuggability ​ Above all else Cypress is built for usability. There are hundreds of custom error messages describing the exact reason Cypress failed your test. There is a rich UI which visually shows you the command execution, assertions, network requests, spies, stubs, page loads, or URL changes. The Cypress App takes snapshots of your application and enables you to time travel back to the state it was in when commands ran. You can use the Developer Tools while your tests run to see every consol",
      "chunk_index": 11
    },
    {
      "chunk_id": "37ed51a4eb09027b29d99a60f82b0e0909bbdb69",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": ", or URL changes. The Cypress App takes snapshots of your application and enables you to time travel back to the state it was in when commands ran. You can use the Developer Tools while your tests run to see every console message or network request. You can inspect elements and use debugger statements in your spec or application code - you can use all the tools you're already comfortable with. This enables you to test and develop all at the same time. Trade offs ​ While there are many new and powerful capabilities of Cypress - there are also important trade-offs that we've made in making this possible. If you're interested in understanding more, we've written an entire guide on this topic. Get Started ​ Install Cypress so you can quickly see your first passing test within minutes for End-to-end tests or Component tests . Cypress in the Real World ​ Cypress makes it quick and easy to start testing, and as you begin to test your app, you'll often wonder if you're using best practices or scalable strategies . To guide the way, the Cypress team has created the Real World App (RWA) , a full stack example application that demonstrates testing with Cypress in practical and realistic scenarios. The RWA achieves full code-coverage with end-to-end tests across multiple browsers and device sizes , but also includes visual regression tests , API tests, unit tests, and runs them all in an e",
      "chunk_index": 12
    },
    {
      "chunk_id": "ffc9a9b2f9839641f2cc52d05701149206acb400",
      "url": "https://docs.cypress.io/guides/overview/why-cypress",
      "title": "Cypress testing solutions | Cypress Documentation | Cypress Documentation",
      "text": " and realistic scenarios. The RWA achieves full code-coverage with end-to-end tests across multiple browsers and device sizes , but also includes visual regression tests , API tests, unit tests, and runs them all in an efficient CI pipeline . Use the RWA to learn, experiment, tinker, and practice web application testing with Cypress. The app is bundled with everything you need, just clone the repository and start testing.",
      "chunk_index": 13
    },
    {
      "chunk_id": "7d68de15f663a2909ffa3481413550a1f85c33ca",
      "url": "https://www.bigocheatsheet.com/",
      "title": "Know Thy Complexities!",
      "text": "Know Thy Complexities! Hi there! This webpage covers the space and time Big-O complexities of common algorithms used in Computer Science. When preparing for technical interviews in the past, I found myself spending hours crawling the internet putting together the best, average, and worst case complexities for search and sorting algorithms so that I wouldn't be stumped when asked about them. Over the last few years, I've interviewed at several Silicon Valley startups, and also some bigger companies, like Google, Facebook, Yahoo, LinkedIn, and Uber, and each time that I prepared for an interview, I thought to myself \"Why hasn't someone created a nice Big-O cheat sheet?\". So, to save all of you fine folks a ton of time, I went ahead and created one. Enjoy! - Eric AngularJS to React Automated Migration Big-O Complexity Chart Horrible Bad Fair Good Excellent O(log n), O(1) O(n) O(n log n) O(n^2) O(2^n) O(n!) Operations Elements Common Data Structure Operations Data Structure Time Complexity Space Complexity Average Worst Worst Access Search Insertion Deletion Access Search Insertion Deletion Array Θ(1) Θ(n) Θ(n) Θ(n) O(1) O(n) O(n) O(n) O(n) Stack Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Queue Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Singly-Linked List Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Doubly-Linked List Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Skip List Θ(log",
      "chunk_index": 0
    },
    {
      "chunk_id": "85d70848bc488e7fae9041c412fb6a8b470498a3",
      "url": "https://www.bigocheatsheet.com/",
      "title": "Know Thy Complexities!",
      "text": " O(n) O(n) O(1) O(1) O(n) Queue Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Singly-Linked List Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Doubly-Linked List Θ(n) Θ(n) Θ(1) Θ(1) O(n) O(n) O(1) O(1) O(n) Skip List Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(n) O(n) O(n) O(n) O(n log(n)) Hash Table N/A Θ(1) Θ(1) Θ(1) N/A O(n) O(n) O(n) O(n) Binary Search Tree Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(n) O(n) O(n) O(n) O(n) Cartesian Tree N/A Θ(log(n)) Θ(log(n)) Θ(log(n)) N/A O(n) O(n) O(n) O(n) B-Tree Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) Red-Black Tree Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) Splay Tree N/A Θ(log(n)) Θ(log(n)) Θ(log(n)) N/A O(log(n)) O(log(n)) O(log(n)) O(n) AVL Tree Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) KD Tree Θ(log(n)) Θ(log(n)) Θ(log(n)) Θ(log(n)) O(n) O(n) O(n) O(n) O(n) Array Sorting Algorithms Learn More Get the Official Big-O Cheat Sheet Poster Contributors Eric Rowell Quentin Pleple Michael Abed Nick Dizazzo Adam Forsyth Felix Zhu Jay Engineer Josh Davis Nodir Turakulov Jennifer Hamon David Dorfman Bart Massey Ray Pereda Si Pham Mike Davis mcverry Max Hoffmann Bahador Saket Damon Davison Alvin Wan Alan Briolat Drew Hannay Andrew Rasmussen Dennis Tsang Vinnie Magro Adam Arold Alejandro Ramirez Aneel Nazareth Rahul Chow",
      "chunk_index": 1
    },
    {
      "chunk_id": "e225d784f515b73a9cfe0c9769e94aaeddb080b1",
      "url": "https://www.bigocheatsheet.com/",
      "title": "Know Thy Complexities!",
      "text": "t Massey Ray Pereda Si Pham Mike Davis mcverry Max Hoffmann Bahador Saket Damon Davison Alvin Wan Alan Briolat Drew Hannay Andrew Rasmussen Dennis Tsang Vinnie Magro Adam Arold Alejandro Ramirez Aneel Nazareth Rahul Chowdhury Jonathan McElroy steven41292 Brandon Amos Joel Friedly Casper Van Gheluwe Eric Lefevre-Ardant Oleg Renfred Harper Piper Chester Miguel Amigot Apurva K Matthew Daronco Yun-Cheng Lin Clay Tyler Orhan Can Ozalp Ayman Singh David Morton Aurelien Ooms Sebastian Paaske Torholm Koushik Krishnan Drew Bailey Robert Burke Make this Page Better Edit these tables!",
      "chunk_index": 2
    }
  ]
}